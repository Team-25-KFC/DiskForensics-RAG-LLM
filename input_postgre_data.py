import os
import json
import time
import ijson
import psycopg2
from psycopg2.extras import execute_values
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# 0ï¸âƒ£ ê¸°ë³¸ ì„¤ì •
# ==========================================
BASE_DIR = r"D:\foresic_project\json_file"
CHUNK_DIR = os.path.join(BASE_DIR, "json_chunks")
os.makedirs(CHUNK_DIR, exist_ok=True)

CHUNK_SIZE = 200 * 1024 * 1024  # 200MB
MAX_WORKERS = 4  # ë™ì‹œì— ì²˜ë¦¬í•  ìŠ¤ë ˆë“œ ìˆ˜ (CPU/ë””ìŠ¤í¬ ì„±ëŠ¥ì— ë§ê²Œ ì¡°ì ˆ)

DB_INFO = dict(
    dbname="forensic_db",
    user="postgres",
    password="admin123",
    host="localhost",
    port="5432"
)

# ==========================================
# 1ï¸âƒ£ PostgreSQL ì—°ê²° (ë©”ì¸ ì»¤ë„¥ì…˜)
# ==========================================
print("ğŸš€ PostgreSQL ì—°ê²° ì‹œë„ ì¤‘...")
conn = psycopg2.connect(**DB_INFO)
cur = conn.cursor()
print("âœ… PostgreSQL ì—°ê²° ì„±ê³µ")

# ==========================================
# 2ï¸âƒ£ JSON â†’ JSONL ë¶„í•  í•¨ìˆ˜
# ==========================================
def split_json_to_jsonl(input_path, output_dir):
    file_name = os.path.basename(input_path)
    base_name = os.path.splitext(file_name)[0]
    print(f"\nğŸ“‚ ë³€í™˜ ì‹œì‘: {file_name}")

    output_files = []
    file_size = os.path.getsize(input_path)

    # âœ… 200MB ë¯¸ë§Œ â†’ ë‹¨ì¼ JSONL
    if file_size < CHUNK_SIZE:
        output_path = os.path.join(output_dir, f"{base_name}.jsonl")
        with open(input_path, "r", encoding="utf-8") as infile, open(output_path, "w", encoding="utf-8") as out:
            try:
                data = json.load(infile)
            except Exception as e:
                print(f"âš ï¸ {file_name} JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                return []

            if isinstance(data, list):
                for record in data:
                    out.write(json.dumps(record, ensure_ascii=False) + "\n")
            elif isinstance(data, dict):
                for key, val in data.items():
                    if isinstance(val, list):
                        for record in val:
                            out.write(json.dumps(record, ensure_ascii=False) + "\n")
            else:
                out.write(json.dumps(data, ensure_ascii=False) + "\n")

        output_files.append(output_path)
        print(f"âœ… {file_name} â†’ ë‹¨ì¼ JSONL ë³€í™˜ ì™„ë£Œ ({file_size/1024/1024:.2f}MB)")
        return output_files

    # âœ… 200MB ì´ìƒ â†’ ìŠ¤íŠ¸ë¦¬ë° ë¶„í• 
    try:
        with open(input_path, "r", encoding="utf-8") as f:
            first_event = next(ijson.parse(f))
            prefix, event, value = first_event
            root_path = "item" if event == "start_array" else "records.item"
    except Exception as e:
        print(f"âš ï¸ {file_name} êµ¬ì¡° ê°ì§€ ì‹¤íŒ¨: {e}")
        return []

    with open(input_path, "r", encoding="utf-8") as f:
        parser = ijson.items(f, root_path)
        chunk_index, current_size, total_records = 0, 0, 0
        out_path = os.path.join(output_dir, f"{base_name}_chunk_{chunk_index}.jsonl")
        out = open(out_path, "w", encoding="utf-8")

        for record in parser:
            record_str = json.dumps(record, ensure_ascii=False)
            record_size = len(record_str.encode("utf-8")) + 1
            current_size += record_size
            total_records += 1

            if current_size >= CHUNK_SIZE:
                out.close()
                output_files.append(out_path)
                print(f"âœ… {os.path.basename(out_path)} ì €ì¥ ì™„ë£Œ (~{current_size/1024/1024:.2f}MB)")
                chunk_index += 1
                out_path = os.path.join(output_dir, f"{base_name}_chunk_{chunk_index}.jsonl")
                out = open(out_path, "w", encoding="utf-8")
                current_size = 0

            out.write(record_str + "\n")

        out.close()
        output_files.append(out_path)
        print(f"âœ… {os.path.basename(out_path)} (ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥ ì™„ë£Œ)")
        print(f"ğŸ¯ {file_name} ì´ {total_records}ê°œ ë ˆì½”ë“œ, {chunk_index+1}ê°œ ì²­í¬ ìƒì„±")

    return output_files

# ==========================================
# 3ï¸âƒ£ í…Œì´ë¸” ìƒì„± í•¨ìˆ˜
# ==========================================
def recreate_table(table_name):
    try:
        cur.execute(f'DROP TABLE IF EXISTS "{table_name}";')
        cur.execute(f"""
        CREATE TABLE "{table_name}" (
            id SERIAL PRIMARY KEY,
            source TEXT,
            artifact_name TEXT,
            file_name TEXT,
            full_description TEXT,
            tag TEXT
        );
        """)
        conn.commit()
        print(f"ğŸ§± {table_name} í…Œì´ë¸” ì¬ìƒì„± ì™„ë£Œ")
    except Exception as e:
        conn.rollback()
        print(f"âš ï¸ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨ ({table_name}): {e}")

# ==========================================
# 4ï¸âƒ£ ì²­í¬ íŒŒì¼ 1ê°œ ì—…ë¡œë“œ (ê°œë³„ ìŠ¤ë ˆë“œ)
# ==========================================
def upload_chunk_to_db(table_name, file_path):
    try:
        local_conn = psycopg2.connect(**DB_INFO)
        local_cur = local_conn.cursor()

        rows = []
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                record = json.loads(line.strip())
                rows.append((
                    record.get("source"),
                    record.get("artifact_name"),
                    record.get("file_name"),
                    record.get("full_description"),
                    record.get("tag")
                ))

        if rows:
            execute_values(
                local_cur,
                f'INSERT INTO "{table_name}" (source, artifact_name, file_name, full_description, tag) VALUES %s',
                rows,
                page_size=5000
            )
            local_conn.commit()

        local_cur.close()
        local_conn.close()
        return (file_path, len(rows), None)
    except Exception as e:
        return (file_path, 0, str(e))

# ==========================================
# 5ï¸âƒ£ ì „ì²´ ì—…ë¡œë“œ (ë³‘ë ¬)
# ==========================================
def upload_jsonl_parallel():
    files = [f for f in os.listdir(CHUNK_DIR) if f.lower().endswith(".jsonl")]
    if not files:
        print("âŒ ì—…ë¡œë“œí•  JSONL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nğŸ” ì´ {len(files)}ê°œ JSONL íŒŒì¼ íƒì§€ë¨")
    table_map = {}
    for f in files:
        base = f.split("_chunk_")[0] if "_chunk_" in f else os.path.splitext(f)[0]
        table_map.setdefault(base, []).append(f)

    start_all = time.time()
    for table_name, chunk_list in table_map.items():
        print(f"\nğŸš€ {table_name} í…Œì´ë¸” ì—…ë¡œë“œ ì‹œì‘ ({len(chunk_list)}ê°œ ì²­í¬)")
        recreate_table(table_name)

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(upload_chunk_to_db, table_name, os.path.join(CHUNK_DIR, file)) for file in chunk_list]
            total_inserted = 0
            for future in as_completed(futures):
                file_path, inserted, error = future.result()
                if error:
                    print(f"âŒ {os.path.basename(file_path)} ì˜¤ë¥˜: {error}")
                else:
                    print(f"âœ… {os.path.basename(file_path)} ì™„ë£Œ ({inserted}ê°œ)")
                    total_inserted += inserted
            print(f"ğŸ¯ {table_name} ì´ {total_inserted}ê°œ ì—…ë¡œë“œ ì™„ë£Œ âœ…")

    print(f"\nâœ… ì „ì²´ DB ì—…ë¡œë“œ ì™„ë£Œ (ì´ {(time.time()-start_all)/60:.2f}ë¶„ ì†Œìš”)")

# ==========================================
# 6ï¸âƒ£ ë©”ì¸ ì‹¤í–‰
# ==========================================
start = time.time()
json_files = [f for f in os.listdir(BASE_DIR) if f.lower().endswith(".json")]

if not json_files:
    print(f"âŒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {BASE_DIR}")
else:
    print(f"ğŸ“ ì²˜ë¦¬ ëŒ€ìƒ JSON íŒŒì¼: {json_files}")

    # âœ… 1ë‹¨ê³„: JSON íŒŒì¼ ë³‘ë ¬ ë³€í™˜
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_file = {executor.submit(split_json_to_jsonl, os.path.join(BASE_DIR, file), CHUNK_DIR): file for file in json_files}
        for future in as_completed(future_to_file):
            file = future_to_file[future]
            try:
                result_files = future.result()
                print(f"âœ… {file} ë³€í™˜ ì™„ë£Œ â†’ {len(result_files)}ê°œ ì²­í¬ ìƒì„±")
            except Exception as e:
                print(f"âŒ {file} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # âœ… 2ë‹¨ê³„: ë³‘ë ¬ DB ì—…ë¡œë“œ
    upload_jsonl_parallel()

print(f"\nğŸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì´ {(time.time()-start)/60:.2f}ë¶„ ì†Œìš”")
cur.close()
conn.close()
print("ğŸ”’ PostgreSQL ì—°ê²° ì¢…ë£Œ ì™„ë£Œ âœ…")
