import re
import json
import psycopg2
from sentence_transformers import SentenceTransformer
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
import torch
from langflow.custom.custom_component.component import Component
from langflow.io import MessageInput, Output
from langflow.schema.data import Data
from langflow.schema.message import Message
import requests
import numpy as np

# ================================================
# 1ï¸âƒ£ í†µí•© ì»´í¬ë„ŒíŠ¸
# ================================================
class ForensicSearchAndUpload(Component):
    display_name = "Forensic Extract â†’ PostgreSQL â†’ Milvus"
    description = "LangFlow Messageì—ì„œ JSONì„ ì¶”ì¶œí•˜ê³  PostgreSQL ê²€ìƒ‰ í›„ Milvus ì—…ë¡œë“œê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤."
    icon = "database"

    inputs = [
        MessageInput(
            name="input_message",
            display_name="Input Message",
            info="LangFlow Message ê°ì²´ (text í•„ë“œ ë‚´ JSON ë¸”ë¡ í¬í•¨)",
            input_types=["Message"],
            required=True,
        ),
    ]

    outputs = [
        Output(
            display_name="Upload Summary",
            name="results",
            type_=Data,
            method="run",
        )
    ]

    def run(self) -> Data:
        try:
            self.log("ğŸ§© ForensicTextExtractor ì‹œì‘")

            raw_message = self.input_message
            if isinstance(raw_message, Message):
                data = raw_message.data
            else:
                data = raw_message

            text_field = None
            if isinstance(data, dict) and "text" in data:
                text_field = data["text"]
            elif hasattr(data, "text"):
                text_field = getattr(data, "text", None)
            else:
                raise ValueError("Message ë°ì´í„°ì— 'text' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

            if not text_field:
                raise ValueError("text í•„ë“œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

            # ```json``` ë¸”ë¡ ì¶”ì¶œ
            json_blocks = re.findall(r"```json\s*(\{.*?\})\s*```", text_field, re.DOTALL)
            self.log(f"ğŸ“¦ JSON ë¸”ë¡ {len(json_blocks)}ê°œ ê°ì§€ë¨")

            texts = []
            for block in json_blocks:
                try:
                    parsed = json.loads(block)
                    texts.append(parsed)
                except json.JSONDecodeError as e:
                    self.log(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

            formatted_json = json.dumps({"count": len(texts), "texts": texts}, indent=2, ensure_ascii=False)
            self.log(f"âœ… ì¶”ì¶œ ì™„ë£Œ:\n{formatted_json}")

            # ================================================
            # 2ï¸âƒ£ PostgreSQL ì—°ê²°
            # ================================================
            self.log("ğŸ”— PostgreSQL ì—°ê²° ì‹œë„ ì¤‘...")

            DB_CONFIG = {
                "host": "localhost",
                "dbname": "forensic_db",
                "user": "postgres",
                "password": "admin123",
            }

            conn = psycopg2.connect(**DB_CONFIG)
            cur = conn.cursor()
            self.log("âœ… PostgreSQL ì—°ê²° ì„±ê³µ")

            # ================================================
            # 3ï¸âƒ£ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            # ================================================
            cur.execute("""
                SELECT table_name
                FROM information_schema.tables
                WHERE table_schema='public'
                  AND (
                      table_name LIKE '%_Output_tagged'
                      OR table_name LIKE '%_Output'
                      OR table_name LIKE '%_ASEPs_Output%'
                      OR table_name LIKE '%_UserActivity_Output%'
                      OR table_name LIKE 'view_%'
                  )
                ORDER BY table_name;
            """)
            tables = [t[0] for t in cur.fetchall()]
            self.log(f"ğŸ“‹ {len(tables)}ê°œ í…Œì´ë¸” í™•ì¸ë¨: {', '.join(tables)}")

            matched_rows = []
            summary = {}
            total_hits = 0

            # ================================================
            # 4ï¸âƒ£ keyword íŒŒì‹± ë° ê²€ìƒ‰ (AND / OR ì™„ì „ ì§€ì›)
            # ================================================

            tags = json.loads(formatted_json)["texts"]
            self.log(f"ğŸ” {len(tags)}ê°œì˜ íƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘")

            for tag in tags:
                text_desc = tag.get("text", "")
                # Keywords ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if "Keywords:" in text_desc:
                    keywords_part = text_desc.split("Keywords:")[-1]
                    keywords = [k.strip() for k in keywords_part.split(",") if k.strip()]
                else:
                    keywords = []

                self.log(f"ğŸ§  [{text_desc[:80]}...] â†’ {len(keywords)} keywords ê°ì§€")

                for kw in keywords:
                    kw = kw.replace("\\", "\\\\")  # ë°±ìŠ¬ë˜ì‹œ ì´ìŠ¤ì¼€ì´í”„

                    for table_name in tables:
                        # ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        cur.execute("""
                            SELECT column_name
                            FROM information_schema.columns
                            WHERE table_schema='public' AND table_name=%s;
                        """, (table_name,))
                        columns = [c[0] for c in cur.fetchall()]

                        # ê²€ìƒ‰ ì»¬ëŸ¼ ìë™ ì„ íƒ
                        if {"type", "description"}.issubset(set(columns)):
                            concat_expr = "COALESCE(type,'') || ' ' || COALESCE(description,'')"
                        elif {"keypath", "value", "data"}.issubset(set(columns)):
                            concat_expr = "COALESCE(keypath,'') || ' ' || COALESCE(value,'') || ' ' || COALESCE(data,'')"
                        else:
                            concat_expr = " || ' ' || ".join([f"COALESCE({col},'')" for col in columns if col not in ('id',)])

                        # ë…¼ë¦¬ ì—°ì‚°ì ì²˜ë¦¬ (í–‰ ë‹¨ìœ„ ë§¤ì¹­)
                        if " AND " in kw:
                            parts = [p.strip() for p in kw.split("AND") if p.strip()]
                            condition = " AND ".join([f"({concat_expr} ILIKE '%{p}%')" for p in parts])
                        elif " OR " in kw:
                            parts = [p.strip() for p in kw.split("OR") if p.strip()]
                            condition = " OR ".join([f"({concat_expr} ILIKE '%{p}%')" for p in parts])
                        else:
                            condition = f"{concat_expr} ILIKE '%{kw}%'"

                        query = f'SELECT * FROM "{table_name}" WHERE {condition};'
                        self.log(f"ğŸ§¾ SQL ì‹¤í–‰: {query}")

                        try:
                            cur.execute(query)
                            rows = cur.fetchall()
                            if rows:
                                matched_rows.extend([(table_name, kw, r) for r in rows])
                                self.log(f"âœ… [{table_name}] '{kw}' â†’ {len(rows)}ê°œ í–‰ ì¼ì¹˜")
                            else:
                                self.log(f"âŒ [{table_name}] '{kw}' ì¼ì¹˜ ì—†ìŒ")
                        except Exception as e:
                            self.log(f"âš ï¸ SQL ì˜¤ë¥˜ ({table_name}, '{kw}'): {e}")
                            conn.rollback()



            # ================================================
            # 5ï¸âƒ£ Milvus ì—…ë¡œë“œ (í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ë§Œ ì €ì¥)
            # ================================================

            COLLECTION_NAME = "tag_test"
            DIM = 384  # all-MiniLM-L12-v2 dimension
            MODEL_NAME = "text-embedding-sentence-transformers_all-minilm-l12-v2"
            API_URL = "http://localhost:1234/v1/embeddings"

            self.log(f"ğŸš€ Milvus ì—°ê²° ì¤‘ ({COLLECTION_NAME})...")
            connections.connect("default", host="localhost", port="19530")

            # ê¸°ì¡´ collection ì œê±° í›„ ìƒˆë¡œ ìƒì„±
            if utility.has_collection(COLLECTION_NAME):
                utility.drop_collection(COLLECTION_NAME)
                self.log(f"ğŸ§¹ ê¸°ì¡´ collection '{COLLECTION_NAME}' ì‚­ì œë¨")

            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIM),
            ]
            schema = CollectionSchema(fields, description="Forensic keyword search results")
            collection = Collection(COLLECTION_NAME, schema)
            self.log(f"âœ… ìƒˆ collection '{COLLECTION_NAME}' ìƒì„± ì™„ë£Œ")

            # ================================================
            # ğŸ” PostgreSQLì—ì„œ í‚¤ì›Œë“œë¡œ ë§¤ì¹­ëœ ê²°ê³¼ ì¶”ì¶œ (Milvus ì…ë ¥ìš©)
            # ================================================
            texts = []
            self.log(f"ğŸ”¥ Milvus insert ì „ matched_rows í™•ì¸: {len(matched_rows)}")

            for table_name, kw, row in matched_rows:
                # í˜„ì¬ í…Œì´ë¸” ì»¬ëŸ¼ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                cur.execute("""
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_schema='public' AND table_name=%s;
                """, (table_name,))
                columns = [c[0] for c in cur.fetchall()]

                # í–‰ ë°ì´í„°ë¥¼ {ì»¬ëŸ¼ëª…: ê°’} í˜•íƒœë¡œ ë§¤í•‘
                row_dict = {columns[i]: str(row[i]) for i in range(len(columns)) if row[i] is not None}

                # ë¶ˆí•„ìš”í•œ í•„ë“œ(ì˜ˆ: id, tag ë“±) ì œê±° ê°€ëŠ¥
                for field in ["id", "tag"]:  # í•„ìš”ì‹œ ì¶”ê°€: "deleted", "recursive", ë“±
                    row_dict.pop(field, None)

                # í–‰ ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (ì„ë² ë”©ì— ì“¸ í•µì‹¬ ë°ì´í„°ë§Œ)
                json_text = json.dumps(row_dict, ensure_ascii=False)

                # Milvusì—ëŠ” ìˆœìˆ˜ JSON ë³¸ë¬¸ë§Œ ì €ì¥
                texts.append(json_text)

            # ì¤‘ë³µ ì œê±°
            texts = list(dict.fromkeys(texts))
            self.log(f"ğŸ§¾ ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ì‚½ì… ëŒ€ìƒ: {len(texts)}ê°œ")

            # ================================================
            # ğŸ’¾ Milvus ì—…ë¡œë“œ ë¯¸ë¦¬ë³´ê¸°
            # ================================================
            self.log("ğŸ§© Milvus ì—…ë¡œë“œ ì§ì „ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ====")
            for i, t in enumerate(texts[:10], 1):
                self.log(f"{i}. {t[:500]}")
            self.log("ğŸ§© =====================================")


            # ================================================
            # ğŸ§  LM Studio APIë¡œ ì„ë² ë”© ìš”ì²­
            # ================================================
            import requests

            try:
                response = requests.post(
                    API_URL,
                    json={
                        "model": MODEL_NAME,
                        "input": texts
                    },
                    timeout=60
                )

                if response.status_code != 200:
                    raise RuntimeError(f"âŒ LM Studio ìš”ì²­ ì‹¤íŒ¨: {response.status_code} {response.text}")

                data = response.json()
                embeddings = [item["embedding"] for item in data["data"]]

                if len(embeddings) != len(texts):
                    raise ValueError("âš ï¸ ì„ë² ë”© ê°œìˆ˜ì™€ ì…ë ¥ í…ìŠ¤íŠ¸ ê°œìˆ˜ê°€ ë‹¤ë¦„")

                # âœ… Milvusì— ì‚½ì…
                collection.insert([texts, embeddings])
                collection.flush()
                self.log(f"âœ… Milvus ì—…ë¡œë“œ ì™„ë£Œ â€” {len(texts)}ê°œ ë°ì´í„° ì‚½ì…ë¨")

                # ì¸ë±ìŠ¤ ìƒì„±
                collection.create_index(
                    field_name="vector",
                    index_params={
                        "index_type": "IVF_FLAT",
                        "metric_type": "COSINE",
                        "params": {"nlist": 1024}
                    }
                )
                self.log("âœ… Milvus ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

            except Exception as e:
                self.log(f"âŒ LM Studio ì„ë² ë”© ì‹¤íŒ¨: {e}")
                return Data(data={"error": str(e)})




            # ================================================
            # 6ï¸âƒ£ ìš”ì•½ ë¡œê·¸ ì¶œë ¥
            # ================================================
            summary_json = json.dumps({
                "status": "success",
                "total_hits": total_hits,
                "matched_tables": summary
            }, indent=2, ensure_ascii=False)

            self.log(f"ğŸ“Š ìš”ì•½ ê²°ê³¼:\n{summary_json}")

            return Data(data={"status": "success", "hits": total_hits, "summary": summary})

        except Exception as e:
            self.log(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
            return Data(data={"error": str(e)})
