import os
import gc
import time
import json
import logging
import pandas as pd
import psycopg2
from concurrent.futures import ThreadPoolExecutor, as_completed
from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility
from sentence_transformers import SentenceTransformer
from langflow.custom.custom_component.component import Component
from langflow.io import MessageTextInput, Output
from langflow.schema.data import Data


class ForensicMilvusEmbedder(Component):
    display_name = "Forensic Milvus Embedder (Unified)"
    description = "Embed all forensic DB tables (by tag) into a single Milvus collection."
    documentation = "https://docs.langflow.org/components-custom-components"
    icon = "database"
    name = "ForensicMilvusEmbedderUnified"

    inputs = [
        MessageTextInput(
            name="tags",
            display_name="Tags (comma-separated)",
            info="Tags detected by HyDE Tag Parser, e.g., 'System, UserActivity'",
            value="System",
            tool_mode=True,
        ),
        MessageTextInput(name="dbname", display_name="Database Name", value="rudrb", tool_mode=True),
        MessageTextInput(name="user", display_name="DB Username", value="rudrb", tool_mode=True),
        MessageTextInput(name="password", display_name="DB Password", value="rudrb123", tool_mode=True),
        MessageTextInput(name="host", display_name="DB Host", value="localhost", tool_mode=True),
        MessageTextInput(name="port", display_name="DB Port", value="5432", tool_mode=True),
        MessageTextInput(name="milvus_host", display_name="Milvus Host", value="localhost", tool_mode=True),
        MessageTextInput(name="milvus_port", display_name="Milvus Port", value="19530", tool_mode=True),
    ]

    outputs = [
        Output(display_name="Embedding Summary", name="output", method="run_embedder"),
    ]

    def run_embedder(self) -> Data:
        tags = [t.strip().lower() for t in self.tags.split(",") if t.strip()]
        if not tags:
            return Data(value="âŒ No tags provided.")

        db_info = dict(
            dbname=self.dbname,
            user=self.user,
            password=self.password,
            host=self.host,
            port=self.port
        )

        log_messages = []
        start_total = time.time()
        DIM = 384
        BATCH_SIZE = 200
        THREADS = 5
        collection_name = "forensic_artifacts"

        # 1ï¸âƒ£ Milvus ì—°ê²°
        try:
            connections.connect("default", host=self.milvus_host, port=self.milvus_port)
            log_messages.append(f"âœ… Connected to Milvus ({self.milvus_host}:{self.milvus_port})")
        except Exception as e:
            return Data(value=f"âŒ Milvus ì—°ê²° ì‹¤íŒ¨: {e}")

        # ê¸°ì¡´ ì½œë ‰ì…˜ ì œê±°
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)

        # 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ
        try:
            model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
            log_messages.append("âœ… Embedding model loaded (MiniLM-L12-v2)")
        except Exception as e:
            return Data(value=f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 3ï¸âƒ£ Milvus ì½œë ‰ì…˜ ìƒì„± (ë‹¨ì¼)
        try:
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIM),
                FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="tag", dtype=DataType.VARCHAR, max_length=50),
            ]
            schema = CollectionSchema(fields, description="Unified forensic artifacts")
            collection = Collection(name=collection_name, schema=schema)
            collection.create_index(
                field_name="vector",
                index_params={"metric_type": "IP", "index_type": "HNSW", "params": {"M": 8, "efConstruction": 64}}
            )
            log_messages.append(f"âœ… Milvus collection '{collection_name}' created.")
        except Exception as e:
            return Data(value=f"âŒ Milvus ì½œë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")

        # 4ï¸âƒ£ PostgreSQL ì—°ê²°
        try:
            conn = psycopg2.connect(**db_info)
            cur = conn.cursor()
            log_messages.append(f"âœ… Connected to PostgreSQL ({self.dbname})")
        except Exception as e:
            return Data(value=f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")

        # 5ï¸âƒ£ ëª¨ë“  íƒœê·¸ í…Œì´ë¸” ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ ì„ë² ë”©
        def embed_table(table_name, tag_label):
            try:
                query = f'SELECT id, source, artifact_name, file_name, full_description, tag FROM "{table_name}"'
                df = pd.read_sql(query, conn)
                if df.empty:
                    return f"âš ï¸ {table_name}: ë°ì´í„° ì—†ìŒ, ê±´ë„ˆëœ€."

                df["tag"] = tag_label
                df["text"] = df.astype(str).apply(
                    lambda row: " | ".join([f"{col}: {val}" for col, val in row.items()]),
                    axis=1
                )

                texts = df["text"].tolist()
                tags_col = df["tag"].tolist()

                # ë°°ì¹˜ ë¶„í• 
                batches = [texts[i:i+BATCH_SIZE] for i in range(0, len(texts), BATCH_SIZE)]

                def process_batch(batch_id, text_list, tag_list):
                    try:
                        start = time.perf_counter()
                        vectors = model.encode(text_list, batch_size=32, convert_to_numpy=True, show_progress_bar=False)
                        collection.insert([vectors, text_list, tag_list])
                        collection.flush()
                        elapsed = time.perf_counter() - start
                        return f"âœ… [{table_name}] Batch-{batch_id} ({len(text_list)} rows, {elapsed:.2f}s)"
                    except Exception as e:
                        return f"âŒ [{table_name}] Batch-{batch_id} ì˜¤ë¥˜: {e}"

                with ThreadPoolExecutor(max_workers=THREADS) as executor:
                    futures = [executor.submit(process_batch, i, batches[i], [tag_label]*len(batches[i])) for i in range(len(batches))]
                    for future in as_completed(futures):
                        log_messages.append(future.result())

                return f"ğŸ¯ {table_name}: {len(df)} rows inserted."
            except Exception as e:
                return f"âŒ {table_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

        for tag in tags:
            table_name = tag.lower()
            result = embed_table(table_name, tag)
            log_messages.append(result)
            gc.collect()

        conn.close()
        collection.flush()
        elapsed = (time.time() - start_total) / 60
        log_messages.append(f"ğŸ ì „ì²´ ì™„ë£Œ ({elapsed:.2f}ë¶„ ì†Œìš”)")
        log_messages.append(f"âœ… ìµœì¢… ì—”í‹°í‹° ìˆ˜: {collection.num_entities}")
        return Data(value="\n".join(log_messages))
