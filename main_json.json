{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatOutput",
            "id": "ChatOutput-b6cWX",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "context",
            "id": "Prompt-gjeP1",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-ChatOutput-b6cWX{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-b6cWXœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-gjeP1{œfieldNameœ:œcontextœ,œidœ:œPrompt-gjeP1œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatOutput-b6cWX",
        "sourceHandle": "{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-b6cWXœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-gjeP1",
        "targetHandle": "{œfieldNameœ:œcontextœ,œidœ:œPrompt-gjeP1œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Reranker",
            "id": "Reranker-VnRFH",
            "name": "output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-b6cWX",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Reranker-VnRFH{œdataTypeœ:œRerankerœ,œidœ:œReranker-VnRFHœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-ChatOutput-b6cWX{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-b6cWXœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Reranker-VnRFH",
        "sourceHandle": "{œdataTypeœ:œRerankerœ,œidœ:œReranker-VnRFHœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}",
        "target": "ChatOutput-b6cWX",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-b6cWXœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-gjeP1",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_message",
            "id": "LMStudioModel-YtP0V",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt-gjeP1{œdataTypeœ:œPromptœ,œidœ:œPrompt-gjeP1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-YtP0V{œfieldNameœ:œinput_messageœ,œidœ:œLMStudioModel-YtP0Vœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-gjeP1",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-gjeP1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-YtP0V",
        "targetHandle": "{œfieldNameœ:œinput_messageœ,œidœ:œLMStudioModel-YtP0Vœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-d6Xiw",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "node_prompt",
            "id": "LMStudioModel-YtP0V",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__TextInput-d6Xiw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-d6Xiwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-YtP0V{œfieldNameœ:œnode_promptœ,œidœ:œLMStudioModel-YtP0Vœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-d6Xiw",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-d6Xiwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-YtP0V",
        "targetHandle": "{œfieldNameœ:œnode_promptœ,œidœ:œLMStudioModel-YtP0Vœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "hydellm",
            "id": "OllamaGenerate-ce1dH",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "LMStudioEmbeddingsComponent-h6IE7",
            "inputTypes": [
              "Message",
              "Data",
              "Text"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__OllamaGenerate-ce1dH{œdataTypeœ:œhydellmœ,œidœ:œOllamaGenerate-ce1dHœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-LMStudioEmbeddingsComponent-h6IE7{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioEmbeddingsComponent-h6IE7œ,œinputTypesœ:[œMessageœ,œDataœ,œTextœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "OllamaGenerate-ce1dH",
        "sourceHandle": "{œdataTypeœ:œhydellmœ,œidœ:œOllamaGenerate-ce1dHœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioEmbeddingsComponent-h6IE7",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioEmbeddingsComponent-h6IE7œ,œinputTypesœ:[œMessageœ,œDataœ,œTextœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Milvus",
            "id": "Milvus-1FHVQ",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-fFi8K",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Milvus-1FHVQ{œdataTypeœ:œMilvusœ,œidœ:œMilvus-1FHVQœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ChatOutput-fFi8K{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-fFi8Kœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Milvus-1FHVQ",
        "sourceHandle": "{œdataTypeœ:œMilvusœ,œidœ:œMilvus-1FHVQœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
        "target": "ChatOutput-fFi8K",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-fFi8Kœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-VBIsB",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          },
          "targetHandle": {
            "fieldName": "agent_llm",
            "id": "Agent-BfTMc",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-VBIsB{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-VBIsBœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-Agent-BfTMc{œfieldNameœ:œagent_llmœ,œidœ:œAgent-BfTMcœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-VBIsB",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-VBIsBœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "Agent-BfTMc",
        "targetHandle": "{œfieldNameœ:œagent_llmœ,œidœ:œAgent-BfTMcœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-r4e17",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "Agent-BfTMc",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__TextInput-r4e17{œdataTypeœ:œTextInputœ,œidœ:œTextInput-r4e17œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Agent-BfTMc{œfieldNameœ:œinput_valueœ,œidœ:œAgent-BfTMcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-r4e17",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-r4e17œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Agent-BfTMc",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œAgent-BfTMcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioEmbeddingsComponent",
            "id": "LMStudioEmbeddingsComponent-V3d8q",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "Milvus-1FHVQ",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__LMStudioEmbeddingsComponent-V3d8q{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-V3d8qœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Milvus-1FHVQ{œfieldNameœ:œembeddingœ,œidœ:œMilvus-1FHVQœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "LMStudioEmbeddingsComponent-V3d8q",
        "sourceHandle": "{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-V3d8qœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "Milvus-1FHVQ",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œMilvus-1FHVQœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-gjeP1",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "prompt",
            "id": "CustomComponent-3K6lL",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt-gjeP1{œdataTypeœ:œPromptœ,œidœ:œPrompt-gjeP1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-3K6lL{œfieldNameœ:œpromptœ,œidœ:œCustomComponent-3K6lLœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-gjeP1",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-gjeP1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-3K6lL",
        "targetHandle": "{œfieldNameœ:œpromptœ,œidœ:œCustomComponent-3K6lLœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-d6Xiw",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_prompt",
            "id": "CustomComponent-3K6lL",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__TextInput-d6Xiw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-d6Xiwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-3K6lL{œfieldNameœ:œsystem_promptœ,œidœ:œCustomComponent-3K6lLœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-d6Xiw",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-d6Xiwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-3K6lL",
        "targetHandle": "{œfieldNameœ:œsystem_promptœ,œidœ:œCustomComponent-3K6lLœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-gjeP1",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "LMStudioModel-BS6P2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt-gjeP1{œdataTypeœ:œPromptœ,œidœ:œPrompt-gjeP1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-BS6P2{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-BS6P2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-gjeP1",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-gjeP1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-BS6P2",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-BS6P2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-d6Xiw",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "LMStudioModel-BS6P2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__TextInput-d6Xiw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-d6Xiwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-BS6P2{œfieldNameœ:œsystem_messageœ,œidœ:œLMStudioModel-BS6P2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-d6Xiw",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-d6Xiwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-BS6P2",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œLMStudioModel-BS6P2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ForensicTagQueryAndSave",
            "id": "LMStudioEmbeddingsComponent-h6IE7",
            "name": "summary",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_message",
            "id": "CustomComponent-VoB5q",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioEmbeddingsComponent-h6IE7{œdataTypeœ:œForensicTagQueryAndSaveœ,œidœ:œLMStudioEmbeddingsComponent-h6IE7œ,œnameœ:œsummaryœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-VoB5q{œfieldNameœ:œinput_messageœ,œidœ:œCustomComponent-VoB5qœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioEmbeddingsComponent-h6IE7",
        "sourceHandle": "{œdataTypeœ:œForensicTagQueryAndSaveœ,œidœ:œLMStudioEmbeddingsComponent-h6IE7œ,œnameœ:œsummaryœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-VoB5q",
        "targetHandle": "{œfieldNameœ:œinput_messageœ,œidœ:œCustomComponent-VoB5qœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "OllamaGenerate",
            "id": "LMStudioModel-YtP0V",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "ollama",
            "id": "CustomComponent-pO11O",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-YtP0V{œdataTypeœ:œOllamaGenerateœ,œidœ:œLMStudioModel-YtP0Vœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-pO11O{œfieldNameœ:œollamaœ,œidœ:œCustomComponent-pO11Oœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-YtP0V",
        "sourceHandle": "{œdataTypeœ:œOllamaGenerateœ,œidœ:œLMStudioModel-YtP0Vœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-pO11O",
        "targetHandle": "{œfieldNameœ:œollamaœ,œidœ:œCustomComponent-pO11Oœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "GeminiAPIClient",
            "id": "CustomComponent-3K6lL",
            "name": "output_message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "gemini",
            "id": "CustomComponent-pO11O",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__CustomComponent-3K6lL{œdataTypeœ:œGeminiAPIClientœ,œidœ:œCustomComponent-3K6lLœ,œnameœ:œoutput_messageœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-pO11O{œfieldNameœ:œgeminiœ,œidœ:œCustomComponent-pO11Oœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "CustomComponent-3K6lL",
        "sourceHandle": "{œdataTypeœ:œGeminiAPIClientœ,œidœ:œCustomComponent-3K6lLœ,œnameœ:œoutput_messageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-pO11O",
        "targetHandle": "{œfieldNameœ:œgeminiœ,œidœ:œCustomComponent-pO11Oœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-BS6P2",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "lmstudio",
            "id": "CustomComponent-pO11O",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-BS6P2{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-BS6P2œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-pO11O{œfieldNameœ:œlmstudioœ,œidœ:œCustomComponent-pO11Oœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-BS6P2",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-BS6P2œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-pO11O",
        "targetHandle": "{œfieldNameœ:œlmstudioœ,œidœ:œCustomComponent-pO11Oœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "MultiLMFormatter",
            "id": "CustomComponent-pO11O",
            "name": "output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-CHoVB",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-pO11O{œdataTypeœ:œMultiLMFormatterœ,œidœ:œCustomComponent-pO11Oœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-CHoVB{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-CHoVBœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-pO11O",
        "sourceHandle": "{œdataTypeœ:œMultiLMFormatterœ,œidœ:œCustomComponent-pO11Oœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-CHoVB",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-CHoVBœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatOutput",
            "id": "ChatOutput-fFi8K",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "docs",
            "id": "Reranker-VnRFH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatOutput-fFi8K{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-fFi8Kœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Reranker-VnRFH{œfieldNameœ:œdocsœ,œidœ:œReranker-VnRFHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatOutput-fFi8K",
        "sourceHandle": "{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-fFi8Kœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Reranker-VnRFH",
        "targetHandle": "{œfieldNameœ:œdocsœ,œidœ:œReranker-VnRFHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-UtzGD",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "node_prompt",
            "id": "OllamaGenerate-ce1dH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__TextInput-UtzGD{œdataTypeœ:œTextInputœ,œidœ:œTextInput-UtzGDœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-OllamaGenerate-ce1dH{œfieldNameœ:œnode_promptœ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-UtzGD",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-UtzGDœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OllamaGenerate-ce1dH",
        "targetHandle": "{œfieldNameœ:œnode_promptœ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "GateByTrigger",
            "id": "CustomComponent-ykZ8B",
            "name": "out",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "wire_in",
            "id": "LMStudioModel-VBIsB",
            "inputTypes": [
              "Message",
              "Text",
              "Data",
              "str"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-ykZ8B{œdataTypeœ:œGateByTriggerœ,œidœ:œCustomComponent-ykZ8Bœ,œnameœ:œoutœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-VBIsB{œfieldNameœ:œwire_inœ,œidœ:œLMStudioModel-VBIsBœ,œinputTypesœ:[œMessageœ,œTextœ,œDataœ,œstrœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-ykZ8B",
        "sourceHandle": "{œdataTypeœ:œGateByTriggerœ,œidœ:œCustomComponent-ykZ8Bœ,œnameœ:œoutœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-VBIsB",
        "targetHandle": "{œfieldNameœ:œwire_inœ,œidœ:œLMStudioModel-VBIsBœ,œinputTypesœ:[œMessageœ,œTextœ,œDataœ,œstrœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-8BxSP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_message",
            "id": "OllamaGenerate-ce1dH",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-8BxSP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-OllamaGenerate-ce1dH{œfieldNameœ:œinput_messageœ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-8BxSP",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OllamaGenerate-ce1dH",
        "targetHandle": "{œfieldNameœ:œinput_messageœ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-8BxSP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "Milvus-1FHVQ",
            "inputTypes": [
              "Message"
            ],
            "type": "query"
          }
        },
        "id": "xy-edge__ChatInput-8BxSP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Milvus-1FHVQ{œfieldNameœ:œsearch_queryœ,œidœ:œMilvus-1FHVQœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}",
        "selected": false,
        "source": "ChatInput-8BxSP",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Milvus-1FHVQ",
        "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œMilvus-1FHVQœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-8BxSP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-gjeP1",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-8BxSP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-gjeP1{œfieldNameœ:œquestionœ,œidœ:œPrompt-gjeP1œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-8BxSP",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-gjeP1",
        "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-gjeP1œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "MCPTools",
            "id": "MCPTools-xU7UW",
            "name": "component_as_tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-BfTMc",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__MCPTools-xU7UW{œdataTypeœ:œMCPToolsœ,œidœ:œMCPTools-xU7UWœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}-Agent-BfTMc{œfieldNameœ:œtoolsœ,œidœ:œAgent-BfTMcœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "MCPTools-xU7UW",
        "sourceHandle": "{œdataTypeœ:œMCPToolsœ,œidœ:œMCPTools-xU7UWœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-BfTMc",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-BfTMcœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-8BxSP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_message",
            "id": "ConditionalRouter-OIPJs",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-8BxSP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-ConditionalRouter-OIPJs{œfieldNameœ:œinput_messageœ,œidœ:œConditionalRouter-OIPJsœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-8BxSP",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-8BxSPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ConditionalRouter-OIPJs",
        "targetHandle": "{œfieldNameœ:œinput_messageœ,œidœ:œConditionalRouter-OIPJsœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "DBArtifactAllExistsRouter",
            "id": "ConditionalRouter-OIPJs",
            "name": "true_result",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "dummy_input_2",
            "id": "OllamaGenerate-ce1dH",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ConditionalRouter-OIPJs{œdataTypeœ:œDBArtifactAllExistsRouterœ,œidœ:œConditionalRouter-OIPJsœ,œnameœ:œtrue_resultœ,œoutput_typesœ:[œMessageœ]}-OllamaGenerate-ce1dH{œfieldNameœ:œdummy_input_2œ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ConditionalRouter-OIPJs",
        "sourceHandle": "{œdataTypeœ:œDBArtifactAllExistsRouterœ,œidœ:œConditionalRouter-OIPJsœ,œnameœ:œtrue_resultœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OllamaGenerate-ce1dH",
        "targetHandle": "{œfieldNameœ:œdummy_input_2œ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "DBArtifactAllExistsRouter",
            "id": "ConditionalRouter-OIPJs",
            "name": "false_result",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "main_message",
            "id": "CustomComponent-ykZ8B",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ConditionalRouter-OIPJs{œdataTypeœ:œDBArtifactAllExistsRouterœ,œidœ:œConditionalRouter-OIPJsœ,œnameœ:œfalse_resultœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-ykZ8B{œfieldNameœ:œmain_messageœ,œidœ:œCustomComponent-ykZ8Bœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ConditionalRouter-OIPJs",
        "sourceHandle": "{œdataTypeœ:œDBArtifactAllExistsRouterœ,œidœ:œConditionalRouter-OIPJsœ,œnameœ:œfalse_resultœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-ykZ8B",
        "targetHandle": "{œfieldNameœ:œmain_messageœ,œidœ:œCustomComponent-ykZ8Bœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Agent",
            "id": "Agent-BfTMc",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "trigger",
            "id": "CustomComponent-P0Y5W",
            "inputTypes": [
              "Text",
              "Message",
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Agent-BfTMc{œdataTypeœ:œAgentœ,œidœ:œAgent-BfTMcœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-P0Y5W{œfieldNameœ:œtriggerœ,œidœ:œCustomComponent-P0Y5Wœ,œinputTypesœ:[œTextœ,œMessageœ,œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Agent-BfTMc",
        "sourceHandle": "{œdataTypeœ:œAgentœ,œidœ:œAgent-BfTMcœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-P0Y5W",
        "targetHandle": "{œfieldNameœ:œtriggerœ,œidœ:œCustomComponent-P0Y5Wœ,œinputTypesœ:[œTextœ,œMessageœ,œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "WaitForReadyServer",
            "id": "CustomComponent-P0Y5W",
            "name": "message",
            "output_types": [
              "Text"
            ]
          },
          "targetHandle": {
            "fieldName": "dummy_input",
            "id": "OllamaGenerate-ce1dH",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__CustomComponent-P0Y5W{œdataTypeœ:œWaitForReadyServerœ,œidœ:œCustomComponent-P0Y5Wœ,œnameœ:œmessageœ,œoutput_typesœ:[œTextœ]}-OllamaGenerate-ce1dH{œfieldNameœ:œdummy_inputœ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "CustomComponent-P0Y5W",
        "sourceHandle": "{œdataTypeœ:œWaitForReadyServerœ,œidœ:œCustomComponent-P0Y5Wœ,œnameœ:œmessageœ,œoutput_typesœ:[œTextœ]}",
        "target": "OllamaGenerate-ce1dH",
        "targetHandle": "{œfieldNameœ:œdummy_inputœ,œidœ:œOllamaGenerate-ce1dHœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "description": "Get chat inputs from the Playground.",
          "display_name": "Chat Input",
          "id": "ChatInput-8BxSP",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "https://docs.langflow.org/components-io#chat-input",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-input\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        # Ensure files is a list and filter out empty/None values\n        files = self.files if self.files else []\n        if files and not isinstance(files, list):\n            files = [files]\n        files = [f for f in files if f is not None and f != \"\"]\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=files,\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "_input_type": "FileInput",
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "list_add_label": "Add More",
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "당신은 고객에게서 해킹 사고를 분석해달라는 의뢰를 받았습니다.\n\n주어진 이미지의 이벤트 로그를 분석하여, 해당 PC가 마지막으로 부팅된 시간을 구해주세요.\n\nInfo\nFLAG = DH{yyyy_MM_dd_hh_mm_ss}\nyy, MM, dd, hh, mm, ss는 시간을 표현하는 방식으로 각각 연, 월, 일, 시, 분, 초를 나타냅니다. 예를 들어 시간이 2024-01-02 03:04:05라면, FLAG는 DH{2024_01_02_03_04_05}입니다.\n시간은 UTC+9를 기준으로 합니다.\n주어진 이미지의 이벤트 로그를 분석하여, 해당 PC가 마지막으로 부팅시간은 언제야?\nrecent는 아니야\n2024-04-06 15:23:27.5000000(UTC 이 시간은 정답이 아니야\n이벤트아이디 : 6005, 4608"
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "message",
          "type": "ChatInput"
        },
        "dragging": false,
        "height": 234,
        "id": "ChatInput-8BxSP",
        "measured": {
          "height": 234,
          "width": 320
        },
        "position": {
          "x": -6573.1921316348025,
          "y": 2444.9867525599298
        },
        "positionAbsolute": {
          "x": 743.9745420290319,
          "y": 463.6977510207854
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-8iAFN",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/components-io#chat-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _icon, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-8iAFN",
        "measured": {
          "height": 165,
          "width": 320
        },
        "position": {
          "x": 4353.764332701606,
          "y": 2201.1089520531727
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-nChwD",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "n_messages",
              "format_instructions",
              "output_schema",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "last_updated": "2025-12-17T05:43:35.702Z",
            "legacy": false,
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {
                  "fields": {
                    "data": {
                      "node": {
                        "display_name": "Connect other models",
                        "icon": "CornerDownLeft",
                        "name": "connect_other_models"
                      }
                    }
                  }
                },
                "info": "The provider of the language model that the agent will use to generate responses.",
                "input_types": [],
                "load_from_db": false,
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "OpenAI"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "OpenAI"
                  }
                ],
                "placeholder": "",
                "real_time_refresh": true,
                "refresh_button": false,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nimport re\n\nfrom langchain_core.tools import StructuredTool\nfrom pydantic import ValidationError\n\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.agents.events import ExceptionWithMessageError\nfrom langflow.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS_DICT,\n    MODELS_METADATA,\n)\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers.current_date import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import (\n    ToolCallingAgentComponent,\n)\nfrom langflow.custom.custom_component.component import _get_component_toolkit\nfrom langflow.custom.utils import update_component_build_config\nfrom langflow.field_typing import Tool\nfrom langflow.helpers.base_model import build_model_from_schema\nfrom langflow.io import (\n    BoolInput,\n    DropdownInput,\n    IntInput,\n    MultilineInput,\n    Output,\n    TableInput,\n)\nfrom langflow.logging import logger\nfrom langflow.schema.data import Data\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\nfrom langflow.schema.table import EditMode\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nMODEL_PROVIDERS_LIST = [\"Anthropic\", \"Google Generative AI\", \"OpenAI\"]\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    # Filter out json_mode from OpenAI inputs since we handle structured output differently\n    openai_inputs_filtered = [\n        input_field\n        for input_field in MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"]\n        if not (hasattr(input_field, \"name\") and input_field.name == \"json_mode\")\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            refresh_button=False,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST],\n            external_options={\n                \"fields\": {\n                    \"data\": {\n                        \"node\": {\n                            \"name\": \"connect_other_models\",\n                            \"display_name\": \"Connect other models\",\n                            \"icon\": \"CornerDownLeft\",\n                        }\n                    }\n                },\n            },\n        ),\n        *openai_inputs_filtered,\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        MultilineInput(\n            name=\"format_instructions\",\n            display_name=\"Output Format Instructions\",\n            info=\"Generic Template for structured output formatting. Valid only with Structured response.\",\n            value=(\n                \"You are an AI that extracts structured JSON objects from unstructured text. \"\n                \"Use a predefined schema with expected types (str, int, float, bool, dict). \"\n                \"Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. \"\n                \"Fill missing or ambiguous values with defaults: null for missing values. \"\n                \"Remove exact duplicates but keep variations that have different field values. \"\n                \"Always return valid JSON in the expected format, never throw errors. \"\n                \"If multiple objects can be extracted, return them all in the structured format.\"\n            ),\n            advanced=True,\n        ),\n        TableInput(\n            name=\"output_schema\",\n            display_name=\"Output Schema\",\n            info=(\n                \"Schema Validation: Define the structure and data types for structured output. \"\n                \"No validation if no output schema.\"\n            ),\n            advanced=True,\n            required=False,\n            value=[],\n            table_schema=[\n                {\n                    \"name\": \"name\",\n                    \"display_name\": \"Name\",\n                    \"type\": \"str\",\n                    \"description\": \"Specify the name of the output field.\",\n                    \"default\": \"field\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n                {\n                    \"name\": \"description\",\n                    \"display_name\": \"Description\",\n                    \"type\": \"str\",\n                    \"description\": \"Describe the purpose of the output field.\",\n                    \"default\": \"description of field\",\n                    \"edit_mode\": EditMode.POPOVER,\n                },\n                {\n                    \"name\": \"type\",\n                    \"display_name\": \"Type\",\n                    \"type\": \"str\",\n                    \"edit_mode\": EditMode.INLINE,\n                    \"description\": (\"Indicate the data type of the output field (e.g., str, int, float, bool, dict).\"),\n                    \"options\": [\"str\", \"int\", \"float\", \"bool\", \"dict\"],\n                    \"default\": \"str\",\n                },\n                {\n                    \"name\": \"multiple\",\n                    \"display_name\": \"As List\",\n                    \"type\": \"boolean\",\n                    \"description\": \"Set to True if this output field should be a list of the specified type.\",\n                    \"default\": \"False\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n            ],\n        ),\n        *LCToolsAgentComponent._base_inputs,\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [\n        Output(name=\"response\", display_name=\"Response\", method=\"message_response\"),\n    ]\n\n    async def get_agent_requirements(self):\n        \"\"\"Get the agent requirements for the agent.\"\"\"\n        llm_model, display_name = await self.get_llm()\n        if llm_model is None:\n            msg = \"No language model selected. Please choose a model to proceed.\"\n            raise ValueError(msg)\n        self.model_name = get_model_name(llm_model, display_name=display_name)\n\n        # Get memory data\n        self.chat_history = await self.get_memory_data()\n        if isinstance(self.chat_history, Message):\n            self.chat_history = [self.chat_history]\n\n        # Add current date tool if enabled\n        if self.add_current_date_tool:\n            if not isinstance(self.tools, list):  # type: ignore[has-type]\n                self.tools = []\n            current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n            if not isinstance(current_date_tool, StructuredTool):\n                msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                raise TypeError(msg)\n            self.tools.append(current_date_tool)\n        return llm_model, self.chat_history, self.tools\n\n    async def message_response(self) -> Message:\n        try:\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            result = await self.run_agent(agent)\n\n            # Store result for potential JSON output\n            self._agent_result = result\n\n        except (ValueError, TypeError, KeyError) as e:\n            await logger.aerror(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            await logger.aerror(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        # Avoid catching blind Exception; let truly unexpected exceptions propagate\n        except Exception as e:\n            await logger.aerror(f\"Unexpected error: {e!s}\")\n            raise\n        else:\n            return result\n\n    def _preprocess_schema(self, schema):\n        \"\"\"Preprocess schema to ensure correct data types for build_model_from_schema.\"\"\"\n        processed_schema = []\n        for field in schema:\n            processed_field = {\n                \"name\": str(field.get(\"name\", \"field\")),\n                \"type\": str(field.get(\"type\", \"str\")),\n                \"description\": str(field.get(\"description\", \"\")),\n                \"multiple\": field.get(\"multiple\", False),\n            }\n            # Ensure multiple is handled correctly\n            if isinstance(processed_field[\"multiple\"], str):\n                processed_field[\"multiple\"] = processed_field[\"multiple\"].lower() in [\n                    \"true\",\n                    \"1\",\n                    \"t\",\n                    \"y\",\n                    \"yes\",\n                ]\n            processed_schema.append(processed_field)\n        return processed_schema\n\n    async def build_structured_output_base(self, content: str):\n        \"\"\"Build structured output with optional BaseModel validation.\"\"\"\n        json_pattern = r\"\\{.*\\}\"\n        schema_error_msg = \"Try setting an output schema\"\n\n        # Try to parse content as JSON first\n        json_data = None\n        try:\n            json_data = json.loads(content)\n        except json.JSONDecodeError:\n            json_match = re.search(json_pattern, content, re.DOTALL)\n            if json_match:\n                try:\n                    json_data = json.loads(json_match.group())\n                except json.JSONDecodeError:\n                    return {\"content\": content, \"error\": schema_error_msg}\n            else:\n                return {\"content\": content, \"error\": schema_error_msg}\n\n        # If no output schema provided, return parsed JSON without validation\n        if not hasattr(self, \"output_schema\") or not self.output_schema or len(self.output_schema) == 0:\n            return json_data\n\n        # Use BaseModel validation with schema\n        try:\n            processed_schema = self._preprocess_schema(self.output_schema)\n            output_model = build_model_from_schema(processed_schema)\n\n            # Validate against the schema\n            if isinstance(json_data, list):\n                # Multiple objects\n                validated_objects = []\n                for item in json_data:\n                    try:\n                        validated_obj = output_model.model_validate(item)\n                        validated_objects.append(validated_obj.model_dump())\n                    except ValidationError as e:\n                        await logger.aerror(f\"Validation error for item: {e}\")\n                        # Include invalid items with error info\n                        validated_objects.append({\"data\": item, \"validation_error\": str(e)})\n                return validated_objects\n\n            # Single object\n            try:\n                validated_obj = output_model.model_validate(json_data)\n                return [validated_obj.model_dump()]  # Return as list for consistency\n            except ValidationError as e:\n                await logger.aerror(f\"Validation error: {e}\")\n                return [{\"data\": json_data, \"validation_error\": str(e)}]\n\n        except (TypeError, ValueError) as e:\n            await logger.aerror(f\"Error building structured output: {e}\")\n            # Fallback to parsed JSON without validation\n            return json_data\n\n    async def json_response(self) -> Data:\n        \"\"\"Convert agent response to structured JSON Data output with schema validation.\"\"\"\n        # Always use structured chat agent for JSON response mode for better JSON formatting\n        try:\n            system_components = []\n\n            # 1. Agent Instructions (system_prompt)\n            agent_instructions = getattr(self, \"system_prompt\", \"\") or \"\"\n            if agent_instructions:\n                system_components.append(f\"{agent_instructions}\")\n\n            # 2. Format Instructions\n            format_instructions = getattr(self, \"format_instructions\", \"\") or \"\"\n            if format_instructions:\n                system_components.append(f\"Format instructions: {format_instructions}\")\n\n            # 3. Schema Information from BaseModel\n            if hasattr(self, \"output_schema\") and self.output_schema and len(self.output_schema) > 0:\n                try:\n                    processed_schema = self._preprocess_schema(self.output_schema)\n                    output_model = build_model_from_schema(processed_schema)\n                    schema_dict = output_model.model_json_schema()\n                    schema_info = (\n                        \"You are given some text that may include format instructions, \"\n                        \"explanations, or other content alongside a JSON schema.\\n\\n\"\n                        \"Your task:\\n\"\n                        \"- Extract only the JSON schema.\\n\"\n                        \"- Return it as valid JSON.\\n\"\n                        \"- Do not include format instructions, explanations, or extra text.\\n\\n\"\n                        \"Input:\\n\"\n                        f\"{json.dumps(schema_dict, indent=2)}\\n\\n\"\n                        \"Output (only JSON schema):\"\n                    )\n                    system_components.append(schema_info)\n                except (ValidationError, ValueError, TypeError, KeyError) as e:\n                    await logger.aerror(f\"Could not build schema for prompt: {e}\", exc_info=True)\n\n            # Combine all components\n            combined_instructions = \"\\n\\n\".join(system_components) if system_components else \"\"\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=combined_instructions,\n            )\n\n            # Create and run structured chat agent\n            try:\n                structured_agent = self.create_agent_runnable()\n            except (NotImplementedError, ValueError, TypeError) as e:\n                await logger.aerror(f\"Error with structured chat agent: {e}\")\n                raise\n            try:\n                result = await self.run_agent(structured_agent)\n            except (\n                ExceptionWithMessageError,\n                ValueError,\n                TypeError,\n                RuntimeError,\n            ) as e:\n                await logger.aerror(f\"Error with structured agent result: {e}\")\n                raise\n            # Extract content from structured agent result\n            if hasattr(result, \"content\"):\n                content = result.content\n            elif hasattr(result, \"text\"):\n                content = result.text\n            else:\n                content = str(result)\n\n        except (\n            ExceptionWithMessageError,\n            ValueError,\n            TypeError,\n            NotImplementedError,\n            AttributeError,\n        ) as e:\n            await logger.aerror(f\"Error with structured chat agent: {e}\")\n            # Fallback to regular agent\n            content_str = \"No content returned from agent\"\n            return Data(data={\"content\": content_str, \"error\": str(e)})\n\n        # Process with structured output validation\n        try:\n            structured_output = await self.build_structured_output_base(content)\n\n            # Handle different output formats\n            if isinstance(structured_output, list) and structured_output:\n                if len(structured_output) == 1:\n                    return Data(data=structured_output[0])\n                return Data(data={\"results\": structured_output})\n            if isinstance(structured_output, dict):\n                return Data(data=structured_output)\n            return Data(data={\"content\": content})\n\n        except (ValueError, TypeError) as e:\n            await logger.aerror(f\"Error in structured output processing: {e}\")\n            return Data(data={\"content\": content, \"error\": str(e)})\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(\n                session_id=self.graph.session_id,\n                order=\"Ascending\",\n                n_messages=self.n_messages,\n            )\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    async def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except (AttributeError, ValueError, TypeError, RuntimeError) as e:\n            await logger.aerror(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            # Filter out json_mode and only use attributes that exist on this component\n            model_kwargs = {}\n            for input_ in inputs:\n                if hasattr(self, f\"{prefix}{input_.name}\"):\n                    model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n                build_config[\"agent_llm\"][\"display_name\"] = \"Model Provider\"\n            elif field_value == \"connect_other_models\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    info=\"The provider of the language model that the agent will use to generate responses.\",\n                    options=[*MODEL_PROVIDERS_LIST],\n                    real_time_refresh=True,\n                    refresh_button=False,\n                    input_types=[\"LanguageModel\"],\n                    placeholder=\"Awaiting model input.\",\n                    options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST],\n                    external_options={\n                        \"fields\": {\n                            \"data\": {\n                                \"node\": {\n                                    \"name\": \"connect_other_models\",\n                                    \"display_name\": \"Connect other models\",\n                                    \"icon\": \"CornerDownLeft\",\n                                },\n                            }\n                        },\n                    },\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name = field_name.replace(prefix, \"\")\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = _get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\",\n            tool_description=description,\n            callbacks=self.get_langchain_callbacks(),\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n        return tools\n"
              },
              "format_instructions": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Output Format Instructions",
                "dynamic": false,
                "info": "Generic Template for structured output formatting. Valid only with Structured response.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "format_instructions",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are an AI that extracts structured JSON objects from unstructured text. Use a predefined schema with expected types (str, int, float, bool, dict). Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. Fill missing or ambiguous values with defaults: null for missing values. Remove exact duplicates but keep variations that have different field values. Always return valid JSON in the expected format, never throw errors. If multiple objects can be extracted, return them all in the structured format."
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 15
              },
              "max_retries": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Retries",
                "dynamic": false,
                "info": "The maximum number of retries to make when generating.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_retries",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 5
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "Additional keyword arguments to pass to the model.",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": true,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "To see the model names, first choose a provider. Then, enter your API key and click the refresh button next to the model name.",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4.1",
                  "gpt-4.1-mini",
                  "gpt-4.1-nano",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-5",
                  "gpt-5-mini",
                  "gpt-5-nano",
                  "gpt-5-chat-latest",
                  "o1",
                  "o3-mini",
                  "o3",
                  "o3-pro",
                  "o4-mini",
                  "o4-mini-high"
                ],
                "options_metadata": [],
                "placeholder": "",
                "real_time_refresh": false,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "gpt-4o-mini"
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "openai_api_base": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "OpenAI API Base",
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_api_base",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "output_schema": {
                "_input_type": "TableInput",
                "advanced": true,
                "display_name": "Output Schema",
                "dynamic": false,
                "info": "Schema Validation: Define the structure and data types for structured output. No validation if no output schema.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "output_schema",
                "placeholder": "",
                "required": false,
                "show": true,
                "table_icon": "Table",
                "table_schema": {
                  "columns": [
                    {
                      "default": "field",
                      "description": "Specify the name of the output field.",
                      "disable_edit": false,
                      "display_name": "Name",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "name",
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": "description of field",
                      "description": "Describe the purpose of the output field.",
                      "disable_edit": false,
                      "display_name": "Description",
                      "edit_mode": "popover",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "description",
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": "str",
                      "description": "Indicate the data type of the output field (e.g., str, int, float, bool, dict).",
                      "disable_edit": false,
                      "display_name": "Type",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "type",
                      "options": [
                        "str",
                        "int",
                        "float",
                        "bool",
                        "dict"
                      ],
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": false,
                      "description": "Set to True if this output field should be a list of the specified type.",
                      "disable_edit": false,
                      "display_name": "As List",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "boolean",
                      "hidden": false,
                      "name": "multiple",
                      "sortable": true,
                      "type": "boolean"
                    }
                  ]
                },
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "trigger_icon": "Table",
                "trigger_text": "Open table",
                "type": "table",
                "value": []
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "placeholder": "",
                "range_spec": {
                  "max": 1,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "type": "slider",
                "value": 0.1
              },
              "timeout": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "The timeout for requests to OpenAI completion API.",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 700
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "id": "Agent-nChwD",
        "measured": {
          "height": 591,
          "width": 320
        },
        "position": {
          "x": 3787.2770217592733,
          "y": 1930.4609833527406
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioEmbeddingsComponent-h6IE7",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "LLM이 생성한 포렌식 태그 JSON을 기반으로 artifact_all을 조회하고, 결과를 forensic_keyword_results 테이블에 저장한 뒤 요약을 반환합니다. JSON에 'keywords' 배열이 있으면 type/lastwritetimestamp/description/tag 컬럼에 부분 일치로 우선 필터를 적용합니다.",
            "display_name": "Forensic Tag Query & Save",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_value",
              "table_name",
              "result_table",
              "reset_table",
              "debug_sql"
            ],
            "frozen": false,
            "icon": "Database",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Summary",
                "group_outputs": false,
                "hidden": null,
                "method": "run",
                "name": "summary",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\r\nfrom typing import Dict, Any, Tuple, List\r\n\r\nimport psycopg2\r\nfrom psycopg2.extras import DictCursor\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import HandleInput, StrInput, BoolInput, Output\r\nfrom langflow.schema.message import Message\r\n\r\n\r\n# ========================================\r\n# 0. DB 설정 & 공통 상수\r\n# ========================================\r\n\r\nDB_CONFIG = {\r\n    \"host\": \"localhost\",\r\n    \"dbname\": \"forensic_db\",\r\n    \"user\": \"postgres\",\r\n    \"password\": \"admin123\",\r\n}\r\n\r\n# 결과 저장 테이블\r\nRESULT_TABLE_DEFAULT = \"forensic_keyword_results\"\r\n\r\n# LLM 태그 카테고리\r\nCATEGORY_KEYS = [\r\n    \"ARTIFACT\",\r\n    \"EVENT\",\r\n    \"AREA\",\r\n    \"SEC\",\r\n    \"FORMAT\",\r\n    \"ACT\",\r\n    \"TIME\",\r\n    \"STATE\",\r\n]\r\n\r\n# 키워드를 어디에 적용할지 (부분 일치 ILIKE)\r\n# ➜ [변경] 키워드는 type, lastwritetimestamp, description, tag 전체에서 검색\r\nKEYWORD_SEARCH_COLUMNS = [\"type\", \"lastwritetimestamp\", \"description\", \"tag\"]\r\n\r\n# tag 문자열을 배열로 바꿀 때 사용할 공통 표현식\r\n# - 공백 제거 후\r\n# - '|' 또는 ',' 를 기준으로 split\r\nTAG_ARRAY_EXPR = \"regexp_split_to_array(replace(tag, ' ', ''), '[|,]')\"\r\n\r\n\r\n# ========================================\r\n# 1. 태그 / 시간 / 키워드 정규화 + SQL 빌더\r\n# ========================================\r\n\r\ndef normalize_tags(llm_result: Dict[str, Any]) -> Dict[str, list]:\r\n    \"\"\"\r\n    LLM 응답에서 태그 부분만 표준화해서 꺼내는 함수.\r\n\r\n    지원 형식 1) 새 형식:\r\n        {\r\n          \"tags\": {\r\n            \"ARTIFACT\": [...],\r\n            \"EVENT\": [...],\r\n            ...\r\n          },\r\n          ...\r\n        }\r\n\r\n    지원 형식 2) 옛 형식:\r\n        {\r\n          \"ARTIFACT\": [...],\r\n          \"EVENT\": [...],\r\n          ...\r\n        }\r\n\r\n    둘 중 어떤 형식이든 받아서 항상:\r\n        {\r\n          \"ARTIFACT\": [...],\r\n          \"EVENT\": [...],\r\n          ...\r\n        }\r\n    형태로 맞춰서 반환.\r\n    \"\"\"\r\n    if \"tags\" in llm_result and isinstance(llm_result[\"tags\"], dict):\r\n        tags = llm_result[\"tags\"]\r\n        return {cat: tags.get(cat, []) or [] for cat in CATEGORY_KEYS}\r\n\r\n    tags: Dict[str, list] = {}\r\n    for cat in CATEGORY_KEYS:\r\n        values = llm_result.get(cat, []) or []\r\n        if not isinstance(values, list):\r\n            values = []\r\n        tags[cat] = values\r\n    return tags\r\n\r\n\r\ndef normalize_time_filter(llm_result: Dict[str, Any]) -> Dict[str, Any]:\r\n    \"\"\"\r\n    LLM 응답에서 시간 필터 정보를 표준화.\r\n\r\n    - 새 형식: time_parsed\r\n        {\r\n          \"time_parsed\": {\r\n            \"type\": \"none\" | \"single\" | \"range\",\r\n            \"start\": \"YYYY-MM-DD\" or null,\r\n            \"end\":   \"YYYY-MM-DD\" or null\r\n          }\r\n        }\r\n\r\n    - 옛 형식: time_filter (기존 로직 유지)\r\n        {\r\n          \"time_filter\": {\r\n            \"field\": \"lastwritetimestamp\",\r\n            \"from\": \"...\",\r\n            \"to\": \"...\",\r\n            \"original_text\": \"...\",\r\n            \"mode\": \"none\" | \"day\" | \"range\" | \"point\"\r\n          }\r\n        }\r\n\r\n    반환 형식(내부 공통):\r\n        {\r\n          \"field\": \"lastwritetimestamp\",\r\n          \"from\": <str or None>,\r\n          \"to\": <str or None>,\r\n          \"original_text\": <str or None>,\r\n          \"mode\": \"none\" | \"single\" | \"range\" | \"day\" | \"point\"\r\n        }\r\n    \"\"\"\r\n    # 기본값\r\n    field = \"lastwritetimestamp\"\r\n    mode = \"none\"\r\n    t_from = None\r\n    t_to = None\r\n    original_text = None\r\n\r\n    # 1) 새 형식: time_parsed 우선 사용\r\n    tp = llm_result.get(\"time_parsed\")\r\n    if isinstance(tp, dict):\r\n        ttype = str(tp.get(\"type\") or \"none\").lower()\r\n        start = tp.get(\"start\")\r\n        end = tp.get(\"end\")\r\n\r\n        # 날짜가 제대로 들어온 경우에만 필터로 사용\r\n        if ttype in (\"single\", \"range\") and isinstance(start, str) and isinstance(end, str):\r\n            start = start.strip()\r\n            end = end.strip()\r\n            if start and end:\r\n                # YYYY-MM-DD 형식이면 하루 범위로 확장\r\n                if len(start) == 10 and len(end) == 10:\r\n                    t_from = f\"{start} 00:00:00\"\r\n                    t_to = f\"{end} 23:59:59\"\r\n                else:\r\n                    # 이미 시간까지 포함돼 있으면 그대로 사용\r\n                    t_from = start\r\n                    t_to = end\r\n                mode = ttype\r\n\r\n        return {\r\n            \"field\": field,\r\n            \"from\": t_from,\r\n            \"to\": t_to,\r\n            \"original_text\": original_text,\r\n            \"mode\": mode,\r\n        }\r\n\r\n    # 2) 옛 형식: time_filter가 있으면 기존 방식 그대로\r\n    tf = llm_result.get(\"time_filter\")\r\n    if isinstance(tf, dict):\r\n        return {\r\n            \"field\": tf.get(\"field\", field),\r\n            \"from\": tf.get(\"from\"),\r\n            \"to\": tf.get(\"to\"),\r\n            \"original_text\": tf.get(\"original_text\"),\r\n            \"mode\": (tf.get(\"mode\") or \"none\").lower(),\r\n        }\r\n\r\n    # 3) 둘 다 없으면 시간 필터 없음\r\n    return {\r\n        \"field\": field,\r\n        \"from\": None,\r\n        \"to\": None,\r\n        \"original_text\": None,\r\n        \"mode\": \"none\",\r\n    }\r\n\r\n\r\ndef normalize_keywords(llm_result: Dict[str, Any]) -> List[str]:\r\n    \"\"\"\r\n    LLM 응답에서 keywords 배열을 꺼내서 정리한다.\r\n\r\n    기대 형식:\r\n        \"keywords\": [\"malware.exe\", \"shellbag\", ...]\r\n    또는 실수로 문자열 하나만 있을 수도 있으니 방어.\r\n    \"\"\"\r\n    kw = llm_result.get(\"keywords\", []) or []\r\n\r\n    if isinstance(kw, str):\r\n        kw = [kw]\r\n\r\n    result: List[str] = []\r\n    for item in kw:\r\n        s = str(item).strip()\r\n        if s:\r\n            result.append(s)\r\n    return result\r\n\r\n\r\ndef build_sql_from_llm(\r\n    llm_result: Dict[str, Any],\r\n    table_name: str = \"artifact_all\",\r\n    columns: str = \"type,lastwritetimestamp,description,tag\",\r\n) -> Tuple[str, Dict[str, Any]]:\r\n\r\n    import re\r\n\r\n    tags = normalize_tags(llm_result)\r\n    time_filter = normalize_time_filter(llm_result)\r\n    keywords = normalize_keywords(llm_result)\r\n\r\n    select_sql = f\"\"\"\r\nSELECT\r\n  {columns}\r\nFROM {table_name}\r\n\"\"\"\r\n\r\n    where_clauses: List[str] = []\r\n    params: Dict[str, Any] = {}\r\n\r\n    # -------------------------\r\n    # 0) KEYWORDS\r\n    #  - eventlog_<digits> 키워드는 type에서만 정확 매칭\r\n    #  - 나머지 키워드는 기존처럼 부분일치(ILIKE)\r\n    # -------------------------\r\n    eventlog_re = re.compile(r\"^eventlog_\\d+$\", re.IGNORECASE)\r\n\r\n    eventlog_type_keywords: List[str] = []\r\n    fuzzy_keywords: List[str] = []\r\n\r\n    for kw in keywords:\r\n        if eventlog_re.match(kw):\r\n            eventlog_type_keywords.append(kw.lower())\r\n        else:\r\n            fuzzy_keywords.append(kw)\r\n\r\n    # (A) eventlog_12 같은 키워드가 있으면 type 정확 일치로 강제\r\n    if eventlog_type_keywords:\r\n        params[\"eventlog_type_list\"] = eventlog_type_keywords\r\n        where_clauses.append(\r\n            \"LOWER(COALESCE(type::text, '')) = ANY(%(eventlog_type_list)s)\"\r\n        )\r\n\r\n    # (B) 그 외 키워드는 기존 로직(부분일치 OR)\r\n    if fuzzy_keywords:\r\n        kw_subconds: List[str] = []\r\n        for idx, kw in enumerate(fuzzy_keywords):\r\n            pname = f\"kw_{idx}\"\r\n            params[pname] = f\"%{kw}%\"\r\n\r\n            col_conds = [\r\n                f\"COALESCE({col}::text, '') ILIKE %({pname})s ESCAPE '@'\"\r\n                for col in KEYWORD_SEARCH_COLUMNS\r\n            ]\r\n            kw_subconds.append(\"(\" + \" OR \".join(col_conds) + \")\")\r\n\r\n        where_clauses.append(\"(\" + \" OR \".join(kw_subconds) + \")\")\r\n\r\n    # -------------------------\r\n    # 1) ARTIFACT: type 또는 tag 기준 강한 필터\r\n    # -------------------------\r\n    artifact_values = tags.get(\"ARTIFACT\", []) or []\r\n    if artifact_values:\r\n        params[\"artifact_list\"] = artifact_values\r\n        artifact_clause = (\r\n            \"(type = ANY(%(artifact_list)s) \"\r\n            f\"OR {TAG_ARRAY_EXPR} && %(artifact_list)s)\"\r\n        )\r\n        where_clauses.append(artifact_clause)\r\n\r\n    # -------------------------\r\n    # 2) soft 조건 (OR): EVENT / AREA / FORMAT / ACT / SEC / TIME / STATE\r\n    # -------------------------\r\n    soft_conditions: List[str] = []\r\n\r\n    event_values = tags.get(\"EVENT\", []) or []\r\n    if event_values:\r\n        params[\"event_list\"] = event_values\r\n        soft_conditions.append(f\"{TAG_ARRAY_EXPR} && %(event_list)s\")\r\n\r\n    area_values = tags.get(\"AREA\", []) or []\r\n    if area_values:\r\n        params[\"area_list\"] = area_values\r\n        soft_conditions.append(f\"{TAG_ARRAY_EXPR} && %(area_list)s\")\r\n\r\n    format_values = tags.get(\"FORMAT\", []) or []\r\n    if format_values:\r\n        params[\"format_list\"] = format_values\r\n        soft_conditions.append(f\"{TAG_ARRAY_EXPR} && %(format_list)s\")\r\n\r\n    act_values = tags.get(\"ACT\", []) or []\r\n    if act_values:\r\n        params[\"act_list\"] = act_values\r\n        soft_conditions.append(f\"{TAG_ARRAY_EXPR} && %(act_list)s\")\r\n\r\n    sec_values = tags.get(\"SEC\", []) or []\r\n    if sec_values:\r\n        params[\"sec_list\"] = sec_values\r\n        soft_conditions.append(f\"{TAG_ARRAY_EXPR} && %(sec_list)s\")\r\n\r\n    time_values = tags.get(\"TIME\", []) or []\r\n    if time_values:\r\n        params[\"time_list\"] = time_values\r\n        soft_conditions.append(f\"{TAG_ARRAY_EXPR} && %(time_list)s\")\r\n\r\n    state_values = tags.get(\"STATE\", []) or []\r\n    if state_values:\r\n        params[\"state_list\"] = state_values\r\n        soft_conditions.append(f\"{TAG_ARRAY_EXPR} && %(state_list)s\")\r\n\r\n    if soft_conditions:\r\n        where_clauses.append(\"(\" + \" OR \".join(soft_conditions) + \")\")\r\n\r\n    # -------------------------\r\n    # 3) (별도 적용) time_parsed/time_filter 기반 BETWEEN\r\n    # -------------------------\r\n    time_mode = time_filter.get(\"mode\", \"none\")\r\n    tf_from = time_filter.get(\"from\")\r\n    tf_to = time_filter.get(\"to\")\r\n\r\n    if time_mode != \"none\" and tf_from and tf_to:\r\n        where_clauses.append(\"lastwritetimestamp BETWEEN %(time_from)s AND %(time_to)s\")\r\n        params[\"time_from\"] = tf_from\r\n        params[\"time_to\"] = tf_to\r\n\r\n    if where_clauses:\r\n        where_sql = \"WHERE \" + \"\\n  AND \".join(where_clauses)\r\n    else:\r\n        where_sql = \"\"\r\n\r\n    order_sql = \"\\nORDER BY lastwritetimestamp;\"\r\n\r\n    full_sql = select_sql + where_sql + order_sql\r\n    full_sql = \"\\n\".join(\r\n        line.rstrip()\r\n        for line in full_sql.splitlines()\r\n        if line.strip() != \"\"\r\n    )\r\n\r\n    return full_sql, params\r\n\r\n\r\n\r\n# ========================================\r\n# 2. SQL 실행 + forensic_keyword_results 저장 로직\r\n# ========================================\r\n\r\ndef ensure_result_table(conn, table_name: str, reset: bool = False):\r\n    \"\"\"\r\n    결과 테이블이 없으면 생성하고,\r\n    reset=True 이면 내용 TRUNCATE.\r\n    \"\"\"\r\n    create_sql = f\"\"\"\r\n    CREATE TABLE IF NOT EXISTS {table_name} (\r\n        id                  SERIAL PRIMARY KEY,\r\n        type                TEXT,\r\n        lastwritetimestamp  TEXT,\r\n        description         TEXT,\r\n        tag                 TEXT\r\n    );\r\n    \"\"\"\r\n    with conn.cursor() as cur:\r\n        cur.execute(create_sql)\r\n        if reset:\r\n            cur.execute(f\"TRUNCATE TABLE {table_name};\")\r\n    conn.commit()\r\n\r\n\r\ndef summarize_result_table(conn, table_name: str, limit: int = 20) -> str:\r\n    \"\"\"\r\n    forensic_keyword_results 테이블의 전체 행 수 + 최근 일부 행 요약.\r\n    \"\"\"\r\n    lines: List[str] = []\r\n    with conn.cursor(cursor_factory=DictCursor) as cur:\r\n        cur.execute(f\"SELECT COUNT(*) AS cnt FROM {table_name};\")\r\n        cnt = cur.fetchone()[\"cnt\"]\r\n        lines.append(f\"결과 테이블 '{table_name}' 총 행 수: {cnt}\")\r\n\r\n        cur.execute(\r\n            f\"\"\"\r\n            SELECT id, type, lastwritetimestamp, description, tag\r\n            FROM {table_name}\r\n            ORDER BY id DESC\r\n            LIMIT %s;\r\n            \"\"\",\r\n            (limit,),\r\n        )\r\n        rows = cur.fetchall()\r\n\r\n    lines.append(f\"최근 {limit}개 결과 (id / type / time / tag / desc):\")\r\n    for r in rows:\r\n        header = (\r\n            f\"[id={r['id']}] type='{r['type']}' \"\r\n            f\"| time='{r['lastwritetimestamp']}' \"\r\n            f\"| tag='{r['tag']}'\"\r\n        )\r\n        body = f\"  desc='{r['description']}'\"\r\n        lines.append(header)\r\n        lines.append(body)\r\n\r\n    return \"\\n\".join(lines)\r\n\r\n\r\ndef run_tag_query_and_save(\r\n    llm_result: Dict[str, Any],\r\n    table_name: str = \"artifact_all\",\r\n    result_table: str = RESULT_TABLE_DEFAULT,\r\n    reset_table: bool = False,\r\n    debug_sql: bool = True,\r\n) -> str:\r\n    \"\"\"\r\n    핵심 로직:\r\n      - LLM 태그 JSON(dict)을 받아서\r\n      - artifact_all 에 대해 SELECT SQL 생성\r\n      - forensic_keyword_results 에 INSERT ... SELECT\r\n      - 요약 문자열 반환\r\n    \"\"\"\r\n    sql_select, params = build_sql_from_llm(\r\n        llm_result,\r\n        table_name=table_name,\r\n        columns=\"type,lastwritetimestamp,description,tag\",\r\n    )\r\n\r\n    insert_sql = (\r\n        f\"INSERT INTO {result_table} (type,lastwritetimestamp,description,tag)\\n\"\r\n        + sql_select\r\n    )\r\n\r\n    if debug_sql:\r\n        print(\"========== [ForensicTagQuery] Generated SELECT SQL ==========\")\r\n        print(sql_select)\r\n        print(\"---------- params ----------\")\r\n        print(json.dumps(params, ensure_ascii=False, indent=2))\r\n\r\n    conn = psycopg2.connect(**DB_CONFIG)\r\n    conn.autocommit = False\r\n\r\n    try:\r\n        ensure_result_table(conn, result_table, reset=reset_table)\r\n\r\n        with conn.cursor() as cur:\r\n            cur.execute(insert_sql, params)\r\n            inserted = cur.rowcount\r\n\r\n        conn.commit()\r\n\r\n        summary_text = summarize_result_table(conn, result_table, limit=20)\r\n\r\n        full_summary = [\r\n            f\"[INFO] INSERT된 행 수: {inserted}\",\r\n            \"\",\r\n            summary_text,\r\n        ]\r\n        return \"\\n\".join(full_summary)\r\n\r\n    except Exception as e:\r\n        conn.rollback()\r\n        return f\"[ERROR] run_tag_query_and_save 중 오류: {e}\"\r\n    finally:\r\n        conn.close()\r\n\r\n\r\n# ========================================\r\n# 3. LangFlow 커스텀 컴포넌트\r\n# ========================================\r\n\r\nclass ForensicTagQueryAndSave(Component):\r\n    \"\"\"\r\n    ForensicTagQueryAndSave\r\n\r\n    역할:\r\n      - LLM 노드에서 나온 Message/Data/Text 안의 JSON(태그)을 입력으로 받는다.\r\n      - JSON 파싱 → 태그 + 키워드 기반 SQL 생성 → artifact_all 검색\r\n      - forensic_keyword_results 테이블에 INSERT ... SELECT\r\n      - 결과 요약을 Message(text=...) 형태로 반환한다.\r\n\r\n    플로우 예시:\r\n      LLM (태그 추천) → ForensicTagQueryAndSave → ChatOutput\r\n    \"\"\"\r\n\r\n    display_name = \"Forensic Tag Query & Save\"\r\n    description = (\r\n        \"LLM이 생성한 포렌식 태그 JSON을 기반으로 artifact_all을 조회하고, \"\r\n        \"결과를 forensic_keyword_results 테이블에 저장한 뒤 요약을 반환합니다. \"\r\n        \"JSON에 'keywords' 배열이 있으면 type/lastwritetimestamp/description/tag 컬럼에 부분 일치로 우선 필터를 적용합니다.\"\r\n    )\r\n    icon = \"Database\"\r\n    name = \"ForensicTagQueryAndSave\"\r\n\r\n    inputs = [\r\n        HandleInput(\r\n            name=\"input_value\",\r\n            display_name=\"LLM Tag JSON\",\r\n            info=\"LLM이 출력한 태그 JSON이 들어 있는 Message/Data/Text.\",\r\n            input_types=[\"Message\", \"Data\", \"Text\"],\r\n            required=True,\r\n        ),\r\n        StrInput(\r\n            name=\"table_name\",\r\n            display_name=\"Source Table Name\",\r\n            info=\"검색을 수행할 테이블 이름 (기본: artifact_all).\",\r\n            required=False,\r\n            value=\"artifact_all\",\r\n        ),\r\n        StrInput(\r\n            name=\"result_table\",\r\n            display_name=\"Result Table Name\",\r\n            info=\"검색 결과를 저장할 테이블 이름.\",\r\n            required=False,\r\n            value=RESULT_TABLE_DEFAULT,\r\n        ),\r\n        BoolInput(\r\n            name=\"reset_table\",\r\n            display_name=\"Reset Result Table (TRUNCATE)\",\r\n            info=\"실행 전에 결과 테이블 내용을 모두 비울지 여부.\",\r\n            value=False,\r\n            required=False,\r\n        ),\r\n        BoolInput(\r\n            name=\"debug_sql\",\r\n            display_name=\"Print Debug SQL\",\r\n            info=\"콘솔에 생성된 SQL과 params를 출력할지 여부.\",\r\n            value=True,\r\n            required=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Summary\",\r\n            name=\"summary\",\r\n            method=\"run\",\r\n            output_type=\"Message\",\r\n        ),\r\n    ]\r\n\r\n    def _extract_text_from_input(self, input_value: Any) -> str:\r\n        if isinstance(input_value, str):\r\n            return input_value\r\n\r\n        if isinstance(input_value, dict):\r\n            for key in [\"text\", \"content\", \"data\", \"message\"]:\r\n                if key in input_value and isinstance(input_value[key], str):\r\n                    return input_value[key]\r\n            return json.dumps(input_value, ensure_ascii=False)\r\n\r\n        text_attr = getattr(input_value, \"text\", None)\r\n        if isinstance(text_attr, str):\r\n            return text_attr\r\n\r\n        return str(input_value)\r\n\r\n    def run(self, input_value: Any = None, **kwargs) -> Message:\r\n        raw = self._extract_text_from_input(\r\n            input_value if input_value is not None else getattr(self, \"input_value\", \"\")\r\n        )\r\n        raw = (raw or \"\").strip()\r\n\r\n        if not raw:\r\n            return Message(text=\"[ERROR] 입력이 비어 있어 태그 JSON을 파싱할 수 없습니다.\")\r\n\r\n        if raw.startswith(\"```\"):\r\n            first_brace = raw.find(\"{\")\r\n            last_brace = raw.rfind(\"}\")\r\n            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\r\n                raw = raw[first_brace:last_brace + 1].strip()\r\n\r\n        try:\r\n            llm_result = json.loads(raw)\r\n        except json.JSONDecodeError as e:\r\n            msg = (\r\n                \"[ERROR] 태그 JSON 파싱 실패: \"\r\n                + str(e)\r\n                + \"\\n\\nRAW INPUT (앞 200자):\\n\"\r\n                + repr(raw[:200])\r\n            )\r\n            return Message(text=msg)\r\n\r\n        table_name = getattr(self, \"table_name\", \"artifact_all\")\r\n        result_table = getattr(self, \"result_table\", RESULT_TABLE_DEFAULT)\r\n        reset_table = bool(getattr(self, \"reset_table\", False))\r\n        debug_sql = bool(getattr(self, \"debug_sql\", True))\r\n\r\n        summary_text = run_tag_query_and_save(\r\n            llm_result=llm_result,\r\n            table_name=table_name,\r\n            result_table=result_table,\r\n            reset_table=reset_table,\r\n            debug_sql=debug_sql,\r\n        )\r\n\r\n        return Message(text=summary_text)\r\n"
              },
              "debug_sql": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Print Debug SQL",
                "dynamic": false,
                "info": "콘솔에 생성된 SQL과 params를 출력할지 여부.",
                "list": false,
                "list_add_label": "Add More",
                "name": "debug_sql",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "LLM Tag JSON",
                "dynamic": false,
                "info": "LLM이 출력한 태그 JSON이 들어 있는 Message/Data/Text.",
                "input_types": [
                  "Message",
                  "Data",
                  "Text"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "reset_table": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Reset Result Table (TRUNCATE)",
                "dynamic": false,
                "info": "실행 전에 결과 테이블 내용을 모두 비울지 여부.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "reset_table",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "result_table": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Result Table Name",
                "dynamic": false,
                "info": "검색 결과를 저장할 테이블 이름.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "result_table",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "forensic_keyword_results"
              },
              "table_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Source Table Name",
                "dynamic": false,
                "info": "검색을 수행할 테이블 이름 (기본: artifact_all).",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "table_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "artifact_all"
              }
            },
            "tool_mode": false
          },
          "selected_output": "sql",
          "showNode": true,
          "type": "ForensicTagQueryAndSave"
        },
        "dragging": false,
        "id": "LMStudioEmbeddingsComponent-h6IE7",
        "measured": {
          "height": 493,
          "width": 320
        },
        "position": {
          "x": -3179.281723454533,
          "y": 2594.691176877385
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-b6cWX",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/components-io#chat-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _icon, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-b6cWX",
        "measured": {
          "height": 165,
          "width": 320
        },
        "position": {
          "x": -1019.9143647743768,
          "y": 2446.9593161369457
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Reranker-VnRFH",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Rerank documents using LM Studio embeddings (api/v0).",
            "display_name": "Reranker",
            "documentation": "",
            "edited": true,
            "field_order": [
              "docs",
              "query",
              "top_k"
            ],
            "frozen": false,
            "icon": "sort",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output",
                "group_outputs": false,
                "hidden": null,
                "method": "rerank",
                "name": "output",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import requests\r\nimport numpy as np\r\nimport json\r\nimport logging\r\nimport re\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import MessageTextInput, Output, IntInput\r\nfrom langflow.schema.data import Data\r\n\r\n# 로깅 설정\r\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s [%(levelname)s] %(message)s\")\r\n\r\n\r\nclass Reranker(Component):\r\n    display_name = \"Reranker\"\r\n    description = \"Rerank documents using LM Studio embeddings (api/v0).\"\r\n    icon = \"sort\"\r\n    name = \"Reranker\"\r\n\r\n    # ===== 입력 =====\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"docs\",\r\n            display_name=\"Documents\",\r\n            info=\"List of docs (string list or list of JSON objects with 'text').\",\r\n            value=\"[]\",\r\n            tool_mode=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"query\",\r\n            display_name=\"Query\",\r\n            info=\"Search query text\",\r\n            value=\"칼바람\",\r\n            tool_mode=True,\r\n        ),\r\n        IntInput(\r\n            name=\"top_k\",\r\n            display_name=\"Number of Results\",\r\n            info=\"상위 몇 개 문서를 반환할지 지정합니다.\",\r\n            value=50,   # 기본값은 50개\r\n        ),\r\n    ]\r\n\r\n    # ===== 출력 =====\r\n    outputs = [\r\n        Output(display_name=\"Output\", name=\"output\", method=\"rerank\"),\r\n    ]\r\n\r\n    # ===== Embedding 호출 =====\r\n    def get_embedding(self, text: str, model: str = \"text-embedding-bge-reranker-v2-m3\"):\r\n        url = \"http://127.0.0.1:1234/api/v0/embeddings\"\r\n        payload = {\"model\": model, \"input\": [text]}\r\n        response = requests.post(url, json=payload)\r\n        response.raise_for_status()\r\n        result = response.json()\r\n        return np.array(result[\"data\"][0][\"embedding\"], dtype=np.float32)\r\n\r\n    def cosine_similarity(self, a, b):\r\n        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\r\n\r\n    # ===== Rerank 로직 =====\r\n    def rerank(self) -> Data:\r\n        docs = self.docs\r\n        query_text = self.query if hasattr(self, \"query\") else \"query\"\r\n        top_k = int(self.top_k) if hasattr(self, \"top_k\") else 50\r\n\r\n        logging.debug(\"Raw docs input: %s\", str(docs)[:500])\r\n\r\n        # 문자열로 들어왔을 때 여러 JSON 블록(````json ... ````) 분리\r\n        if isinstance(docs, str):\r\n            parts = re.split(r\"```json|```\", docs)\r\n            docs = []\r\n            for part in parts:\r\n                part = part.strip()\r\n                if not part:\r\n                    continue\r\n                try:\r\n                    parsed = json.loads(part)\r\n                    docs.append(parsed)\r\n                except Exception:\r\n                    logging.debug(\"Skip non-JSON part: %s\", part[:100])\r\n        elif isinstance(docs, list):\r\n            pass\r\n        else:\r\n            docs = [docs]\r\n\r\n        logging.debug(\"Parsed %d documents\", len(docs))\r\n\r\n        # 쿼리 임베딩\r\n        query_emb = self.get_embedding(query_text)\r\n\r\n        reranked_docs = []\r\n        for i, doc in enumerate(docs):\r\n            if isinstance(doc, dict):\r\n                text = doc.get(\"text\", \"\")\r\n            else:\r\n                text = str(doc)\r\n\r\n            if not text.strip():\r\n                continue\r\n\r\n            logging.debug(\"Processing doc #%d text preview: %s\", i, text[:200])\r\n\r\n            doc_emb = self.get_embedding(text)\r\n            score = self.cosine_similarity(query_emb, doc_emb)\r\n\r\n            reranked_docs.append({\r\n                \"text\": text,\r\n                \"score\": score\r\n            })\r\n\r\n        # 스코어 높은 순으로 정렬\r\n        reranked_docs.sort(key=lambda x: x[\"score\"], reverse=True)\r\n\r\n        # top_k 만큼만 선택\r\n        top_n = reranked_docs[:top_k]\r\n\r\n        logging.debug(\"Final top_n docs count=%d\", len(top_n))\r\n\r\n        # 최종 출력은 text만\r\n        only_texts = [{\"text\": doc[\"text\"]} for doc in top_n]\r\n\r\n        return Data(value=only_texts)\r\n"
              },
              "docs": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Documents",
                "dynamic": false,
                "info": "List of docs (string list or list of JSON objects with 'text').",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "docs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "query": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Query",
                "dynamic": false,
                "info": "Search query text",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "query",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "top_k": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "상위 몇 개 문서를 반환할지 지정합니다.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "top_k",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Reranker"
        },
        "dragging": false,
        "id": "Reranker-VnRFH",
        "measured": {
          "height": 383,
          "width": 320
        },
        "position": {
          "x": -1240.028687271644,
          "y": 1930.029340347079
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-gjeP1",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "context",
                "question"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "method": "build_prompt",
                "name": "prompt",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "context": {
                "advanced": false,
                "display_name": "context",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "context",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nQuestion: {question}\n\nAnswer: "
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "id": "Prompt-gjeP1",
        "measured": {
          "height": 447,
          "width": 320
        },
        "position": {
          "x": -801.8982891270673,
          "y": 1916.5362959999193
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-YtP0V",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Send text to a local Ollama model via /api/generate and return the response.",
            "display_name": "Ollama Generate",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_message",
              "node_prompt",
              "system_message",
              "model_name",
              "base_url",
              "stream"
            ],
            "frozen": false,
            "icon": "Ollama",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "hidden": null,
                "method": "run",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Ollama /api/generate 엔드포인트 URL.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:11434/api/generate"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import requests\r\nfrom typing import Any\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import (\r\n    MessageInput,\r\n    Output,\r\n    StrInput,\r\n    BoolInput,\r\n    MultilineInput,  # ✅ 여러 줄 프롬프트용\r\n)\r\nfrom langflow.schema.message import Message\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass OllamaGenerate(Component):\r\n    \"\"\"\r\n    Chat Input 같은 이전 노드에서 받은 메시지 + 노드 안에서 쓴 프롬프트를\r\n    Ollama /api/generate 로 보내고,\r\n    응답 텍스트를 다시 Message로 돌려주는 컴포넌트.\r\n    \"\"\"\r\n\r\n    display_name = \"Ollama Generate\"\r\n    description = \"Send text to a local Ollama model via /api/generate and return the response.\"\r\n    icon = \"Ollama\"\r\n    name = \"OllamaGenerate\"\r\n\r\n    # -------- 입력 정의 --------\r\n    inputs = [\r\n        # 1) 앞 노드에서 들어오는 메인 프롬프트 (Message / Text 둘 다 허용)\r\n        MessageInput(\r\n            name=\"input_message\",\r\n            display_name=\"Input\",\r\n            info=\"이전 노드에서 받은 텍스트/Message. 그대로 Ollama 프롬프트로 사용됩니다.\",\r\n            input_types=[\"Message\", \"Text\"],\r\n            required=True,\r\n        ),\r\n        # 2) 노드 안에서 직접 쓰는 추가 프롬프트 (여러 줄 가능)\r\n        MultilineInput(\r\n            name=\"node_prompt\",\r\n            display_name=\"Node Prompt\",\r\n            info=\"노드 안에서 직접 입력하는 추가 프롬프트. Chat 입력과 함께 합쳐집니다.\",\r\n            value=\"\",\r\n        ),\r\n        # 3) 선택적인 시스템 메시지 (역할 지정용)\r\n        StrInput(\r\n            name=\"system_message\",\r\n            display_name=\"System Message\",\r\n            info=\"모델에게 주고 싶은 지시(system prompt). 비워두면 사용하지 않습니다.\",\r\n            advanced=True,\r\n            value=\"\",\r\n        ),\r\n        # 4) 모델 이름\r\n        StrInput(\r\n            name=\"model_name\",\r\n            display_name=\"Model Name\",\r\n            info=\"ollama list 에 표시되는 모델 이름 그대로 입력하세요.\",\r\n            # ⚠️ 네 ollama 환경에서 실제 이름 확인해서 필요하면 바꿔줘\r\n            value=\"ogito-2.1:671b-cloud\",\r\n        ),\r\n        # 5) Ollama /api/generate URL\r\n        StrInput(\r\n            name=\"base_url\",\r\n            display_name=\"Base URL\",\r\n            info=\"Ollama /api/generate 엔드포인트 URL.\",\r\n            value=\"http://localhost:11434/api/generate\",\r\n        ),\r\n        # 6) 스트리밍 여부\r\n        BoolInput(\r\n            name=\"stream\",\r\n            display_name=\"Stream\",\r\n            info=\"Ollama 스트리밍 응답 사용 여부. LangFlow에서는 False 권장.\",\r\n            value=False,\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    # -------- 출력 정의 --------\r\n    outputs = [\r\n        Output(\r\n            name=\"response\",\r\n            display_name=\"Model Response\",\r\n            type_=Message,   # Chat Output이 바로 받을 수 있는 타입\r\n            method=\"run\",\r\n        )\r\n    ]\r\n\r\n    # -------- 실제 실행 로직 --------\r\n    def run(self) -> Message:\r\n        \"\"\"\r\n        1) self.input_message 에서 텍스트를 꺼내고\r\n        2) self.node_prompt, self.system_message 와 합쳐서 최종 prompt 생성\r\n        3) Ollama /api/generate 로 HTTP 요청을 보내고\r\n        4) 응답 텍스트를 Message(text=...) 로 감싸서 반환\r\n        \"\"\"\r\n\r\n        # ----- 1) 입력 메시지에서 텍스트 꺼내기 -----\r\n        raw_input = self.input_message\r\n        print(\"[OllamaGenerate] raw input_message:\", repr(raw_input), type(raw_input))\r\n\r\n        if isinstance(raw_input, Message):\r\n            user_text = raw_input.text or \"\"\r\n        elif isinstance(raw_input, Data):\r\n            try:\r\n                user_text = str(raw_input.data or \"\")\r\n            except Exception:\r\n                user_text = \"\"\r\n        elif isinstance(raw_input, str):\r\n            user_text = raw_input\r\n        else:\r\n            try:\r\n                user_text = str(raw_input) if raw_input is not None else \"\"\r\n            except Exception:\r\n                user_text = \"\"\r\n\r\n        node_prompt = self.node_prompt or \"\"\r\n        system_message = self.system_message or \"\"\r\n\r\n        # ----- 2) system_message / node_prompt / user_text 합쳐서 최종 프롬프트 만들기 -----\r\n        parts = []\r\n        if system_message.strip():\r\n            parts.append(system_message.strip())\r\n        if node_prompt.strip():\r\n            parts.append(node_prompt.strip())\r\n        if user_text.strip():\r\n            parts.append(user_text.strip())\r\n\r\n        prompt = \"\\n\\n\".join(parts)\r\n\r\n        if not prompt:\r\n            print(\"[OllamaGenerate] 빈 프롬프트가 전달되었습니다.\")\r\n            return Message(text=\"[OllamaGenerate] 빈 프롬프트입니다.\")\r\n\r\n        # ----- 3) Ollama 설정값 읽기 -----\r\n        model_name = self.model_name or \"ogito-2.1:671b-cloud\"\r\n        base_url = self.base_url or \"http://localhost:11434/api/generate\"\r\n        stream = bool(self.stream)\r\n\r\n        payload = {\r\n            \"model\": model_name,\r\n            \"prompt\": prompt,\r\n            \"stream\": stream,\r\n        }\r\n\r\n        print(\"[OllamaGenerate] 요청 payload:\", payload)\r\n\r\n        # ----- 4) Ollama /api/generate 호출 -----\r\n        res = requests.post(base_url, json=payload)\r\n\r\n        if res.status_code != 200:\r\n            print(\"[OllamaGenerate] HTTP Status:\", res.status_code)\r\n            print(\"[OllamaGenerate] Body:\", res.text)\r\n            res.raise_for_status()\r\n\r\n        data = res.json()\r\n        print(\"[OllamaGenerate] 응답 JSON:\", data)\r\n\r\n        # ----- 5) 응답 텍스트 뽑기 -----\r\n        answer_text = \"\"\r\n        if isinstance(data, dict):\r\n            # Ollama 기본: {\"response\": \"...\", \"done\": true}\r\n            answer_text = (\r\n                data.get(\"response\")\r\n                or data.get(\"output\")\r\n                or data.get(\"message\")\r\n                or data.get(\"content\")\r\n                or \"\"\r\n            )\r\n        else:\r\n            answer_text = str(data)\r\n\r\n        if not answer_text:\r\n            answer_text = \"[OllamaGenerate] 응답에 텍스트 필드를 찾지 못했습니다:\\n{}\".format(\r\n                data\r\n            )\r\n\r\n        return Message(text=answer_text)\r\n"
              },
              "input_message": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "이전 노드에서 받은 텍스트/Message. 그대로 Ollama 프롬프트로 사용됩니다.",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_message",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "model_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Model Name",
                "dynamic": false,
                "info": "ollama list 에 표시되는 모델 이름 그대로 입력하세요.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "model_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "cogito-2.1:671b-cloud"
              },
              "node_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Node Prompt",
                "dynamic": false,
                "info": "노드 안에서 직접 입력하는 추가 프롬프트. Chat 입력과 함께 합쳐집니다.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "node_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Ollama 스트리밍 응답 사용 여부. LangFlow에서는 False 권장.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "System Message",
                "dynamic": false,
                "info": "모델에게 주고 싶은 지시(system prompt). 비워두면 사용하지 않습니다.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "OllamaGenerate"
        },
        "dragging": false,
        "id": "LMStudioModel-YtP0V",
        "measured": {
          "height": 465,
          "width": 320
        },
        "position": {
          "x": -134.15774235109686,
          "y": 1774.3541309739512
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-VBIsB",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "wire_in",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "last_updated": "2025-12-16T16:19:29.359Z",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "hidden": null,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "hidden": null,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\r\nfrom urllib.parse import urljoin\r\n\r\nimport httpx\r\nfrom langchain_openai import ChatOpenAI\r\nfrom typing_extensions import override\r\n\r\nfrom langflow.base.models.model import LCModelComponent\r\nfrom langflow.field_typing import LanguageModel\r\nfrom langflow.field_typing.range_spec import RangeSpec\r\nfrom langflow.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\r\n\r\n# ✅ \"연결용 포트\"를 만들기 위한 HandleInput (버전별 import 차이 대응)\r\ntry:\r\n    from langflow.io import HandleInput\r\nexcept Exception:\r\n    from langflow.inputs.inputs import HandleInput\r\n\r\n\r\nclass LMStudioModelComponent(LCModelComponent):\r\n    display_name = \"LM Studio\"\r\n    description = \"Generate text using LM Studio Local LLMs.\"\r\n    icon = \"LMStudio\"\r\n    name = \"LMStudioModel\"\r\n\r\n    @override\r\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\r\n        if field_name == \"model_name\":\r\n            base_url_dict = build_config.get(\"base_url\", {})\r\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\r\n            base_url_value = base_url_dict.get(\"value\")\r\n            if base_url_load_from_db:\r\n                base_url_value = await self.get_variables(base_url_value, field_name)\r\n            elif not base_url_value:\r\n                base_url_value = \"http://localhost:1234/v1\"\r\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\r\n\r\n        return build_config\r\n\r\n    @staticmethod\r\n    async def get_model(base_url_value: str) -> list[str]:\r\n        try:\r\n            url = urljoin(base_url_value, \"/v1/models\")\r\n            async with httpx.AsyncClient() as client:\r\n                response = await client.get(url)\r\n                response.raise_for_status()\r\n                data = response.json()\r\n                return [model[\"id\"] for model in data.get(\"data\", [])]\r\n        except Exception as e:\r\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\r\n            raise ValueError(msg) from e\r\n\r\n    inputs = [\r\n        *LCModelComponent._base_inputs,\r\n\r\n        # ✅ 여기! “연결용 입력 포트(핸들)” 추가\r\n        HandleInput(\r\n            name=\"wire_in\",\r\n            display_name=\"Wire In (ignored)\",\r\n            input_types=[\"Message\", \"Text\", \"Data\", \"str\"],\r\n            info=\"연결(의존성)만 만들기 위한 더미 포트입니다. 들어오는 값은 무시합니다.\",\r\n            required=False,\r\n        ),\r\n\r\n        IntInput(\r\n            name=\"max_tokens\",\r\n            display_name=\"Max Tokens\",\r\n            advanced=True,\r\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\r\n            range_spec=RangeSpec(min=0, max=128000),\r\n        ),\r\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\r\n        DropdownInput(\r\n            name=\"model_name\",\r\n            display_name=\"Model Name\",\r\n            advanced=False,\r\n            refresh_button=True,\r\n        ),\r\n        StrInput(\r\n            name=\"base_url\",\r\n            display_name=\"Base URL\",\r\n            advanced=False,\r\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\r\n            value=\"http://localhost:1234/v1\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"api_key\",\r\n            display_name=\"LM Studio API Key\",\r\n            info=\"The LM Studio API Key to use for LM Studio.\",\r\n            advanced=True,\r\n            value=\"LMSTUDIO_API_KEY\",\r\n        ),\r\n        FloatInput(\r\n            name=\"temperature\",\r\n            display_name=\"Temperature\",\r\n            value=0.1,\r\n            advanced=True,\r\n        ),\r\n        IntInput(\r\n            name=\"seed\",\r\n            display_name=\"Seed\",\r\n            info=\"The seed controls the reproducibility of the job.\",\r\n            advanced=True,\r\n            value=1,\r\n        ),\r\n    ]\r\n\r\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\r\n        # ✅ wire_in은 완전히 무시 (연결만)\r\n        _ = getattr(self, \"wire_in\", None)\r\n\r\n        lmstudio_api_key = self.api_key\r\n        temperature = self.temperature\r\n        model_name: str = self.model_name\r\n        max_tokens = self.max_tokens\r\n        model_kwargs = self.model_kwargs or {}\r\n        base_url = self.base_url or \"http://localhost:1234/v1\"\r\n        seed = self.seed\r\n\r\n        return ChatOpenAI(\r\n            max_tokens=max_tokens or None,\r\n            model_kwargs=model_kwargs,\r\n            model=model_name,\r\n            base_url=base_url,\r\n            api_key=lmstudio_api_key,\r\n            temperature=temperature if temperature is not None else 0.1,\r\n            seed=seed,\r\n        )\r\n\r\n    def _get_exception_message(self, e: Exception):\r\n        try:\r\n            from openai import BadRequestError\r\n        except ImportError:\r\n            return None\r\n        if isinstance(e, BadRequestError):\r\n            message = e.body.get(\"message\")\r\n            if message:\r\n                return message\r\n        return None\r\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "model_name",
                "options": [
                  "text-embedding-sentence-transformers_all-minilm-l12-v2",
                  "qwen/qwen3-4b-thinking-2507",
                  "text-embedding-bge-reranker-v2-m3",
                  "forensicmistra.gguf",
                  "llama3.1-8b_forensicv0.1",
                  "text-embedding-nomic-embed-text-v1.5",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              },
              "wire_in": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Wire In (ignored)",
                "dynamic": false,
                "info": "연결(의존성)만 만들기 위한 더미 포트입니다. 들어오는 값은 무시합니다.",
                "input_types": [
                  "Message",
                  "Text",
                  "Data",
                  "str"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "wire_in",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "model_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-VBIsB",
        "measured": {
          "height": 493,
          "width": 320
        },
        "position": {
          "x": -5203.1703048032,
          "y": 2809.592428803032
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioEmbeddingsComponent-V3d8q",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "category": "lmstudio",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate embeddings using LM Studio.",
            "display_name": "LM Studio Embeddings",
            "documentation": "",
            "edited": false,
            "field_order": [
              "model",
              "base_url",
              "api_key",
              "temperature"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "key": "LMStudioEmbeddingsComponent",
            "last_updated": "2025-12-17T05:43:35.707Z",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Model",
                "group_outputs": false,
                "method": "build_embeddings",
                "name": "embeddings",
                "options": null,
                "required_inputs": null,
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.000018578044550916993,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "LM Studio Base URL",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\n\n\nclass LMStudioEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"LM Studio Embeddings\"\n    description: str = \"Generate embeddings using LM Studio.\"\n    icon = \"LMStudio\"\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):  # noqa: ARG002\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            refresh_button=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"LM Studio Base URL\",\n            refresh_button=True,\n            value=\"http://localhost:1234/v1\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use LM Studio Embeddings.\"\n            raise ImportError(msg) from e\n        try:\n            output = NVIDIAEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n                nvidia_api_key=self.api_key,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to LM Studio API. Error: {e}\"\n            raise ValueError(msg) from e\n        return output\n"
              },
              "model": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "info": "",
                "name": "model",
                "options": [
                  "qwen/qwen3-4b-thinking-2507",
                  "text-embedding-sentence-transformers_all-minilm-l12-v2",
                  "text-embedding-bge-reranker-v2-m3",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "qwen/qwen3-4b-2507",
                  "text-embedding-nomic-embed-text-v1.5",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text-embedding-sentence-transformers_all-minilm-l12-v2"
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Model Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "LMStudioEmbeddingsComponent"
        },
        "dragging": false,
        "id": "LMStudioEmbeddingsComponent-V3d8q",
        "measured": {
          "height": 285,
          "width": 320
        },
        "position": {
          "x": -2398.731415885646,
          "y": 2615.328610177691
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-fFi8K",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/components-io#chat-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "hidden": null,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _icon, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-fFi8K",
        "measured": {
          "height": 165,
          "width": 320
        },
        "position": {
          "x": -1443.0956327541794,
          "y": 2467.554127397748
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-d6Xiw",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "너는 디지털 포렌식 분석을 도와주는 유능한 포렌식 조수이다.\n\n입력으로는 보통 다음 두 가지가 주어진다.\n\n질문(question)\n\n아티팩트 목록: 각 행은\n\ntype\n\nlastwritetimestamp\n\ndescription\n\ntag (쉼표로 구분된 문자열)\n을 포함한다.\n\n여기서 description과 tag는 “해당 아티팩트가 무엇을 의미하는지 설명하는 자세한 정보”라고 생각하면 된다.\n태그 이름 하나하나의 사전적 정의를 설명하려 하지 말고, 전체를 참고 정보로만 사용하라.\n\n분석 시 지켜야 할 원칙:\n\n증거 기반 + 사실/추론 구분\n\ndescription, type, tag, time 안의 내용은 “관찰된 사실”로 취급한다.\n\n그 사실을 바탕으로 가능성 있는 시나리오는 제시하되,\n“~일 가능성이 있다”, “정황상 ~로 보인다”처럼 표현하여 추측임을 표시한다.\n\n질문 중심\n\n질문과 직접 관련 있어 보이는 아티팩트만 선택해 분석한다.\n\n관련성이 약하면 “현재 제공된 아티팩트만으로는 직접 답하기 어렵다”고 말하고 이유를 설명한다.\n\n부족한 증거와 추가 제안\n\n무엇을 확정할 수 없고, 어떤 아티팩트(예: MFT, EventLog, BrowserHistory 등)를 추가로 보면 좋은지 함께 제안한다.\n\n답변은 한국어로, 리포트 초안처럼 조금 자세히 쓴다.\n\n중요: 1번 섹션은 ‘요약’이나 ‘해석’을 금지한다.\n\n1번에는 너(모델)가 분석에 사용했다고 판단한 아티팩트 행만 적는다.\n\n각 행은 입력으로 받은 값을 그대로 적는다(수정/요약/의미추가 금지).\n\n즉, 1번은 “이번 답변에서 실제로 근거로 사용한 아티팩트 목록”이다.\n\n항상 아래 다섯 가지 구조로 답하라.\n\n관찰된 아티팩트\n\n이번 분석에 실제로 사용한 아티팩트만 포함한다.\n\n아래 형식으로 입력값을 그대로 나열한다(분석/요약 금지).\n하나의 아티팩트만 분석하는 게 아니라 여러 아티팩트를 분석해라\n\ntype: …\nlastwritetimestamp: …\ndescription: …\ntag: …\n\n해석 및 추론\n\n1번의 아티팩트를 근거로 질문에 답한다.\n\n“관찰된 사실(아티팩트 내용)”과 “추론(가능성)”을 명확히 구분해서 적는다.\n\n추가로 필요한 증거 / 한계\n\n현재 제공된 아티팩트만으로 확정 불가한 부분을 적는다.\n\n추가로 확인하면 좋은 아티팩트/분석을 구체적으로 제안한다.\n\n질문 추천\n\n다음 단계에서 사용자가 물어보면 좋은 후속 질문을 3~5개 추천한다.\n\n종합 의견\n\n어디까지는 말할 수 있고, 어디부터는 확정 불가인지 요약한다."
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-d6Xiw",
        "measured": {
          "height": 203,
          "width": 320
        },
        "position": {
          "x": -905.4433239227973,
          "y": 1658.7050541535236
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "OllamaGenerate-ce1dH",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "hyde 기술을 사용해서 질문을 다듬고, 태그를 선정한다다",
            "display_name": "hyde llm",
            "documentation": "",
            "edited": true,
            "field_order": [
              "dummy_input",
              "dummy_input_2",
              "input_message",
              "node_prompt",
              "system_message",
              "model_name",
              "base_url",
              "stream"
            ],
            "frozen": false,
            "icon": "LM studio",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "hidden": null,
                "method": "run",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Ollama /api/generate 엔드포인트 URL.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:11434/api/generate"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import requests\r\nfrom typing import Any\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import (\r\n    MessageInput,\r\n    Output,\r\n    StrInput,\r\n    BoolInput,\r\n    MultilineInput,\r\n)\r\nfrom langflow.schema.message import Message\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass OllamaGenerate(Component):\r\n    \"\"\"\r\n    Chat Input 같은 이전 노드에서 받은 메시지 + 노드 안에서 쓴 프롬프트를\r\n    Ollama /api/generate 로 보내고,\r\n    응답 텍스트를 다시 Message로 돌려주는 컴포넌트.\r\n    \"\"\"\r\n\r\n    display_name = \"hyde llm\"\r\n    description = \"hyde 기술을 사용해서 질문을 다듬고, 태그를 선정한다다\"\r\n    icon = \"LM studio\"\r\n    name = \"hydellm\"\r\n\r\n    # -------- 입력 정의 --------\r\n    inputs = [\r\n        # 0) 의미 없는 더미 입력 1 (플로우 연결용)\r\n        MessageInput(\r\n            name=\"dummy_input\",\r\n            display_name=\"Dummy Input 1 (ignored)\",\r\n            info=\"플로우 연결만을 위한 입력입니다. 값은 전혀 사용하지 않습니다.\",\r\n            input_types=[\"Message\", \"Text\"],\r\n            required=False,\r\n        ),\r\n\r\n        # 0-2) 의미 없는 더미 입력 2 (플로우 연결용)\r\n        MessageInput(\r\n            name=\"dummy_input_2\",\r\n            display_name=\"Dummy Input 2 (ignored)\",\r\n            info=\"플로우 연결만을 위한 두 번째 입력입니다. 값은 전혀 사용하지 않습니다.\",\r\n            input_types=[\"Message\", \"Text\"],\r\n            required=False,\r\n        ),\r\n\r\n        # 1) 앞 노드에서 들어오는 메인 프롬프트\r\n        MessageInput(\r\n            name=\"input_message\",\r\n            display_name=\"Input\",\r\n            info=\"이전 노드에서 받은 텍스트/Message. 그대로 Ollama 프롬프트로 사용됩니다.\",\r\n            input_types=[\"Message\", \"Text\"],\r\n            required=True,\r\n        ),\r\n\r\n        # 2) 노드 안에서 직접 쓰는 추가 프롬프트\r\n        MultilineInput(\r\n            name=\"node_prompt\",\r\n            display_name=\"Node Prompt\",\r\n            info=\"노드 안에서 직접 입력하는 추가 프롬프트. Chat 입력과 함께 합쳐집니다.\",\r\n            value=\"\",\r\n        ),\r\n\r\n        # 3) 선택적인 시스템 메시지\r\n        StrInput(\r\n            name=\"system_message\",\r\n            display_name=\"System Message\",\r\n            info=\"모델에게 주고 싶은 지시(system prompt). 비워두면 사용하지 않습니다.\",\r\n            advanced=True,\r\n            value=\"\",\r\n        ),\r\n\r\n        # 4) 모델 이름\r\n        StrInput(\r\n            name=\"model_name\",\r\n            display_name=\"Model Name\",\r\n            info=\"ollama list 에 표시되는 모델 이름 그대로 입력하세요.\",\r\n            value=\"ogito-2.1:671b-cloud\",\r\n        ),\r\n\r\n        # 5) Ollama /api/generate URL\r\n        StrInput(\r\n            name=\"base_url\",\r\n            display_name=\"Base URL\",\r\n            info=\"Ollama /api/generate 엔드포인트 URL.\",\r\n            value=\"http://localhost:11434/api/generate\",\r\n        ),\r\n\r\n        # 6) 스트리밍 여부\r\n        BoolInput(\r\n            name=\"stream\",\r\n            display_name=\"Stream\",\r\n            info=\"Ollama 스트리밍 응답 사용 여부. LangFlow에서는 False 권장.\",\r\n            value=False,\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    # -------- 출력 정의 --------\r\n    outputs = [\r\n        Output(\r\n            name=\"response\",\r\n            display_name=\"Model Response\",\r\n            type_=Message,\r\n            method=\"run\",\r\n        )\r\n    ]\r\n\r\n    # -------- 내부 유틸: 입력을 문자열로 안전 변환 --------\r\n    def _to_text(self, x: Any) -> str:\r\n        if x is None:\r\n            return \"\"\r\n        if isinstance(x, Message):\r\n            return x.text or \"\"\r\n        if isinstance(x, Data):\r\n            try:\r\n                return str(x.data or \"\")\r\n            except Exception:\r\n                return \"\"\r\n        if isinstance(x, str):\r\n            return x\r\n        try:\r\n            return str(x)\r\n        except Exception:\r\n            return \"\"\r\n\r\n    # -------- 실제 실행 로직 --------\r\n    def run(self) -> Message:\r\n        \"\"\"\r\n        더미1/더미2 중 하나는 반드시 들어오게 강제(OR 조건).\r\n        \"\"\"\r\n\r\n        # ✅ 0) 더미 입력 강제 체크 (둘 다 비면 실패)\r\n        dummy1 = getattr(self, \"dummy_input\", None)\r\n        dummy2 = getattr(self, \"dummy_input_2\", None)\r\n\r\n        d1 = self._to_text(dummy1).strip()\r\n        d2 = self._to_text(dummy2).strip()\r\n\r\n        if not (d1 or d2):\r\n            # 여기서 raise 해도 되지만, LangFlow에서 보기 편하게 Message로 반환\r\n            return Message(\r\n                text=\"[hydellm] Dummy Input 1 또는 Dummy Input 2 중 하나는 반드시 연결/입력되어야 합니다.\"\r\n            )\r\n\r\n        # ----- 1) 입력 메시지에서 텍스트 꺼내기 -----\r\n        raw_input = self.input_message\r\n        print(\"[OllamaGenerate] raw input_message:\", repr(raw_input), type(raw_input))\r\n\r\n        user_text = self._to_text(raw_input)\r\n\r\n        node_prompt = self.node_prompt or \"\"\r\n        system_message = self.system_message or \"\"\r\n\r\n        # ----- 2) system_message / node_prompt / user_text 합쳐서 최종 프롬프트 만들기 -----\r\n        parts = []\r\n        if system_message.strip():\r\n            parts.append(system_message.strip())\r\n        if node_prompt.strip():\r\n            parts.append(node_prompt.strip())\r\n        if user_text.strip():\r\n            parts.append(user_text.strip())\r\n\r\n        prompt = \"\\n\\n\".join(parts)\r\n\r\n        if not prompt:\r\n            print(\"[OllamaGenerate] 빈 프롬프트가 전달되었습니다.\")\r\n            return Message(text=\"[OllamaGenerate] 빈 프롬프트입니다.\")\r\n\r\n        # ----- 3) Ollama 설정값 읽기 -----\r\n        model_name = self.model_name or \"ogito-2.1:671b-cloud\"\r\n        base_url = self.base_url or \"http://localhost:11434/api/generate\"\r\n        stream = bool(self.stream)\r\n\r\n        payload = {\r\n            \"model\": model_name,\r\n            \"prompt\": prompt,\r\n            \"stream\": stream,\r\n        }\r\n\r\n        print(\"[OllamaGenerate] 요청 payload:\", payload)\r\n\r\n        # ----- 4) Ollama /api/generate 호출 -----\r\n        res = requests.post(base_url, json=payload)\r\n\r\n        if res.status_code != 200:\r\n            print(\"[OllamaGenerate] HTTP Status:\", res.status_code)\r\n            print(\"[OllamaGenerate] Body:\", res.text)\r\n            res.raise_for_status()\r\n\r\n        data = res.json()\r\n        print(\"[OllamaGenerate] 응답 JSON:\", data)\r\n\r\n        # ----- 5) 응답 텍스트 뽑기 -----\r\n        answer_text = \"\"\r\n        if isinstance(data, dict):\r\n            answer_text = (\r\n                data.get(\"response\")\r\n                or data.get(\"output\")\r\n                or data.get(\"message\")\r\n                or data.get(\"content\")\r\n                or \"\"\r\n            )\r\n        else:\r\n            answer_text = str(data)\r\n\r\n        if not answer_text:\r\n            answer_text = \"[OllamaGenerate] 응답에 텍스트 필드를 찾지 못했습니다:\\n{}\".format(data)\r\n\r\n        return Message(text=answer_text)\r\n"
              },
              "dummy_input": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Dummy Input 1 (ignored)",
                "dynamic": false,
                "info": "플로우 연결만을 위한 입력입니다. 값은 전혀 사용하지 않습니다.",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "dummy_input",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "dummy_input_2": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Dummy Input 2 (ignored)",
                "dynamic": false,
                "info": "플로우 연결만을 위한 두 번째 입력입니다. 값은 전혀 사용하지 않습니다.",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "dummy_input_2",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "input_message": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "이전 노드에서 받은 텍스트/Message. 그대로 Ollama 프롬프트로 사용됩니다.",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_message",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "model_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Model Name",
                "dynamic": false,
                "info": "ollama list 에 표시되는 모델 이름 그대로 입력하세요.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "model_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "cogito-2.1:671b-cloud"
              },
              "node_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Node Prompt",
                "dynamic": false,
                "info": "노드 안에서 직접 입력하는 추가 프롬프트. Chat 입력과 함께 합쳐집니다.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "node_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Ollama 스트리밍 응답 사용 여부. LangFlow에서는 False 권장.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "System Message",
                "dynamic": false,
                "info": "모델에게 주고 싶은 지시(system prompt). 비워두면 사용하지 않습니다.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "hydellm"
        },
        "dragging": false,
        "id": "OllamaGenerate-ce1dH",
        "measured": {
          "height": 629,
          "width": 320
        },
        "position": {
          "x": -3607.0300304601674,
          "y": 2665.2977466670354
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "note-eSS3R",
          "node": {
            "description": "sql 구문까지는 제작 완료\nDB에 데이터가 없어서 DB 부분은 다음으로 우선 DB에 넣을 수 있게 파이프라인 제작할 예쩡",
            "display_name": "",
            "documentation": "",
            "template": {}
          },
          "type": "note"
        },
        "dragging": false,
        "id": "note-eSS3R",
        "measured": {
          "height": 324,
          "width": 575
        },
        "position": {
          "x": -3321.868518353548,
          "y": 1392.2478576811372
        },
        "selected": false,
        "type": "noteNode"
      },
      {
        "data": {
          "id": "CustomComponent-3K6lL",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "이전 노드의 메시지 또는 직접 입력한 질문을 로컬 서버(server.py)에 보내고, 결과를 Message 형식으로 반환합니다.",
            "display_name": "Gemini MCP Client",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_message",
              "prompt",
              "system_prompt",
              "api_url"
            ],
            "frozen": false,
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "hidden": null,
                "method": "send_request",
                "name": "output_message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_url": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "API URL",
                "dynamic": false,
                "info": "요청을 보낼 FastAPI 서버 주소입니다.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "api_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:8000/chat"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom import Component\r\nfrom langflow.io import MessageInput, MultilineInput, Output\r\nfrom langflow.schema.message import Message\r\nimport requests\r\nfrom datetime import datetime\r\n\r\n\r\nclass GeminiAPIClient(Component):\r\n    display_name = \"Gemini MCP Client\"\r\n    description = \"이전 노드의 메시지 또는 직접 입력한 질문을 로컬 서버(server.py)에 보내고, 결과를 Message 형식으로 반환합니다.\"\r\n\r\n    # 화면에 보일 입력 노드들\r\n    inputs = [\r\n        # 1) 앞 노드에서 Message를 연결해서 받을 포트\r\n        MessageInput(\r\n            name=\"input_message\",\r\n            display_name=\"Input Message (연결용)\",\r\n            info=\"Chat Input 등 앞 노드의 Message 출력을 연결하세요. 없으면 아래 'Prompt' 값을 사용합니다.\",\r\n        ),\r\n        # 2) 노드 안에서 직접 적는 질문 (user 프롬프트)\r\n        MultilineInput(\r\n            name=\"prompt\",\r\n            display_name=\"Prompt (질문)\",\r\n            info=\"앞 노드를 쓰지 않을 때, 여기 적은 텍스트가 질문으로 사용됩니다.\",\r\n            value=\"안녕\",\r\n        ),\r\n        # 3) 시스템 프롬프트 입력\r\n        MultilineInput(\r\n            name=\"system_prompt\",\r\n            display_name=\"System Prompt (시스템 지시문)\",\r\n            info=(\r\n                \"모델에게 항상 먼저 전달할 시스템 지시문입니다.\\n\"\r\n                \"예) 넌 디지털 포렌식 분석 도우미다. 사용자의 질문에 대해 증거 기반으로만 답하라.\"\r\n            ),\r\n            value=\"\",\r\n        ),\r\n        # 4) API URL\r\n        MultilineInput(\r\n            name=\"api_url\",\r\n            display_name=\"API URL\",\r\n            info=\"요청을 보낼 FastAPI 서버 주소입니다.\",\r\n            value=\"http://localhost:8000/chat\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"send_request\"),\r\n    ]\r\n\r\n    def send_request(self) -> Message:\r\n        try:\r\n            # LangFlow가 inputs의 name으로 속성을 주입해 줌\r\n            input_msg = getattr(self, \"input_message\", None)\r\n            prompt_text = getattr(self, \"prompt\", \"\") or \"\"\r\n            system_prompt = getattr(self, \"system_prompt\", \"\") or \"\"\r\n            system_prompt = system_prompt.strip()\r\n            url = (getattr(self, \"api_url\", \"\") or \"\").strip() or \"http://localhost:8000/chat\"\r\n\r\n            # 1순위: 앞 노드 Message 내용\r\n            if isinstance(input_msg, Message) and getattr(input_msg, \"text\", None):\r\n                final_prompt = input_msg.text\r\n            # 2순위: 노드 안의 Prompt 입력 값\r\n            elif isinstance(prompt_text, str) and prompt_text.strip():\r\n                final_prompt = prompt_text.strip()\r\n            else:\r\n                final_prompt = \"질문이 없습니다.\"\r\n\r\n            # 시스템 프롬프트 + 유저 프롬프트를 하나의 문자열로 합치기\r\n            if system_prompt:\r\n                composed_prompt = f\"[SYSTEM]\\n{system_prompt}\\n\\n[USER]\\n{final_prompt}\"\r\n            else:\r\n                composed_prompt = final_prompt\r\n\r\n            headers = {\"Content-Type\": \"application/json\"}\r\n            payload = {\"prompt\": composed_prompt}\r\n\r\n            response = requests.post(url, json=payload, headers=headers, timeout=1200)\r\n\r\n            if response.status_code == 200:\r\n                # 응답이 JSON이 아닐 수도 있으니 방어적으로 처리\r\n                try:\r\n                    data = response.json()\r\n                except ValueError:\r\n                    return Message(\r\n                        text=f\"서버 응답이 JSON 형식이 아닙니다:\\n{response.text}\",\r\n                        sender=\"System Error\",\r\n                        timestamp=datetime.now().isoformat(),\r\n                    )\r\n\r\n                result_text = (\r\n                    data.get(\"response\")\r\n                    or data.get(\"message\")\r\n                    or str(data)\r\n                )\r\n\r\n                return Message(\r\n                    text=result_text,\r\n                    sender=\"Gemini (MCP)\",\r\n                    timestamp=datetime.now().isoformat(),\r\n                )\r\n            else:\r\n                return Message(\r\n                    text=f\"서버 에러 ({response.status_code}): {response.text}\",\r\n                    sender=\"System Error\",\r\n                    timestamp=datetime.now().isoformat(),\r\n                )\r\n\r\n        except requests.exceptions.ConnectionError:\r\n            return Message(\r\n                text=\"'server.py'가 실행 중인지, 그리고 API URL이 올바른지 확인하세요.\",\r\n                sender=\"System Alert\",\r\n                timestamp=datetime.now().isoformat(),\r\n            )\r\n        except Exception as e:\r\n            # 여기로 빠지면 더 이상 LangFlow 쪽에서 Error Code: 1 로 죽지 않고\r\n            # 노드 출력으로 에러 내용이 그대로 보일 거야.\r\n            return Message(\r\n                text=f\"클라이언트 에러(컴포넌트 내부): {repr(e)}\",\r\n                sender=\"System Error\",\r\n                timestamp=datetime.now().isoformat(),\r\n            )\r\n"
              },
              "input_message": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input Message (연결용)",
                "dynamic": false,
                "info": "Chat Input 등 앞 노드의 Message 출력을 연결하세요. 없으면 아래 'Prompt' 값을 사용합니다.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Prompt (질문)",
                "dynamic": false,
                "info": "앞 노드를 쓰지 않을 때, 여기 적은 텍스트가 질문으로 사용됩니다.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Prompt (시스템 지시문)",
                "dynamic": false,
                "info": "모델에게 항상 먼저 전달할 시스템 지시문입니다.\n예) 넌 디지털 포렌식 분석 도우미다. 사용자의 질문에 대해 증거 기반으로만 답하라.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "response",
          "showNode": true,
          "type": "GeminiAPIClient"
        },
        "dragging": false,
        "id": "CustomComponent-3K6lL",
        "measured": {
          "height": 481,
          "width": 320
        },
        "position": {
          "x": -137.82258815481555,
          "y": 2304.696750546862
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Milvus-1FHVQ",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Milvus vector store with search capabilities",
            "display_name": "Milvus",
            "documentation": "",
            "edited": true,
            "field_order": [
              "trigger",
              "collection_name",
              "collection_description",
              "uri",
              "password",
              "connection_args",
              "primary_field",
              "text_field",
              "vector_field",
              "consistency_level",
              "index_params",
              "search_params",
              "drop_old",
              "timeout",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "embedding",
              "number_of_results"
            ],
            "frozen": false,
            "icon": "Milvus",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search_documents",
                "name": "search_results",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "hidden": null,
                "method": "as_dataframe",
                "name": "dataframe",
                "options": null,
                "required_inputs": null,
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\r\nfrom langflow.helpers.data import docs_to_data\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    DictInput,\r\n    DropdownInput,\r\n    FloatInput,\r\n    HandleInput,\r\n    IntInput,\r\n    SecretStrInput,\r\n    StrInput,\r\n)\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass MilvusVectorStoreComponent(LCVectorStoreComponent):\r\n    \"\"\"Milvus vector store with search capabilities.\"\"\"\r\n\r\n    display_name: str = \"Milvus\"\r\n    description: str = \"Milvus vector store with search capabilities\"\r\n    name = \"Milvus\"\r\n    icon = \"Milvus\"\r\n\r\n    inputs = [\r\n        # 🔹 트리거용(무시되는) 입력 포트 추가\r\n        HandleInput(\r\n            name=\"trigger\",\r\n            display_name=\"Trigger (Ignored)\",\r\n            info=(\r\n                \"앞 노드에서 어떤 데이터가 넘어와도 이 포트에서는 \"\r\n                \"내용을 전혀 사용하지 않고, 단순히 실행 트리거 역할만 합니다.\"\r\n            ),\r\n            input_types=[\"Data\", \"Text\", \"Message\"],\r\n        ),\r\n\r\n        StrInput(name=\"collection_name\", display_name=\"Collection Name\", value=\"langflow\"),\r\n        StrInput(name=\"collection_description\", display_name=\"Collection Description\", value=\"\"),\r\n        StrInput(\r\n            name=\"uri\",\r\n            display_name=\"Connection URI\",\r\n            value=\"http://localhost:19530\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"password\",\r\n            display_name=\"Milvus Token\",\r\n            value=\"\",\r\n            info=\"Ignore this field if no token is required to make connection.\",\r\n        ),\r\n        DictInput(name=\"connection_args\", display_name=\"Other Connection Arguments\", advanced=True),\r\n        StrInput(name=\"primary_field\", display_name=\"Primary Field Name\", value=\"pk\"),\r\n        StrInput(name=\"text_field\", display_name=\"Text Field Name\", value=\"text\"),\r\n        StrInput(name=\"vector_field\", display_name=\"Vector Field Name\", value=\"vector\"),\r\n        DropdownInput(\r\n            name=\"consistency_level\",\r\n            display_name=\"Consistencey Level\",\r\n            options=[\"Bounded\", \"Session\", \"Strong\", \"Eventual\"],\r\n            value=\"Session\",\r\n            advanced=True,\r\n        ),\r\n        DictInput(name=\"index_params\", display_name=\"Index Parameters\", advanced=True),\r\n        DictInput(name=\"search_params\", display_name=\"Search Parameters\", advanced=True),\r\n        BoolInput(name=\"drop_old\", display_name=\"Drop Old Collection\", value=False, advanced=True),\r\n        FloatInput(name=\"timeout\", display_name=\"Timeout\", advanced=True),\r\n\r\n        *LCVectorStoreComponent.inputs,\r\n\r\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\r\n        IntInput(\r\n            name=\"number_of_results\",\r\n            display_name=\"Number of Results\",\r\n            info=\"Number of results to return.\",\r\n            value=4,\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    @check_cached_vector_store\r\n    def build_vector_store(self):\r\n        print(\"\\n--- Milvus LOG 1: build_vector_store 함수 시작 ---\")\r\n        \r\n        try:\r\n            from langchain_milvus.vectorstores import Milvus as LangchainMilvus\r\n        except ImportError as e:\r\n            msg = \"Could not import Milvus integration package. Please install it with `pip install langchain-milvus`.\"\r\n            raise ImportError(msg) from e\r\n\r\n        # trigger 입력은 완전히 무시됨 (속성으로 들어오지만 사용 X)\r\n\r\n        self.connection_args.update(uri=self.uri, token=self.password)\r\n    \r\n        milvus_store = LangchainMilvus(\r\n            embedding_function=self.embedding,\r\n            collection_name=self.collection_name,\r\n            collection_description=self.collection_description,\r\n            connection_args=self.connection_args,\r\n            consistency_level=self.consistency_level,\r\n            index_params=self.index_params,\r\n            search_params=self.search_params,\r\n            drop_old=self.drop_old,\r\n            auto_id=True,\r\n            primary_field=self.primary_field,\r\n            text_field=self.text_field,\r\n            vector_field=self.vector_field,\r\n        )\r\n\r\n        self.ingest_data = self._prepare_ingest_data()\r\n\r\n        documents = []\r\n        for _input in self.ingest_data or []:\r\n            if isinstance(_input, Data):\r\n                documents.append(_input.to_lc_document())\r\n            else:\r\n                documents.append(_input)\r\n\r\n        print(f\"--- Milvus LOG 2: 입력으로부터 {len(documents)}개의 문서를 받았습니다.\")\r\n        \r\n        print(\"--- Milvus LOG 3: 필터링 전 각 문서의 내용:\")\r\n        for i, doc in enumerate(documents):\r\n            content = getattr(doc, 'page_content', 'NO_PAGE_CONTENT_ATTRIBUTE_ERROR')\r\n            print(f\"  - 문서 #{i}: '{content}'\")\r\n\r\n        if documents:\r\n            filtered_documents = [\r\n                doc for doc in documents\r\n                if hasattr(doc, \"page_content\")\r\n                and doc.page_content\r\n                and doc.page_content.strip()\r\n            ]\r\n            \r\n            print(f\"--- Milvus LOG 4: 빈 문서 필터링 후 {len(filtered_documents)}개가 남았습니다.\")\r\n\r\n            if not filtered_documents:\r\n                print(\"--- Milvus LOG 5: 유효한 문서가 없어 저장을 중단합니다.\")\r\n            else:\r\n                try:\r\n                    texts_to_embed = [doc.page_content for doc in filtered_documents]\r\n                    print(f\"--- Milvus LOG 7.1: 임베딩 모델에 {len(texts_to_embed)}개의 텍스트를 전달합니다.\")\r\n\r\n                    print(\"--- Milvus LOG 7.2: 임베딩 모델을 호출하여 벡터 변환을 시작합니다...\")\r\n                    \r\n                    embeddings = self.embedding.embed_documents(texts_to_embed)\r\n\r\n                    print(f\"--- Milvus LOG 7.3: 임베딩 모델이 {len(embeddings)}개의 벡터를 반환했습니다.\")\r\n\r\n                    if len(texts_to_embed) != len(embeddings):\r\n                        print(f\"--- Milvus LOG 7.4 FATAL: 텍스트 개수({len(texts_to_embed)})와 벡터 개수({len(embeddings)})가 일치하지 않아 저장할 수 없습니다!\")\r\n                        raise ValueError(f\"Mismatched lengths: {len(texts_to_embed)} texts vs {len(embeddings)} embeddings\")\r\n\r\n                    print(\"--- Milvus LOG 7.4: 텍스트와 벡터 개수가 일치합니다. Milvus에 저장을 시작합니다...\")\r\n                    milvus_store.add_embeddings(texts=texts_to_embed, embeddings=embeddings)\r\n                    print(\"--- Milvus LOG 7.5: 문서 추가에 성공했습니다.\")\r\n                    \r\n                except Exception as e:\r\n                    print(f\"--- Milvus LOG 8: 작업 중 에러 발생: {e}\")\r\n                    raise e\r\n\r\n        return milvus_store\r\n\r\n    def search_documents(self) -> list[Data]:\r\n        vector_store = self.build_vector_store()\r\n\r\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\r\n            docs = vector_store.similarity_search(\r\n                query=self.search_query,\r\n                k=self.number_of_results,\r\n            )\r\n\r\n            data = docs_to_data(docs)\r\n            self.status = data\r\n            return data\r\n        return []\r\n"
              },
              "collection_description": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Description",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "collection_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "tag_test"
              },
              "connection_args": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Other Connection Arguments",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "connection_args",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "consistency_level": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Consistencey Level",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "consistency_level",
                "options": [
                  "Bounded",
                  "Session",
                  "Strong",
                  "Eventual"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Session"
              },
              "drop_old": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Drop Old Collection",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "drop_old",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "index_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Index Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "index_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1000
              },
              "password": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Milvus Token",
                "dynamic": false,
                "info": "Ignore this field if no token is required to make connection.",
                "input_types": [],
                "load_from_db": false,
                "name": "password",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "primary_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Primary Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "primary_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "pk"
              },
              "search_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Search Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "search_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Text Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text"
              },
              "timeout": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": ""
              },
              "trigger": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Trigger (Ignored)",
                "dynamic": false,
                "info": "앞 노드에서 어떤 데이터가 넘어와도 이 포트에서는 내용을 전혀 사용하지 않고, 단순히 실행 트리거 역할만 합니다.",
                "input_types": [
                  "Data",
                  "Text",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "trigger",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "uri": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Connection URI",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "uri",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:19530"
              },
              "vector_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Vector Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "vector_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "vector"
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "Milvus"
        },
        "dragging": false,
        "id": "Milvus-1FHVQ",
        "measured": {
          "height": 991,
          "width": 320
        },
        "position": {
          "x": -1814.6892997110283,
          "y": 1846.1117724188584
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-VoB5q",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "로컬 FastAPI 서버(test.py)의 /run-embed 엔드포인트를 호출하여 forensic_keyword_results → Milvus 임베딩 파이프라인을 실행합니다. 앞 노드 입력은 단순 연결용이며 실제로는 사용하지 않습니다.",
            "display_name": "Forensic Milvus Embed Client",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_message",
              "api_url"
            ],
            "frozen": false,
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Summary",
                "group_outputs": false,
                "hidden": null,
                "method": "call_embed_api",
                "name": "summary",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_url": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Embed API URL",
                "dynamic": false,
                "info": "임베딩을 실행할 FastAPI 서버 주소입니다.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "api_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:8001/run-embed"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom import Component\r\nfrom langflow.io import MessageInput, MultilineInput, Output\r\nfrom langflow.schema.message import Message\r\n\r\nimport requests\r\nfrom datetime import datetime\r\n\r\n\r\nclass ForensicMilvusEmbedClient(Component):\r\n    display_name = \"Forensic Milvus Embed Client\"\r\n    description = (\r\n        \"로컬 FastAPI 서버(test.py)의 /run-embed 엔드포인트를 호출하여 \"\r\n        \"forensic_keyword_results → Milvus 임베딩 파이프라인을 실행합니다. \"\r\n        \"앞 노드 입력은 단순 연결용이며 실제로는 사용하지 않습니다.\"\r\n    )\r\n\r\n    # 화면에 보일 입력들\r\n    inputs = [\r\n        # 1) 앞 노드에서 Message를 연결하기 위한 포트 (실제로는 무시)\r\n        MessageInput(\r\n            name=\"input_message\",\r\n            display_name=\"Input Message (무시됨)\",\r\n            info=\"ChatInput 등 앞 노드의 Message 출력을 연결만 합니다. 내용은 사용하지 않습니다.\",\r\n        ),\r\n        # 2) API URL 설정\r\n        MultilineInput(\r\n            name=\"api_url\",\r\n            display_name=\"Embed API URL\",\r\n            info=\"임베딩을 실행할 FastAPI 서버 주소입니다.\",\r\n            value=\"http://localhost:8001/run-embed\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Embedding Summary\",\r\n            name=\"summary\",\r\n            method=\"call_embed_api\",\r\n        ),\r\n    ]\r\n\r\n    def call_embed_api(self) -> Message:\r\n        \"\"\"\r\n        - input_message는 완전히 무시\r\n        - self.api_url로 POST 요청 전송\r\n        - 서버에서 돌린 요약(summary)을 그대로 Chat 메시지로 반환\r\n        \"\"\"\r\n        url = (self.api_url or \"\").strip() or \"http://localhost:8001/run-embed\"\r\n\r\n        try:\r\n            # /run-embed 는 바디 필요 없으니까 그냥 POST\r\n            response = requests.post(url, timeout=3600)\r\n\r\n            if response.status_code == 200:\r\n                data = response.json()\r\n                # test.py에서 {\"status\": \"done\", \"summary\": \"...\"} 형태로 줬다고 가정\r\n                summary_text = data.get(\"summary\") or str(data)\r\n\r\n                return Message(\r\n                    text=summary_text,\r\n                    sender=\"Milvus Embed Server\",\r\n                    timestamp=datetime.now().isoformat(),\r\n                )\r\n            else:\r\n                return Message(\r\n                    text=f\"서버 에러 ({response.status_code}): {response.text}\",\r\n                    sender=\"Milvus Embed Server (Error)\",\r\n                    timestamp=datetime.now().isoformat(),\r\n                )\r\n\r\n        except requests.exceptions.ConnectionError:\r\n            return Message(\r\n                text=(\r\n                    \"임베딩 서버에 연결할 수 없습니다.\\n\"\r\n                    \"- uvicorn test:app --host 0.0.0.0 --port 8001 로 서버가 실행 중인지\\n\"\r\n                    \"- API URL이 올바른지(http://localhost:8001/run-embed) 확인하세요.\"\r\n                ),\r\n                sender=\"Milvus Embed Server (ConnectionError)\",\r\n                timestamp=datetime.now().isoformat(),\r\n            )\r\n        except Exception as e:\r\n            return Message(\r\n                text=f\"클라이언트 예외 발생: {str(e)}\",\r\n                sender=\"Milvus Embed Server (Exception)\",\r\n                timestamp=datetime.now().isoformat(),\r\n            )\r\n"
              },
              "input_message": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input Message (무시됨)",
                "dynamic": false,
                "info": "ChatInput 등 앞 노드의 Message 출력을 연결만 합니다. 내용은 사용하지 않습니다.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ForensicMilvusEmbedClient"
        },
        "dragging": false,
        "id": "CustomComponent-VoB5q",
        "measured": {
          "height": 333,
          "width": 320
        },
        "position": {
          "x": -2773.1332625729488,
          "y": 1864.8602196059528
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "MCPTools-xU7UW",
          "node": {
            "base_classes": [
              "DataFrame"
            ],
            "beta": false,
            "category": "agents",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Connect to an MCP server to use its tools.",
            "display_name": "MCP Tools",
            "documentation": "https://docs.langflow.org/mcp-client",
            "edited": false,
            "field_order": [
              "mcp_server",
              "tool",
              "tool_placeholder"
            ],
            "frozen": false,
            "icon": "Mcp",
            "key": "MCPTools",
            "last_updated": "2025-12-17T05:50:37.923Z",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Toolset",
                "group_outputs": false,
                "hidden": null,
                "method": "to_toolkit",
                "name": "component_as_tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.16666666666666666,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from __future__ import annotations\n\nimport asyncio\nimport uuid\nfrom typing import Any\n\nfrom langchain_core.tools import StructuredTool  # noqa: TC002\n\nfrom langflow.api.v2.mcp import get_server\nfrom langflow.base.agents.utils import maybe_unflatten_dict, safe_cache_get, safe_cache_set\nfrom langflow.base.mcp.util import (\n    MCPSseClient,\n    MCPStdioClient,\n    create_input_schema_from_json_schema,\n    update_tools,\n)\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs.inputs import InputTypes  # noqa: TC001\nfrom langflow.io import DropdownInput, McpInput, MessageTextInput, Output\nfrom langflow.io.schema import flatten_schema, schema_to_langflow_inputs\nfrom langflow.logging import logger\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\n\n# Import get_server from the backend API\nfrom langflow.services.database.models.user.crud import get_user_by_id\nfrom langflow.services.deps import get_settings_service, get_storage_service, session_scope\n\n\nclass MCPToolsComponent(ComponentWithCache):\n    schema_inputs: list = []\n    tools: list[StructuredTool] = []\n    _not_load_actions: bool = False\n    _tool_cache: dict = {}\n    _last_selected_server: str | None = None  # Cache for the last selected server\n\n    def __init__(self, **data) -> None:\n        super().__init__(**data)\n        # Initialize cache keys to avoid CacheMiss when accessing them\n        self._ensure_cache_structure()\n\n        # Initialize clients with access to the component cache\n        self.stdio_client: MCPStdioClient = MCPStdioClient(component_cache=self._shared_component_cache)\n        self.sse_client: MCPSseClient = MCPSseClient(component_cache=self._shared_component_cache)\n\n    def _ensure_cache_structure(self):\n        \"\"\"Ensure the cache has the required structure.\"\"\"\n        # Check if servers key exists and is not CacheMiss\n        servers_value = safe_cache_get(self._shared_component_cache, \"servers\")\n        if servers_value is None:\n            safe_cache_set(self._shared_component_cache, \"servers\", {})\n\n        # Check if last_selected_server key exists and is not CacheMiss\n        last_server_value = safe_cache_get(self._shared_component_cache, \"last_selected_server\")\n        if last_server_value is None:\n            safe_cache_set(self._shared_component_cache, \"last_selected_server\", \"\")\n\n    default_keys: list[str] = [\n        \"code\",\n        \"_type\",\n        \"tool_mode\",\n        \"tool_placeholder\",\n        \"mcp_server\",\n        \"tool\",\n    ]\n\n    display_name = \"MCP Tools\"\n    description = \"Connect to an MCP server to use its tools.\"\n    documentation: str = \"https://docs.langflow.org/mcp-client\"\n    icon = \"Mcp\"\n    name = \"MCPTools\"\n\n    inputs = [\n        McpInput(\n            name=\"mcp_server\",\n            display_name=\"MCP Server\",\n            info=\"Select the MCP Server that will be used by this component\",\n            real_time_refresh=True,\n        ),\n        DropdownInput(\n            name=\"tool\",\n            display_name=\"Tool\",\n            options=[],\n            value=\"\",\n            info=\"Select the tool to execute\",\n            show=False,\n            required=True,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            info=\"Placeholder for the tool\",\n            value=\"\",\n            show=False,\n            tool_mode=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Response\", name=\"response\", method=\"build_output\"),\n    ]\n\n    async def _validate_schema_inputs(self, tool_obj) -> list[InputTypes]:\n        \"\"\"Validate and process schema inputs for a tool.\"\"\"\n        try:\n            if not tool_obj or not hasattr(tool_obj, \"args_schema\"):\n                msg = \"Invalid tool object or missing input schema\"\n                raise ValueError(msg)\n\n            flat_schema = flatten_schema(tool_obj.args_schema.schema())\n            input_schema = create_input_schema_from_json_schema(flat_schema)\n            if not input_schema:\n                msg = f\"Empty input schema for tool '{tool_obj.name}'\"\n                raise ValueError(msg)\n\n            schema_inputs = schema_to_langflow_inputs(input_schema)\n            if not schema_inputs:\n                msg = f\"No input parameters defined for tool '{tool_obj.name}'\"\n                await logger.awarning(msg)\n                return []\n\n        except Exception as e:\n            msg = f\"Error validating schema inputs: {e!s}\"\n            await logger.aexception(msg)\n            raise ValueError(msg) from e\n        else:\n            return schema_inputs\n\n    async def update_tool_list(self, mcp_server_value=None):\n        # Accepts mcp_server_value as dict {name, config} or uses self.mcp_server\n        mcp_server = mcp_server_value if mcp_server_value is not None else getattr(self, \"mcp_server\", None)\n        server_name = None\n        server_config_from_value = None\n        if isinstance(mcp_server, dict):\n            server_name = mcp_server.get(\"name\")\n            server_config_from_value = mcp_server.get(\"config\")\n        else:\n            server_name = mcp_server\n        if not server_name:\n            self.tools = []\n            return [], {\"name\": server_name, \"config\": server_config_from_value}\n\n        # Use shared cache if available\n        servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n        cached = servers_cache.get(server_name) if isinstance(servers_cache, dict) else None\n\n        if cached is not None:\n            self.tools = cached[\"tools\"]\n            self.tool_names = cached[\"tool_names\"]\n            self._tool_cache = cached[\"tool_cache\"]\n            server_config_from_value = cached[\"config\"]\n            return self.tools, {\"name\": server_name, \"config\": server_config_from_value}\n\n        try:\n            async with session_scope() as db:\n                if not self.user_id:\n                    msg = \"User ID is required for fetching MCP tools.\"\n                    raise ValueError(msg)\n                current_user = await get_user_by_id(db, self.user_id)\n\n                # Try to get server config from DB/API\n                server_config = await get_server(\n                    server_name,\n                    current_user,\n                    db,\n                    storage_service=get_storage_service(),\n                    settings_service=get_settings_service(),\n                )\n\n            # If get_server returns empty but we have a config, use it\n            if not server_config and server_config_from_value:\n                server_config = server_config_from_value\n\n            if not server_config:\n                self.tools = []\n                return [], {\"name\": server_name, \"config\": server_config}\n\n            _, tool_list, tool_cache = await update_tools(\n                server_name=server_name,\n                server_config=server_config,\n                mcp_stdio_client=self.stdio_client,\n                mcp_sse_client=self.sse_client,\n            )\n\n            self.tool_names = [tool.name for tool in tool_list if hasattr(tool, \"name\")]\n            self._tool_cache = tool_cache\n            self.tools = tool_list\n            # Cache the result using shared cache\n            cache_data = {\n                \"tools\": tool_list,\n                \"tool_names\": self.tool_names,\n                \"tool_cache\": tool_cache,\n                \"config\": server_config,\n            }\n\n            # Safely update the servers cache\n            current_servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n            if isinstance(current_servers_cache, dict):\n                current_servers_cache[server_name] = cache_data\n                safe_cache_set(self._shared_component_cache, \"servers\", current_servers_cache)\n\n        except (TimeoutError, asyncio.TimeoutError) as e:\n            msg = f\"Timeout updating tool list: {e!s}\"\n            await logger.aexception(msg)\n            raise TimeoutError(msg) from e\n        except Exception as e:\n            msg = f\"Error updating tool list: {e!s}\"\n            await logger.aexception(msg)\n            raise ValueError(msg) from e\n        else:\n            return tool_list, {\"name\": server_name, \"config\": server_config}\n\n    async def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:\n        \"\"\"Toggle the visibility of connection-specific fields based on the selected mode.\"\"\"\n        try:\n            if field_name == \"tool\":\n                try:\n                    if len(self.tools) == 0:\n                        try:\n                            self.tools, build_config[\"mcp_server\"][\"value\"] = await self.update_tool_list()\n                            build_config[\"tool\"][\"options\"] = [tool.name for tool in self.tools]\n                            build_config[\"tool\"][\"placeholder\"] = \"Select a tool\"\n                        except (TimeoutError, asyncio.TimeoutError) as e:\n                            msg = f\"Timeout updating tool list: {e!s}\"\n                            await logger.aexception(msg)\n                            if not build_config[\"tools_metadata\"][\"show\"]:\n                                build_config[\"tool\"][\"show\"] = True\n                                build_config[\"tool\"][\"options\"] = []\n                                build_config[\"tool\"][\"value\"] = \"\"\n                                build_config[\"tool\"][\"placeholder\"] = \"Timeout on MCP server\"\n                            else:\n                                build_config[\"tool\"][\"show\"] = False\n                        except ValueError:\n                            if not build_config[\"tools_metadata\"][\"show\"]:\n                                build_config[\"tool\"][\"show\"] = True\n                                build_config[\"tool\"][\"options\"] = []\n                                build_config[\"tool\"][\"value\"] = \"\"\n                                build_config[\"tool\"][\"placeholder\"] = \"Error on MCP Server\"\n                            else:\n                                build_config[\"tool\"][\"show\"] = False\n\n                    if field_value == \"\":\n                        return build_config\n                    tool_obj = None\n                    for tool in self.tools:\n                        if tool.name == field_value:\n                            tool_obj = tool\n                            break\n                    if tool_obj is None:\n                        msg = f\"Tool {field_value} not found in available tools: {self.tools}\"\n                        await logger.awarning(msg)\n                        return build_config\n                    await self._update_tool_config(build_config, field_value)\n                except Exception as e:\n                    build_config[\"tool\"][\"options\"] = []\n                    msg = f\"Failed to update tools: {e!s}\"\n                    raise ValueError(msg) from e\n                else:\n                    return build_config\n            elif field_name == \"mcp_server\":\n                if not field_value:\n                    build_config[\"tool\"][\"show\"] = False\n                    build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"value\"] = \"\"\n                    build_config[\"tool\"][\"placeholder\"] = \"\"\n                    build_config[\"tool_placeholder\"][\"tool_mode\"] = False\n                    self.remove_non_default_keys(build_config)\n                    return build_config\n\n                build_config[\"tool_placeholder\"][\"tool_mode\"] = True\n\n                current_server_name = field_value.get(\"name\") if isinstance(field_value, dict) else field_value\n                _last_selected_server = safe_cache_get(self._shared_component_cache, \"last_selected_server\", \"\")\n\n                # To avoid unnecessary updates, only proceed if the server has actually changed\n                if (_last_selected_server in (current_server_name, \"\")) and build_config[\"tool\"][\"show\"]:\n                    if current_server_name:\n                        servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n                        if isinstance(servers_cache, dict):\n                            cached = servers_cache.get(current_server_name)\n                            if cached is not None and cached.get(\"tool_names\"):\n                                cached_tools = cached[\"tool_names\"]\n                                current_tools = build_config[\"tool\"][\"options\"]\n                                if current_tools == cached_tools:\n                                    return build_config\n                    else:\n                        return build_config\n\n                # Determine if \"Tool Mode\" is active by checking if the tool dropdown is hidden.\n                is_in_tool_mode = build_config[\"tools_metadata\"][\"show\"]\n                safe_cache_set(self._shared_component_cache, \"last_selected_server\", current_server_name)\n\n                # Check if tools are already cached for this server before clearing\n                cached_tools = None\n                if current_server_name:\n                    servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n                    if isinstance(servers_cache, dict):\n                        cached = servers_cache.get(current_server_name)\n                        if cached is not None:\n                            cached_tools = cached[\"tools\"]\n                            self.tools = cached_tools\n                            self.tool_names = cached[\"tool_names\"]\n                            self._tool_cache = cached[\"tool_cache\"]\n\n                # Only clear tools if we don't have cached tools for the current server\n                if not cached_tools:\n                    self.tools = []  # Clear previous tools only if no cache\n\n                self.remove_non_default_keys(build_config)  # Clear previous tool inputs\n\n                # Only show the tool dropdown if not in tool_mode\n                if not is_in_tool_mode:\n                    build_config[\"tool\"][\"show\"] = True\n                    if cached_tools:\n                        # Use cached tools to populate options immediately\n                        build_config[\"tool\"][\"options\"] = [tool.name for tool in cached_tools]\n                        build_config[\"tool\"][\"placeholder\"] = \"Select a tool\"\n                    else:\n                        # Show loading state only when we need to fetch tools\n                        build_config[\"tool\"][\"placeholder\"] = \"Loading tools...\"\n                        build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"value\"] = uuid.uuid4()\n                else:\n                    # Keep the tool dropdown hidden if in tool_mode\n                    self._not_load_actions = True\n                    build_config[\"tool\"][\"show\"] = False\n\n            elif field_name == \"tool_mode\":\n                build_config[\"tool\"][\"placeholder\"] = \"\"\n                build_config[\"tool\"][\"show\"] = not bool(field_value) and bool(build_config[\"mcp_server\"])\n                self.remove_non_default_keys(build_config)\n                self.tool = build_config[\"tool\"][\"value\"]\n                if field_value:\n                    self._not_load_actions = True\n                else:\n                    build_config[\"tool\"][\"value\"] = uuid.uuid4()\n                    build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"show\"] = True\n                    build_config[\"tool\"][\"placeholder\"] = \"Loading tools...\"\n            elif field_name == \"tools_metadata\":\n                self._not_load_actions = False\n\n        except Exception as e:\n            msg = f\"Error in update_build_config: {e!s}\"\n            await logger.aexception(msg)\n            raise ValueError(msg) from e\n        else:\n            return build_config\n\n    def get_inputs_for_all_tools(self, tools: list) -> dict:\n        \"\"\"Get input schemas for all tools.\"\"\"\n        inputs = {}\n        for tool in tools:\n            if not tool or not hasattr(tool, \"name\"):\n                continue\n            try:\n                flat_schema = flatten_schema(tool.args_schema.schema())\n                input_schema = create_input_schema_from_json_schema(flat_schema)\n                langflow_inputs = schema_to_langflow_inputs(input_schema)\n                inputs[tool.name] = langflow_inputs\n            except (AttributeError, ValueError, TypeError, KeyError) as e:\n                msg = f\"Error getting inputs for tool {getattr(tool, 'name', 'unknown')}: {e!s}\"\n                logger.exception(msg)\n                continue\n        return inputs\n\n    def remove_input_schema_from_build_config(\n        self, build_config: dict, tool_name: str, input_schema: dict[list[InputTypes], Any]\n    ):\n        \"\"\"Remove the input schema for the tool from the build config.\"\"\"\n        # Keep only schemas that don't belong to the current tool\n        input_schema = {k: v for k, v in input_schema.items() if k != tool_name}\n        # Remove all inputs from other tools\n        for value in input_schema.values():\n            for _input in value:\n                if _input.name in build_config:\n                    build_config.pop(_input.name)\n\n    def remove_non_default_keys(self, build_config: dict) -> None:\n        \"\"\"Remove non-default keys from the build config.\"\"\"\n        for key in list(build_config.keys()):\n            if key not in self.default_keys:\n                build_config.pop(key)\n\n    async def _update_tool_config(self, build_config: dict, tool_name: str) -> None:\n        \"\"\"Update tool configuration with proper error handling.\"\"\"\n        if not self.tools:\n            self.tools, build_config[\"mcp_server\"][\"value\"] = await self.update_tool_list()\n\n        if not tool_name:\n            return\n\n        tool_obj = next((tool for tool in self.tools if tool.name == tool_name), None)\n        if not tool_obj:\n            msg = f\"Tool {tool_name} not found in available tools: {self.tools}\"\n            self.remove_non_default_keys(build_config)\n            build_config[\"tool\"][\"value\"] = \"\"\n            await logger.awarning(msg)\n            return\n\n        try:\n            # Store current values before removing inputs\n            current_values = {}\n            for key, value in build_config.items():\n                if key not in self.default_keys and isinstance(value, dict) and \"value\" in value:\n                    current_values[key] = value[\"value\"]\n\n            # Get all tool inputs and remove old ones\n            input_schema_for_all_tools = self.get_inputs_for_all_tools(self.tools)\n            self.remove_input_schema_from_build_config(build_config, tool_name, input_schema_for_all_tools)\n\n            # Get and validate new inputs\n            self.schema_inputs = await self._validate_schema_inputs(tool_obj)\n            if not self.schema_inputs:\n                msg = f\"No input parameters to configure for tool '{tool_name}'\"\n                await logger.ainfo(msg)\n                return\n\n            # Add new inputs to build config\n            for schema_input in self.schema_inputs:\n                if not schema_input or not hasattr(schema_input, \"name\"):\n                    msg = \"Invalid schema input detected, skipping\"\n                    await logger.awarning(msg)\n                    continue\n\n                try:\n                    name = schema_input.name\n                    input_dict = schema_input.to_dict()\n                    input_dict.setdefault(\"value\", None)\n                    input_dict.setdefault(\"required\", True)\n\n                    build_config[name] = input_dict\n\n                    # Preserve existing value if the parameter name exists in current_values\n                    if name in current_values:\n                        build_config[name][\"value\"] = current_values[name]\n\n                except (AttributeError, KeyError, TypeError) as e:\n                    msg = f\"Error processing schema input {schema_input}: {e!s}\"\n                    await logger.aexception(msg)\n                    continue\n        except ValueError as e:\n            msg = f\"Schema validation error for tool {tool_name}: {e!s}\"\n            await logger.aexception(msg)\n            self.schema_inputs = []\n            return\n        except (AttributeError, KeyError, TypeError) as e:\n            msg = f\"Error updating tool config: {e!s}\"\n            await logger.aexception(msg)\n            raise ValueError(msg) from e\n\n    async def build_output(self) -> DataFrame:\n        \"\"\"Build output with improved error handling and validation.\"\"\"\n        try:\n            self.tools, _ = await self.update_tool_list()\n            if self.tool != \"\":\n                # Set session context for persistent MCP sessions using Langflow session ID\n                session_context = self._get_session_context()\n                if session_context:\n                    self.stdio_client.set_session_context(session_context)\n                    self.sse_client.set_session_context(session_context)\n\n                exec_tool = self._tool_cache[self.tool]\n                tool_args = self.get_inputs_for_all_tools(self.tools)[self.tool]\n                kwargs = {}\n                for arg in tool_args:\n                    value = getattr(self, arg.name, None)\n                    if value is not None:\n                        if isinstance(value, Message):\n                            kwargs[arg.name] = value.text\n                        else:\n                            kwargs[arg.name] = value\n\n                unflattened_kwargs = maybe_unflatten_dict(kwargs)\n\n                output = await exec_tool.coroutine(**unflattened_kwargs)\n\n                tool_content = []\n                for item in output.content:\n                    item_dict = item.model_dump()\n                    tool_content.append(item_dict)\n                return DataFrame(data=tool_content)\n            return DataFrame(data=[{\"error\": \"You must select a tool\"}])\n        except Exception as e:\n            msg = f\"Error in build_output: {e!s}\"\n            await logger.aexception(msg)\n            raise ValueError(msg) from e\n\n    def _get_session_context(self) -> str | None:\n        \"\"\"Get the Langflow session ID for MCP session caching.\"\"\"\n        # Try to get session ID from the component's execution context\n        if hasattr(self, \"graph\") and hasattr(self.graph, \"session_id\"):\n            session_id = self.graph.session_id\n            # Include server name to ensure different servers get different sessions\n            server_name = \"\"\n            mcp_server = getattr(self, \"mcp_server\", None)\n            if isinstance(mcp_server, dict):\n                server_name = mcp_server.get(\"name\", \"\")\n            elif mcp_server:\n                server_name = str(mcp_server)\n            return f\"{session_id}_{server_name}\" if session_id else None\n        return None\n\n    async def _get_tools(self):\n        \"\"\"Get cached tools or update if necessary.\"\"\"\n        mcp_server = getattr(self, \"mcp_server\", None)\n        if not self._not_load_actions:\n            tools, _ = await self.update_tool_list(mcp_server)\n            return tools\n        return []\n"
              },
              "mcp_server": {
                "_input_type": "McpInput",
                "advanced": false,
                "display_name": "MCP Server",
                "dynamic": false,
                "info": "Select the MCP Server that will be used by this component",
                "name": "mcp_server",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "mcp",
                "value": {
                  "config": {},
                  "name": "seungwon_mcp"
                }
              },
              "tool": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Tool",
                "dynamic": false,
                "external_options": {},
                "info": "Select the tool to execute",
                "name": "tool",
                "options": [],
                "options_metadata": [],
                "placeholder": "",
                "real_time_refresh": true,
                "required": true,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "Placeholder for the tool",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools_metadata": {
                "_input_type": "ToolsInput",
                "advanced": false,
                "display_name": "Actions",
                "dynamic": false,
                "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "tools_metadata",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tools",
                "value": [
                  {
                    "args": {
                      "args": {
                        "anyOf": [
                          {
                            "type": "string"
                          },
                          {
                            "type": "null"
                          }
                        ],
                        "default": "",
                        "title": "Args"
                      },
                      "python_exe": {
                        "anyOf": [
                          {
                            "type": "string"
                          },
                          {
                            "type": "null"
                          }
                        ],
                        "default": "",
                        "title": "Python Exe"
                      }
                    },
                    "description": "\n[중요] E01 파싱 등 오래 걸리는 포렌식 작업을 '백그라운드'에서 시작합니다.\n이 툴은 즉시 'job_id'를 문자열로 반환하며, 타임아웃되지 않습니다.\n작업 상태 및 결과는 'check_forensic_job_status' 툴로 확인해야 합니다.\n",
                    "display_description": "\n[중요] E01 파싱 등 오래 걸리는 포렌식 작업을 '백그라운드'에서 시작합니다.\n이 툴은 즉시 'job_id'를 문자열로 반환하며, 타임아웃되지 않습니다.\n작업 상태 및 결과는 'check_forensic_job_status' 툴로 확인해야 합니다.\n",
                    "display_name": "start_forensic_job",
                    "name": "start_forensic_job",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "start_forensic_job"
                    ]
                  },
                  {
                    "args": {
                      "job_id": {
                        "title": "Job Id",
                        "type": "string"
                      },
                      "tail_lines": {
                        "anyOf": [
                          {
                            "type": "integer"
                          },
                          {
                            "type": "null"
                          }
                        ],
                        "default": 30,
                        "title": "Tail Lines"
                      }
                    },
                    "description": "\n'start_forensic_job'으로 시작된 작업의 '현재 상태'와 '로그 꼬리'를 확인합니다.\n반환값은 사람이 읽기 좋은 요약 문자열입니다.\n",
                    "display_description": "\n'start_forensic_job'으로 시작된 작업의 '현재 상태'와 '로그 꼬리'를 확인합니다.\n반환값은 사람이 읽기 좋은 요약 문자열입니다.\n",
                    "display_name": "check_forensic_job_status",
                    "name": "check_forensic_job_status",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "check_forensic_job_status"
                    ]
                  }
                ]
              }
            },
            "tool_mode": true
          },
          "showNode": true,
          "type": "MCPTools"
        },
        "dragging": false,
        "id": "MCPTools-xU7UW",
        "measured": {
          "height": 311,
          "width": 320
        },
        "position": {
          "x": -5791.049755878125,
          "y": 3429.673495878628
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-BfTMc",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "category": "agents",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "n_messages",
              "format_instructions",
              "output_schema",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "key": "Agent",
            "last_updated": "2025-12-17T05:43:35.805Z",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.027777777777777776,
            "template": {
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Language Model",
                "dynamic": false,
                "external_options": {
                  "fields": {
                    "data": {
                      "node": {
                        "display_name": "Connect other models",
                        "icon": "CornerDownLeft",
                        "name": "connect_other_models"
                      }
                    }
                  }
                },
                "info": "The provider of the language model that the agent will use to generate responses.",
                "input_types": [
                  "LanguageModel"
                ],
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "OpenAI"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "OpenAI"
                  }
                ],
                "placeholder": "Awaiting model input.",
                "real_time_refresh": true,
                "refresh_button": false,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nimport re\n\nfrom langchain_core.tools import StructuredTool\nfrom pydantic import ValidationError\n\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.agents.events import ExceptionWithMessageError\nfrom langflow.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS_DICT,\n    MODELS_METADATA,\n)\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers.current_date import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import (\n    ToolCallingAgentComponent,\n)\nfrom langflow.custom.custom_component.component import _get_component_toolkit\nfrom langflow.custom.utils import update_component_build_config\nfrom langflow.field_typing import Tool\nfrom langflow.helpers.base_model import build_model_from_schema\nfrom langflow.io import (\n    BoolInput,\n    DropdownInput,\n    IntInput,\n    MultilineInput,\n    Output,\n    TableInput,\n)\nfrom langflow.logging import logger\nfrom langflow.schema.data import Data\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\nfrom langflow.schema.table import EditMode\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nMODEL_PROVIDERS_LIST = [\"Anthropic\", \"Google Generative AI\", \"OpenAI\"]\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    # Filter out json_mode from OpenAI inputs since we handle structured output differently\n    openai_inputs_filtered = [\n        input_field\n        for input_field in MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"]\n        if not (hasattr(input_field, \"name\") and input_field.name == \"json_mode\")\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            refresh_button=False,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST],\n            external_options={\n                \"fields\": {\n                    \"data\": {\n                        \"node\": {\n                            \"name\": \"connect_other_models\",\n                            \"display_name\": \"Connect other models\",\n                            \"icon\": \"CornerDownLeft\",\n                        }\n                    }\n                },\n            },\n        ),\n        *openai_inputs_filtered,\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        MultilineInput(\n            name=\"format_instructions\",\n            display_name=\"Output Format Instructions\",\n            info=\"Generic Template for structured output formatting. Valid only with Structured response.\",\n            value=(\n                \"You are an AI that extracts structured JSON objects from unstructured text. \"\n                \"Use a predefined schema with expected types (str, int, float, bool, dict). \"\n                \"Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. \"\n                \"Fill missing or ambiguous values with defaults: null for missing values. \"\n                \"Remove exact duplicates but keep variations that have different field values. \"\n                \"Always return valid JSON in the expected format, never throw errors. \"\n                \"If multiple objects can be extracted, return them all in the structured format.\"\n            ),\n            advanced=True,\n        ),\n        TableInput(\n            name=\"output_schema\",\n            display_name=\"Output Schema\",\n            info=(\n                \"Schema Validation: Define the structure and data types for structured output. \"\n                \"No validation if no output schema.\"\n            ),\n            advanced=True,\n            required=False,\n            value=[],\n            table_schema=[\n                {\n                    \"name\": \"name\",\n                    \"display_name\": \"Name\",\n                    \"type\": \"str\",\n                    \"description\": \"Specify the name of the output field.\",\n                    \"default\": \"field\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n                {\n                    \"name\": \"description\",\n                    \"display_name\": \"Description\",\n                    \"type\": \"str\",\n                    \"description\": \"Describe the purpose of the output field.\",\n                    \"default\": \"description of field\",\n                    \"edit_mode\": EditMode.POPOVER,\n                },\n                {\n                    \"name\": \"type\",\n                    \"display_name\": \"Type\",\n                    \"type\": \"str\",\n                    \"edit_mode\": EditMode.INLINE,\n                    \"description\": (\"Indicate the data type of the output field (e.g., str, int, float, bool, dict).\"),\n                    \"options\": [\"str\", \"int\", \"float\", \"bool\", \"dict\"],\n                    \"default\": \"str\",\n                },\n                {\n                    \"name\": \"multiple\",\n                    \"display_name\": \"As List\",\n                    \"type\": \"boolean\",\n                    \"description\": \"Set to True if this output field should be a list of the specified type.\",\n                    \"default\": \"False\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n            ],\n        ),\n        *LCToolsAgentComponent._base_inputs,\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [\n        Output(name=\"response\", display_name=\"Response\", method=\"message_response\"),\n    ]\n\n    async def get_agent_requirements(self):\n        \"\"\"Get the agent requirements for the agent.\"\"\"\n        llm_model, display_name = await self.get_llm()\n        if llm_model is None:\n            msg = \"No language model selected. Please choose a model to proceed.\"\n            raise ValueError(msg)\n        self.model_name = get_model_name(llm_model, display_name=display_name)\n\n        # Get memory data\n        self.chat_history = await self.get_memory_data()\n        if isinstance(self.chat_history, Message):\n            self.chat_history = [self.chat_history]\n\n        # Add current date tool if enabled\n        if self.add_current_date_tool:\n            if not isinstance(self.tools, list):  # type: ignore[has-type]\n                self.tools = []\n            current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n            if not isinstance(current_date_tool, StructuredTool):\n                msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                raise TypeError(msg)\n            self.tools.append(current_date_tool)\n        return llm_model, self.chat_history, self.tools\n\n    async def message_response(self) -> Message:\n        try:\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            result = await self.run_agent(agent)\n\n            # Store result for potential JSON output\n            self._agent_result = result\n\n        except (ValueError, TypeError, KeyError) as e:\n            await logger.aerror(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            await logger.aerror(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        # Avoid catching blind Exception; let truly unexpected exceptions propagate\n        except Exception as e:\n            await logger.aerror(f\"Unexpected error: {e!s}\")\n            raise\n        else:\n            return result\n\n    def _preprocess_schema(self, schema):\n        \"\"\"Preprocess schema to ensure correct data types for build_model_from_schema.\"\"\"\n        processed_schema = []\n        for field in schema:\n            processed_field = {\n                \"name\": str(field.get(\"name\", \"field\")),\n                \"type\": str(field.get(\"type\", \"str\")),\n                \"description\": str(field.get(\"description\", \"\")),\n                \"multiple\": field.get(\"multiple\", False),\n            }\n            # Ensure multiple is handled correctly\n            if isinstance(processed_field[\"multiple\"], str):\n                processed_field[\"multiple\"] = processed_field[\"multiple\"].lower() in [\n                    \"true\",\n                    \"1\",\n                    \"t\",\n                    \"y\",\n                    \"yes\",\n                ]\n            processed_schema.append(processed_field)\n        return processed_schema\n\n    async def build_structured_output_base(self, content: str):\n        \"\"\"Build structured output with optional BaseModel validation.\"\"\"\n        json_pattern = r\"\\{.*\\}\"\n        schema_error_msg = \"Try setting an output schema\"\n\n        # Try to parse content as JSON first\n        json_data = None\n        try:\n            json_data = json.loads(content)\n        except json.JSONDecodeError:\n            json_match = re.search(json_pattern, content, re.DOTALL)\n            if json_match:\n                try:\n                    json_data = json.loads(json_match.group())\n                except json.JSONDecodeError:\n                    return {\"content\": content, \"error\": schema_error_msg}\n            else:\n                return {\"content\": content, \"error\": schema_error_msg}\n\n        # If no output schema provided, return parsed JSON without validation\n        if not hasattr(self, \"output_schema\") or not self.output_schema or len(self.output_schema) == 0:\n            return json_data\n\n        # Use BaseModel validation with schema\n        try:\n            processed_schema = self._preprocess_schema(self.output_schema)\n            output_model = build_model_from_schema(processed_schema)\n\n            # Validate against the schema\n            if isinstance(json_data, list):\n                # Multiple objects\n                validated_objects = []\n                for item in json_data:\n                    try:\n                        validated_obj = output_model.model_validate(item)\n                        validated_objects.append(validated_obj.model_dump())\n                    except ValidationError as e:\n                        await logger.aerror(f\"Validation error for item: {e}\")\n                        # Include invalid items with error info\n                        validated_objects.append({\"data\": item, \"validation_error\": str(e)})\n                return validated_objects\n\n            # Single object\n            try:\n                validated_obj = output_model.model_validate(json_data)\n                return [validated_obj.model_dump()]  # Return as list for consistency\n            except ValidationError as e:\n                await logger.aerror(f\"Validation error: {e}\")\n                return [{\"data\": json_data, \"validation_error\": str(e)}]\n\n        except (TypeError, ValueError) as e:\n            await logger.aerror(f\"Error building structured output: {e}\")\n            # Fallback to parsed JSON without validation\n            return json_data\n\n    async def json_response(self) -> Data:\n        \"\"\"Convert agent response to structured JSON Data output with schema validation.\"\"\"\n        # Always use structured chat agent for JSON response mode for better JSON formatting\n        try:\n            system_components = []\n\n            # 1. Agent Instructions (system_prompt)\n            agent_instructions = getattr(self, \"system_prompt\", \"\") or \"\"\n            if agent_instructions:\n                system_components.append(f\"{agent_instructions}\")\n\n            # 2. Format Instructions\n            format_instructions = getattr(self, \"format_instructions\", \"\") or \"\"\n            if format_instructions:\n                system_components.append(f\"Format instructions: {format_instructions}\")\n\n            # 3. Schema Information from BaseModel\n            if hasattr(self, \"output_schema\") and self.output_schema and len(self.output_schema) > 0:\n                try:\n                    processed_schema = self._preprocess_schema(self.output_schema)\n                    output_model = build_model_from_schema(processed_schema)\n                    schema_dict = output_model.model_json_schema()\n                    schema_info = (\n                        \"You are given some text that may include format instructions, \"\n                        \"explanations, or other content alongside a JSON schema.\\n\\n\"\n                        \"Your task:\\n\"\n                        \"- Extract only the JSON schema.\\n\"\n                        \"- Return it as valid JSON.\\n\"\n                        \"- Do not include format instructions, explanations, or extra text.\\n\\n\"\n                        \"Input:\\n\"\n                        f\"{json.dumps(schema_dict, indent=2)}\\n\\n\"\n                        \"Output (only JSON schema):\"\n                    )\n                    system_components.append(schema_info)\n                except (ValidationError, ValueError, TypeError, KeyError) as e:\n                    await logger.aerror(f\"Could not build schema for prompt: {e}\", exc_info=True)\n\n            # Combine all components\n            combined_instructions = \"\\n\\n\".join(system_components) if system_components else \"\"\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=combined_instructions,\n            )\n\n            # Create and run structured chat agent\n            try:\n                structured_agent = self.create_agent_runnable()\n            except (NotImplementedError, ValueError, TypeError) as e:\n                await logger.aerror(f\"Error with structured chat agent: {e}\")\n                raise\n            try:\n                result = await self.run_agent(structured_agent)\n            except (\n                ExceptionWithMessageError,\n                ValueError,\n                TypeError,\n                RuntimeError,\n            ) as e:\n                await logger.aerror(f\"Error with structured agent result: {e}\")\n                raise\n            # Extract content from structured agent result\n            if hasattr(result, \"content\"):\n                content = result.content\n            elif hasattr(result, \"text\"):\n                content = result.text\n            else:\n                content = str(result)\n\n        except (\n            ExceptionWithMessageError,\n            ValueError,\n            TypeError,\n            NotImplementedError,\n            AttributeError,\n        ) as e:\n            await logger.aerror(f\"Error with structured chat agent: {e}\")\n            # Fallback to regular agent\n            content_str = \"No content returned from agent\"\n            return Data(data={\"content\": content_str, \"error\": str(e)})\n\n        # Process with structured output validation\n        try:\n            structured_output = await self.build_structured_output_base(content)\n\n            # Handle different output formats\n            if isinstance(structured_output, list) and structured_output:\n                if len(structured_output) == 1:\n                    return Data(data=structured_output[0])\n                return Data(data={\"results\": structured_output})\n            if isinstance(structured_output, dict):\n                return Data(data=structured_output)\n            return Data(data={\"content\": content})\n\n        except (ValueError, TypeError) as e:\n            await logger.aerror(f\"Error in structured output processing: {e}\")\n            return Data(data={\"content\": content, \"error\": str(e)})\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(\n                session_id=self.graph.session_id,\n                order=\"Ascending\",\n                n_messages=self.n_messages,\n            )\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    async def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except (AttributeError, ValueError, TypeError, RuntimeError) as e:\n            await logger.aerror(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            # Filter out json_mode and only use attributes that exist on this component\n            model_kwargs = {}\n            for input_ in inputs:\n                if hasattr(self, f\"{prefix}{input_.name}\"):\n                    model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n                build_config[\"agent_llm\"][\"display_name\"] = \"Model Provider\"\n            elif field_value == \"connect_other_models\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    info=\"The provider of the language model that the agent will use to generate responses.\",\n                    options=[*MODEL_PROVIDERS_LIST],\n                    real_time_refresh=True,\n                    refresh_button=False,\n                    input_types=[\"LanguageModel\"],\n                    placeholder=\"Awaiting model input.\",\n                    options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST],\n                    external_options={\n                        \"fields\": {\n                            \"data\": {\n                                \"node\": {\n                                    \"name\": \"connect_other_models\",\n                                    \"display_name\": \"Connect other models\",\n                                    \"icon\": \"CornerDownLeft\",\n                                },\n                            }\n                        },\n                    },\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name = field_name.replace(prefix, \"\")\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = _get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\",\n            tool_description=description,\n            callbacks=self.get_langchain_callbacks(),\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n        return tools\n"
              },
              "format_instructions": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Output Format Instructions",
                "dynamic": false,
                "info": "Generic Template for structured output formatting. Valid only with Structured response.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "format_instructions",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are an AI that extracts structured JSON objects from unstructured text. Use a predefined schema with expected types (str, int, float, bool, dict). Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. Fill missing or ambiguous values with defaults: null for missing values. Remove exact duplicates but keep variations that have different field values. Always return valid JSON in the expected format, never throw errors. If multiple objects can be extracted, return them all in the structured format."
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 15
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "output_schema": {
                "_input_type": "TableInput",
                "advanced": true,
                "display_name": "Output Schema",
                "dynamic": false,
                "info": "Schema Validation: Define the structure and data types for structured output. No validation if no output schema.",
                "input_types": [],
                "is_list": true,
                "list_add_label": "Add More",
                "name": "output_schema",
                "placeholder": "",
                "required": false,
                "show": true,
                "table_icon": "Table",
                "table_schema": {
                  "columns": [
                    {
                      "default": "field",
                      "description": "Specify the name of the output field.",
                      "disable_edit": false,
                      "display_name": "Name",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "name",
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": "description of field",
                      "description": "Describe the purpose of the output field.",
                      "disable_edit": false,
                      "display_name": "Description",
                      "edit_mode": "popover",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "description",
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": "str",
                      "description": "Indicate the data type of the output field (e.g., str, int, float, bool, dict).",
                      "disable_edit": false,
                      "display_name": "Type",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "type",
                      "options": [
                        "str",
                        "int",
                        "float",
                        "bool",
                        "dict"
                      ],
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": false,
                      "description": "Set to True if this output field should be a list of the specified type.",
                      "disable_edit": false,
                      "display_name": "As List",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "boolean",
                      "hidden": false,
                      "name": "multiple",
                      "sortable": true,
                      "type": "boolean"
                    }
                  ]
                },
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "trigger_icon": "Table",
                "trigger_text": "Open table",
                "type": "table",
                "value": []
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are a helpful assistant that can use tools to answer questions and perform tasks."
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "dragging": false,
        "id": "Agent-BfTMc",
        "measured": {
          "height": 427,
          "width": 320
        },
        "position": {
          "x": -4776.2666548321,
          "y": 3296.781681549978
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-r4e17",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "너는 포렌식 파이프라인 실행 전용 에이전트다.\n이 플로우가 실행될 때 해야 할 기본 동작은 다음과 같다.\n[핵심 원칙]\nstart_forensic_job`을 호출\n\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-r4e17",
        "measured": {
          "height": 203,
          "width": 320
        },
        "position": {
          "x": -5241.499194818938,
          "y": 3646.7893518218466
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-UtzGD",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "너는 디지털 포렌식 파이프라인에서 \"사용자 질문 → 태그 조합\"을 만드는 태그 추천기(포렌식 조수)다.\n\n역할:\n- 사용자의 질문을 읽고, 관련된 아티팩트/행위/위치/위험/시간/상태를 태그 조합으로 표현한다.\n- 태그는 \"검색용 필터\"이며, 핵심만 고른다.\n- 각 카테고리(ARTIFACT/EVENT/AREA/SEC/FORMAT/ACT/TIME/STATE)당 최대 3개까지만 선택한다.\n- 사용자가 질문 끝에 \"키워드: xxx\" 또는 \"keyword: xxx\"를 명시한 경우에만 keywords 배열을 채운다(질문 본문에서 임의 추출 금지).\n- 날짜(연/월/일이 모두 숫자로 명시된 정확한 날짜)만 time_parsed에 구조화한다.\n\n────────────────────\n[사용 가능한 태그]\n────────────────────\n\n1) ARTIFACT_\n- ARTIFACT_REGISTRY\n- ARTIFACT_EVENT_LOG\n- ARTIFACT_PREFETCH\n- ARTIFACT_LNK\n- ARTIFACT_JUMPLIST\n- ARTIFACT_SHELLBAG\n- ARTIFACT_AMCACHE\n- ARTIFACT_SRUM\n- ARTIFACT_USN_JOURNAL\n- ARTIFACT_MFT\n- ARTIFACT_RECYCLE_BIN\n- ARTIFACT_BROWSER_HISTORY\n- ARTIFACT_BROWSER_COOKIE\n- ARTIFACT_BROWSER_CACHE\n- ARTIFACT_BROWSER_DOWNLOAD\n- ARTIFACT_EMAIL\n- ARTIFACT_DB\n- ARTIFACT_ESE_DB\n- ARTIFACT_SCHEDULED_TASK\n- ARTIFACT_WMI\n- ARTIFACT_BITS\n- ARTIFACT_THUMBNAIL\n- ARTIFACT_MEMORY_DUMP\n- ARTIFACT_VSS\n\n2) EVENT_\n- EVENT_CREATE\n- EVENT_MODIFY\n- EVENT_DELETE\n- EVENT_RENAME\n- EVENT_MOVE\n- EVENT_COPY\n- EVENT_RECOVERED\n- EVENT_ACCESSED\n- EVENT_EXECUTED\n- EVENT_BROWSER_VISIT\n- EVENT_BROWSER_FORM_SUBMIT\n- EVENT_BROWSER_DOWNLOAD\n- EVENT_BROWSER_COOKIE_ACCESS\n- EVENT_BROWSER_CACHE_READ\n- EVENT_BROWSER_PASSWORD_ACCESS\n- EVENT_BROWSER_TAB_ACTIVITY\n- EVENT_BROWSER_EXTENSION_CHANGE\n\n3) AREA_\n- AREA_SYSTEM32\n- AREA_WINDOWS\n- AREA_USER_DESKTOP\n- AREA_USER_DOCUMENTS\n- AREA_USER_DOWNLOADS\n- AREA_USER_RECENT\n- AREA_APPDATA_LOCAL\n- AREA_APPDATA_ROAMING\n- AREA_APPDATA_LOCALLOW\n- AREA_PROGRAMFILES\n- AREA_PROGRAMDATA\n- AREA_TEMP\n- AREA_STARTUP\n- AREA_PREFETCH\n- AREA_RECYCLE_BIN\n- AREA_VSS\n- AREA_NETWORK_SHARE\n- AREA_EXTERNAL_DRIVE\n\n4) SEC_\n- SEC_EXECUTABLE\n- SEC_HIDDEN_EXECUTABLE\n- SEC_SCRIPT\n- SEC_SUSPICIOUS_NAME\n- SEC_SUSPICIOUS_PATH\n- SEC_SUSPICIOUS_EXTENSION\n- SEC_PERSISTENCE_REGISTRY\n- SEC_PERSISTENCE_STARTUP\n- SEC_PERSISTENCE_TASK\n- SEC_PERSISTENCE_WMI\n- SEC_FIREWALL_RELATED\n- SEC_DEFENDER_DISABLED\n- SEC_LOG_CLEARED\n- SEC_CREDENTIAL_ACCESS\n- SEC_LATERAL_MOVEMENT\n- SEC_PRIVILEGE_ESCALATION\n- SEC_RANSOMWARE_INDICATOR\n- SEC_EXFILTRATION\n\n5) FORMAT_\n- FORMAT_DOCUMENT\n- FORMAT_SPREADSHEET\n- FORMAT_PRESENTATION\n- FORMAT_IMAGE\n- FORMAT_VIDEO\n- FORMAT_AUDIO\n- FORMAT_ARCHIVE\n- FORMAT_EXECUTABLE\n- FORMAT_SCRIPT\n- FORMAT_DATABASE\n- FORMAT_LOG\n- FORMAT_CONFIG\n- FORMAT_REGISTRY\n- FORMAT_EMAIL\n- FORMAT_SHORTCUT\n\n6) ACT_\n- ACT_DOWNLOAD\n- ACT_UPLOAD\n- ACT_INSTALL\n- ACT_UNINSTALL\n- ACT_EXECUTE\n- ACT_BROWSING\n- ACT_SEARCH\n- ACT_COMMUNICATION\n- ACT_FILE_OPERATION\n- ACT_NETWORK_ACCESS\n\n7) TIME_\n- TIME_RECENT\n- TIME_WEEK\n- TIME_MONTH\n- TIME_OLD\n- TIME_CREATED\n- TIME_MODIFIED\n- TIME_ACCESSED\n- TIME_MFT_CREATED\n- TIME_FN_CREATED\n\n8) STATE_\n- STATE_ACTIVE\n- STATE_DELETED\n- STATE_ENCRYPTED\n- STATE_COMPRESSED\n- STATE_HIDDEN\n- STATE_READONLY\n- STATE_SYSTEM\n- STATE_CORRUPTED\n- STATE_SLACK_SPACE\n- STATE_UNALLOCATED\n\n────────────────────\n[키워드 처리 규칙]\n────────────────────\n\n1) 키워드 추출\n- 질문에서 \"키워드:\" 또는 \"keyword:\"가 포함된 줄을 찾는다.\n- 그 줄에서 \"키워드:\" 뒤의 내용을 키워드 전체 문자열로 간주한다.\n- 쉼표(,), 슬래시(/), 공백(스페이스) 등으로 구분된 경우 분리하여 각각 하나의 키워드로 만든다.\n- 각 키워드는 원형을 최대한 보존하되, 앞뒤 공백만 제거한다.\n- 키워드가 없으면 keywords: [] 로 둔다.\n- 질문 본문에서 임의로 키워드를 “추출”하지 않는다(반드시 키워드 줄에서만).\n\n2) [특수 정규화 규칙: 이벤트 로그 ID]\n- keywords 토큰(분리된 키워드들) 안에 아래 조건이 동시에 성립하면, eventlog_로 정규화한다.\n  조건 A) \"이벤트로그\" 또는 \"eventlog\" 또는 \"event log\" 중 하나가 존재\n  조건 B) \"ID\" 또는 \"id\" 또는 \"eventid\" 또는 \"event id\" 또는 \"이벤트id\" 또는 \"이벤트 ID\" 중 하나가 존재\n  조건 C) 숫자 ID가 존재 (예: 1102, 4624 등)\n- 동작:\n  - 숫자 ID를 모두 추출한다(여러 개면 모두).\n  - 추출한 각 숫자에 대해 keywords에 \"eventlog_<숫자>\" 형태로 추가한다.\n  - 기존에 있던 \"이벤트로그/eventlog/ID/eventid\" 등의 토큰은 keywords에서 제거한다(중복 방지).\n- 예시:\n  - \"키워드: 이벤트로그, ID 1102\" → [\"eventlog_1102\"]\n  - \"키워드: eventlog eventid 4624, 4688\" → [\"eventlog_4624\",\"eventlog_4688\"]\n  - \"키워드: 이벤트로그ID:1102 / 로그삭제\" → [\"eventlog_1102\",\"로그삭제\"]\n\n────────────────────\n[날짜 파싱 규칙 – time_parsed]\n────────────────────\n\n- 질문 안의 “연·월·일이 모두 숫자로 명시된 날짜”만 반영한다.\n- 허용되는 날짜 문자열:\n  - 2025-11-24 / 2025/11/24 / 2025.11.24 / 2025년 11월 24일\n- time_parsed.type:\n  - 날짜가 없으면: type=\"none\", start=null, end=null\n  - 정확한 날짜 1개면: type=\"single\", start=end=\"YYYY-MM-DD\"\n  - 정확한 날짜 2개 이상이면: type=\"range\", start=가장 이른 날짜, end=가장 늦은 날짜\n- “최근 1주일/지난 한 달/작년” 같은 상대 기간 표현은 time_parsed에 넣지 말고 TIME_ 태그로만 표현한다.\n\n────────────────────\n[태그 선택 원칙]\n────────────────────\n\n- 질문 의도에 직접 관련 있는 태그만 고른다(과다 선택 금지).\n- 같은 카테고리 내 여러 태그는 OR 의미, 카테고리 간은 AND 의미로 사용된다고 가정한다.\n- 각 카테고리당 최대 3개까지만 선택한다.\n- 정의된 태그만 사용하며, 새 태그를 만들지 않는다.\n\n────────────────────\n[출력 JSON 형식]\n────────────────────\n반드시 아래 형식의 JSON “한 개”만 출력한다. JSON 바깥 텍스트는 절대 출력하지 않는다.\n\n{\n  \"question\": \"<사용자 질문 원문 그대로>\",\n  \"keywords\": [\"<키워드1>\", \"<키워드2>\", ...],\n  \"ARTIFACT\": [ ... ],\n  \"EVENT\": [ ... ],\n  \"AREA\": [ ... ],\n  \"SEC\": [ ... ],\n  \"FORMAT\": [ ... ],\n  \"ACT\": [ ... ],\n  \"TIME\": [ ... ],\n  \"STATE\": [ ... ],\n  \"time_parsed\": {\n    \"type\": \"none\" | \"single\" | \"range\",\n    \"start\": \"YYYY-MM-DD\" 또는 null,\n    \"end\": \"YYYY-MM-DD\" 또는 null\n  },\n  \"reason\": \"<선택한 태그와 키워드/날짜 해석을 1~3문장으로 짧게 설명(한국어)>\"\n}\n\n추가 규칙:\n- question에는 '키워드:' 줄이 포함되어 있어도 원문 그대로 넣는다.\n- keywords는 반드시 '키워드:' 줄에서만 추출하며, 위 이벤트로그 ID 정규화 규칙을 적용한다.\n- 빈 카테고리는 []로 둔다.\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-UtzGD",
        "measured": {
          "height": 203,
          "width": 320
        },
        "position": {
          "x": -3880.01930971204,
          "y": 3479.9652735470727
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-P0Y5W",
          "node": {
            "base_classes": [
              "Text"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "CSV 적재 스크립트가 띄운 http://127.0.0.1:8002/ready ",
            "display_name": "Wait For Ready Server",
            "documentation": "",
            "edited": true,
            "field_order": [
              "trigger",
              "url",
              "interval_sec"
            ],
            "frozen": false,
            "icon": "Clock",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Message",
                "group_outputs": false,
                "hidden": null,
                "method": "run",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Text",
                "tool_mode": true,
                "types": [
                  "Text"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import time\r\nimport requests\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import Output, IntInput, StrInput, HandleInput\r\n\r\n\r\nclass WaitForReadyServer(Component):\r\n    display_name = \"Wait For Ready Server\"\r\n    description = \"CSV 적재 스크립트가 띄운 http://127.0.0.1:8002/ready \"\r\n    icon = \"Clock\"\r\n    name = \"WaitForReadyServer\"\r\n\r\n    inputs = [\r\n        # 의미 없는 입력(앞 노드랑 연결만 하기 위한 용도)\r\n        HandleInput(\r\n            name=\"trigger\",\r\n            display_name=\"Trigger Input\",\r\n            info=\"이 값은 사용하지 않고, 앞 노드와의 연결을 위한 더미 입력입니다.\",\r\n            input_types=[\"Text\", \"Message\", \"Data\"],\r\n            required=False,\r\n        ),\r\n        StrInput(\r\n            name=\"url\",\r\n            display_name=\"Ready URL\",\r\n            info=\"준비 신호를 확인할 URL\",\r\n            value=\"http://127.0.0.1:8002/ready\",\r\n            required=False,\r\n        ),\r\n        IntInput(\r\n            name=\"interval_sec\",\r\n            display_name=\"Interval (sec)\",\r\n            info=\"재시도 간격(초 단위)\",\r\n            value=3,\r\n            required=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Message\",\r\n            name=\"message\",\r\n            method=\"run\",\r\n            output_type=\"Text\",\r\n        ),\r\n    ]\r\n\r\n    def run(\r\n        self,\r\n        trigger=None,  # 의미 없는 입력, 사용 안 함\r\n        url: str = \"http://127.0.0.1:8002/ready\",\r\n        interval_sec: int = 3,\r\n    ) -> str:\r\n        interval_sec = max(1, int(interval_sec))\r\n\r\n        while True:\r\n            try:\r\n                resp = requests.get(url, timeout=2)\r\n                if resp.status_code == 200:\r\n                    return \"[INFO] Ready server responded 200. Proceed to next node.\"\r\n            except Exception as e:\r\n                # 연결 실패하면 그냥 다시 시도\r\n                print(f\"[INFO] Ready server not yet available: {e}\")\r\n\r\n            time.sleep(interval_sec)\r\n"
              },
              "interval_sec": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Interval (sec)",
                "dynamic": false,
                "info": "재시도 간격(초 단위)",
                "list": false,
                "list_add_label": "Add More",
                "name": "interval_sec",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 3
              },
              "trigger": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Trigger Input",
                "dynamic": false,
                "info": "이 값은 사용하지 않고, 앞 노드와의 연결을 위한 더미 입력입니다.",
                "input_types": [
                  "Text",
                  "Message",
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "trigger",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Ready URL",
                "dynamic": false,
                "info": "준비 신호를 확인할 URL",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://127.0.0.1:8002/ready"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "WaitForReadyServer"
        },
        "dragging": false,
        "id": "CustomComponent-P0Y5W",
        "measured": {
          "height": 345,
          "width": 320
        },
        "position": {
          "x": -4326.263224351213,
          "y": 3085.7600947626347
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-BS6P2",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "last_updated": "2025-10-05T19:05:46.155Z",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "name": "model_name",
                "options": [
                  "qwen/qwen3-4b-thinking-2507",
                  "qwen/qwen3-4b-2507",
                  "text-embedding-bge-reranker-v2-m3",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "text-embedding-nomic-embed-text-v1.5",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qwen/qwen3-4b-thinking-2507"
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-BS6P2",
        "measured": {
          "height": 449,
          "width": 320
        },
        "position": {
          "x": -137.5201091288627,
          "y": 2829.562694696081
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-pO11O",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Ollama, Gemini, LM Studio 등 다양한 모델의 응답을 보기 좋게 병합합니다.",
            "display_name": "Multi-LM Merger",
            "documentation": "",
            "edited": true,
            "field_order": [
              "ollama",
              "gemini",
              "lmstudio"
            ],
            "frozen": false,
            "icon": "merge",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Formatted Output",
                "group_outputs": false,
                "hidden": null,
                "method": "format_output",
                "name": "output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom.custom_component.component import Component\r\nfrom langflow.io import MessageTextInput, Output\r\nfrom langflow.schema.data import Data\r\nfrom langflow.schema.message import Message\r\n\r\n\r\nclass MultiLMFormatter(Component):\r\n    display_name = \"Multi-LM Merger\"\r\n    description = \"Ollama, Gemini, LM Studio 등 다양한 모델의 응답을 보기 좋게 병합합니다.\"\r\n    icon = \"merge\"\r\n    name = \"MultiLMFormatter\"\r\n\r\n    inputs = [\r\n        MessageTextInput(name=\"ollama\", display_name=\"🧠 Ollama 결과\"),\r\n        MessageTextInput(name=\"gemini\", display_name=\"🔍 Gemini 결과\"),\r\n        MessageTextInput(name=\"lmstudio\", display_name=\"💬 LM Studio 결과\"),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(name=\"output\", display_name=\"Formatted Output\", method=\"format_output\", type_=Message)\r\n    ]\r\n\r\n    def format_output(self) -> Message:\r\n        def format_block(title: str, content: str):\r\n            if content.strip():\r\n                return f\"## {title}\\n---\\n{content.strip()}\"\r\n            else:\r\n                return f\"## {title}\\n---\\n(응답 없음)\"\r\n\r\n        sections = []\r\n\r\n        # 그대로 유지 - LangFlow의 ChatOutput은 개행 포함 출력 가능\r\n        if hasattr(self, \"ollama\") and self.ollama:\r\n            sections.append(format_block(\"🧠 Ollama\", self.ollama))\r\n\r\n        if hasattr(self, \"gemini\") and self.gemini:\r\n            sections.append(format_block(\"🔍 Gemini\", self.gemini))\r\n\r\n        if hasattr(self, \"lmstudio\") and self.lmstudio:\r\n            sections.append(format_block(\"💬 LM Studio\", self.lmstudio))\r\n\r\n        final_output = \"\\n\\n\".join(sections) if sections else \"📭 아무 모델에서도 응답을 받지 못했습니다.\"\r\n\r\n        return Message(text=final_output)\r\n"
              },
              "gemini": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "🔍 Gemini 결과",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "gemini",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "lmstudio": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "💬 LM Studio 결과",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "lmstudio",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "ollama": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "🧠 Ollama 결과",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "ollama",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "MultiLMFormatter"
        },
        "dragging": false,
        "id": "CustomComponent-pO11O",
        "measured": {
          "height": 383,
          "width": 320
        },
        "position": {
          "x": 1021.685479720129,
          "y": 2620.899796440555
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-CHoVB",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/components-io#chat-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _icon, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-CHoVB",
        "measured": {
          "height": 165,
          "width": 320
        },
        "position": {
          "x": 1491.874493938326,
          "y": 2746.410035220389
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-ykZ8B",
          "node": {
            "base_classes": [
              "Message",
              "NoneType"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "trigger가 들어올 때만 main_message를 통과시킵니다.",
            "display_name": "Gate By Trigger",
            "documentation": "",
            "edited": true,
            "field_order": [
              "main_message",
              "trigger"
            ],
            "frozen": false,
            "icon": "filter",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output",
                "group_outputs": false,
                "hidden": null,
                "method": "run",
                "name": "out",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message",
                  "NoneType"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Optional, Any\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import MessageInput, Output\r\nfrom langflow.schema.message import Message\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass GateByTrigger(Component):\r\n    display_name = \"Gate By Trigger\"\r\n    description = \"trigger가 들어올 때만 main_message를 통과시킵니다.\"\r\n    icon = \"filter\"\r\n    name = \"GateByTrigger\"\r\n\r\n    inputs = [\r\n        MessageInput(\r\n            name=\"main_message\",\r\n            display_name=\"Main Message\",\r\n            input_types=[\"Message\", \"Text\"],\r\n            required=True,\r\n            value=\"\",   # ✅ None 방지\r\n        ),\r\n        MessageInput(\r\n            name=\"trigger\",\r\n            display_name=\"Trigger (required to pass)\",\r\n            input_types=[\"Message\", \"Text\"],\r\n            required=False,\r\n            value=\"\",   # ✅ None 방지 (여기가 핵심)\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(name=\"out\", display_name=\"Output\", method=\"run\", type_=Message)\r\n    ]\r\n\r\n    def _to_text(self, x: Any) -> str:\r\n        if x is None:\r\n            return \"\"\r\n        if isinstance(x, Message):\r\n            return x.text or \"\"\r\n        if isinstance(x, Data):\r\n            try:\r\n                return str(x.data or \"\")\r\n            except Exception:\r\n                return \"\"\r\n        if isinstance(x, str):\r\n            return x\r\n        try:\r\n            return str(x)\r\n        except Exception:\r\n            return \"\"\r\n\r\n    def run(self) -> Optional[Message]:\r\n        t = self._to_text(getattr(self, \"trigger\", \"\")).strip()\r\n        if not t:\r\n            # trigger 없으면 아무것도 내보내지 않음\r\n            return None\r\n\r\n        return self.main_message\r\n"
              },
              "main_message": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Main Message",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "main_message",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "trigger": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Trigger (required to pass)",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "trigger",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "GateByTrigger"
        },
        "dragging": false,
        "id": "CustomComponent-ykZ8B",
        "measured": {
          "height": 301,
          "width": 320
        },
        "position": {
          "x": -5799.999667823157,
          "y": 2874.1200522256136
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ConditionalRouter-OIPJs",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Routes True if artifact_all has at least 1 row, else False.",
            "display_name": "DB If-Else (artifact_all has rows)",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_message",
              "pass_through",
              "host",
              "port",
              "dbname",
              "user",
              "password",
              "true_case_message",
              "false_case_message",
              "max_iterations"
            ],
            "frozen": false,
            "icon": "split",
            "legacy": false,
            "lf_version": "1.6.7",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "True",
                "group_outputs": true,
                "hidden": null,
                "method": "true_response",
                "name": "true_result",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "False",
                "group_outputs": true,
                "hidden": null,
                "method": "false_response",
                "name": "false_result",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from __future__ import annotations\r\n\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    IntInput,\r\n    MessageInput,\r\n    Output,\r\n    SecretStrInput,\r\n    StrInput,\r\n)\r\nfrom langflow.schema.message import Message\r\n\r\n\r\nclass DBArtifactAllExistsRouter(Component):\r\n    display_name = \"DB If-Else (artifact_all has rows)\"\r\n    description = \"Routes True if artifact_all has at least 1 row, else False.\"\r\n    icon = \"split\"\r\n    name = \"DBArtifactAllExistsRouter\"\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.__iteration_updated = False\r\n\r\n    inputs = [\r\n        # (선택) 분기 후 그대로 다음 노드로 넘길 메시지\r\n        MessageInput(\r\n            name=\"input_message\",\r\n            display_name=\"Input Message (pass-through)\",\r\n            info=\"If pass_through=True, forwards this message on the selected branch.\",\r\n            required=False,\r\n        ),\r\n        BoolInput(\r\n            name=\"pass_through\",\r\n            display_name=\"Pass Through Input Message\",\r\n            info=\"If true, forwards input_message on the selected branch.\",\r\n            value=True,\r\n            advanced=True,\r\n        ),\r\n\r\n        # DB_CONFIG를 Langflow 입력으로 받기\r\n        StrInput(name=\"host\", display_name=\"DB Host\", value=\"localhost\", required=True),\r\n        IntInput(name=\"port\", display_name=\"DB Port\", value=5432, required=True),\r\n        StrInput(name=\"dbname\", display_name=\"DB Name\", value=\"forensic_db\", required=True),\r\n        StrInput(name=\"user\", display_name=\"DB User\", value=\"postgres\", required=True),\r\n        SecretStrInput(name=\"password\", display_name=\"DB Password\", required=True),\r\n\r\n        # 분기 시 내보낼 메시지(선택)\r\n        MessageInput(\r\n            name=\"true_case_message\",\r\n            display_name=\"Case True Message\",\r\n            info=\"Used when pass_through=False (or input_message missing).\",\r\n            required=False,\r\n            advanced=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"false_case_message\",\r\n            display_name=\"Case False Message\",\r\n            info=\"Used when pass_through=False (or input_message missing).\",\r\n            required=False,\r\n            advanced=True,\r\n        ),\r\n\r\n        # 사이클 안전장치(필요할 때만)\r\n        IntInput(\r\n            name=\"max_iterations\",\r\n            display_name=\"Max Iterations (cycle-safe)\",\r\n            value=10,\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"True\", name=\"true_result\", method=\"true_response\", group_outputs=True),\r\n        Output(display_name=\"False\", name=\"false_result\", method=\"false_response\", group_outputs=True),\r\n    ]\r\n\r\n    def _pre_run_setup(self):\r\n        self.__iteration_updated = False\r\n\r\n    def _select_output_message(self, is_true: bool) -> Message:\r\n        if getattr(self, \"pass_through\", True) and getattr(self, \"input_message\", None):\r\n            return self.input_message\r\n\r\n        if is_true and getattr(self, \"true_case_message\", None):\r\n            return self.true_case_message\r\n        if (not is_true) and getattr(self, \"false_case_message\", None):\r\n            return self.false_case_message\r\n\r\n        return Message(content=\"\")\r\n\r\n    def _stop_other_branch_once(self, route_to_stop: str):\r\n        # ConditionalRouterComponent처럼 사이클에서 무한 루프 방지용\r\n        if self.__iteration_updated:\r\n            return\r\n\r\n        self.update_ctx({f\"{self._id}_iteration\": self.ctx.get(f\"{self._id}_iteration\", 0) + 1})\r\n        self.__iteration_updated = True\r\n        current_iteration = self.ctx.get(f\"{self._id}_iteration\", 0)\r\n\r\n        # max_iterations 넘어가면 stop만 하고 끝(최소 안전장치)\r\n        if current_iteration >= self.max_iterations:\r\n            self.stop(route_to_stop)\r\n            return\r\n\r\n        self.stop(route_to_stop)\r\n        self.graph.exclude_branch_conditionally(self._id, output_name=route_to_stop)\r\n\r\n    def _artifact_all_has_rows(self) -> bool:\r\n        try:\r\n            import psycopg2  # type: ignore\r\n\r\n            conn = psycopg2.connect(\r\n                host=str(self.host),\r\n                port=int(self.port),\r\n                dbname=str(self.dbname),\r\n                user=str(self.user),\r\n                password=str(self.password),\r\n            )\r\n            try:\r\n                conn.autocommit = True\r\n                with conn.cursor() as cur:\r\n                    cur.execute(\"SELECT 1 FROM artifact_all LIMIT 1;\")\r\n                    return cur.fetchone() is not None\r\n            finally:\r\n                conn.close()\r\n\r\n        except Exception as e:\r\n            # DB 에러는 False로 보내고 상태에 기록\r\n            self.status = f\"[DB check error] {type(e).__name__}: {e}\"\r\n            return False\r\n\r\n    def true_response(self) -> Message:\r\n        exists = self._artifact_all_has_rows()\r\n        if exists:\r\n            msg = self._select_output_message(True)\r\n            self.status = msg\r\n            self._stop_other_branch_once(\"false_result\")\r\n            return msg\r\n\r\n        self._stop_other_branch_once(\"true_result\")\r\n        return Message(content=\"\")\r\n\r\n    def false_response(self) -> Message:\r\n        exists = self._artifact_all_has_rows()\r\n        if not exists:\r\n            msg = self._select_output_message(False)\r\n            self.status = msg\r\n            self._stop_other_branch_once(\"true_result\")\r\n            return msg\r\n\r\n        self._stop_other_branch_once(\"false_result\")\r\n        return Message(content=\"\")\r\n"
              },
              "dbname": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "DB Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "dbname",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "forensic_db"
              },
              "false_case_message": {
                "_input_type": "MessageInput",
                "advanced": true,
                "display_name": "Case False Message",
                "dynamic": false,
                "info": "Used when pass_through=False (or input_message missing).",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "false_case_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "host": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "DB Host",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "host",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "localhost"
              },
              "input_message": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input Message (pass-through)",
                "dynamic": false,
                "info": "If pass_through=True, forwards this message on the selected branch.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations (cycle-safe)",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 10
              },
              "pass_through": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Pass Through Input Message",
                "dynamic": false,
                "info": "If true, forwards input_message on the selected branch.",
                "list": false,
                "list_add_label": "Add More",
                "name": "pass_through",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "password": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "DB Password",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "password",
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "port": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "DB Port",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "port",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 5432
              },
              "true_case_message": {
                "_input_type": "MessageInput",
                "advanced": true,
                "display_name": "Case True Message",
                "dynamic": false,
                "info": "Used when pass_through=False (or input_message missing).",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "true_case_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "user": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "DB User",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "user",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "postgres"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "DBArtifactAllExistsRouter"
        },
        "dragging": false,
        "id": "ConditionalRouter-OIPJs",
        "measured": {
          "height": 673,
          "width": 320
        },
        "position": {
          "x": -5940.6462127809,
          "y": 1988.7519539710213
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 3224.8610279505174,
      "y": -1063.9829211133515,
      "zoom": 0.561231067684683
    }
  },
  "description": "Load your data for chat context with Retrieval Augmented Generation.",
  "endpoint_name": null,
  "id": "be9f1653-0245-4bac-b4cb-5b9018973bfd",
  "is_component": false,
  "last_tested_version": "1.6.7",
  "name": "1차태그 테스트",
  "tags": [
    "openai",
    "astradb",
    "rag",
    "q-a"
  ]
}