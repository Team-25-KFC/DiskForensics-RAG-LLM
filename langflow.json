{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "parser",
            "id": "parser-Z3nso",
            "name": "parsed_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "context",
            "id": "Prompt-G4fxK",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-parser-Z3nso{œdataTypeœ:œparserœ,œidœ:œparser-Z3nsoœ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-G4fxK{œfieldNameœ:œcontextœ,œidœ:œPrompt-G4fxKœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "parser-Z3nso",
        "sourceHandle": "{œdataTypeœ:œparserœ,œidœ:œparser-Z3nsoœ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-G4fxK",
        "targetHandle": "{œfieldNameœ:œcontextœ,œidœ:œPrompt-G4fxKœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "File",
            "id": "File-BlymP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "data_inputs",
            "id": "SplitText-u0gyp",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-File-BlymP{œdataTypeœ:œFileœ,œidœ:œFile-BlymPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-SplitText-u0gyp{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-u0gypœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "File-BlymP",
        "sourceHandle": "{œdataTypeœ:œFileœ,œidœ:œFile-BlymPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "SplitText-u0gyp",
        "targetHandle": "{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-u0gypœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioEmbeddingsComponent",
            "id": "LMStudioEmbeddingsComponent-OcmK9",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "Milvus-mYB10",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__LMStudioEmbeddingsComponent-YCNlt{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-YCNltœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Milvus-z3V2G{œfieldNameœ:œembeddingœ,œidœ:œMilvus-z3V2Gœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "LMStudioEmbeddingsComponent-YCNlt",
        "sourceHandle": "{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-YCNltœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "Milvus-z3V2G",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œMilvus-z3V2Gœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioEmbeddingsComponent",
            "id": "LMStudioEmbeddingsComponent-Ilrp9",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "Milvus-3RBiF",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__LMStudioEmbeddingsComponent-Ilrp9{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-Ilrp9œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Milvus-3RBiF{œfieldNameœ:œembeddingœ,œidœ:œMilvus-3RBiFœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "LMStudioEmbeddingsComponent-Ilrp9",
        "sourceHandle": "{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-Ilrp9œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "Milvus-3RBiF",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œMilvus-3RBiFœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "SplitText",
            "id": "SplitText-u0gyp",
            "name": "dataframe",
            "output_types": [
              "DataFrame"
            ]
          },
          "targetHandle": {
            "fieldName": "ingest_data",
            "id": "Milvus-3RBiF",
            "inputTypes": [
              "Data",
              "DataFrame"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__SplitText-u0gyp{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-u0gypœ,œnameœ:œdataframeœ,œoutput_typesœ:[œDataFrameœ]}-Milvus-3RBiF{œfieldNameœ:œingest_dataœ,œidœ:œMilvus-3RBiFœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "SplitText-u0gyp",
        "sourceHandle": "{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-u0gypœ,œnameœ:œdataframeœ,œoutput_typesœ:[œDataFrameœ]}",
        "target": "Milvus-3RBiF",
        "targetHandle": "{œfieldNameœ:œingest_dataœ,œidœ:œMilvus-3RBiFœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Milvus",
            "id": "Milvus-z3V2G",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "parser-Z3nso",
            "inputTypes": [
              "DataFrame",
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Milvus-z3V2G{œdataTypeœ:œMilvusœ,œidœ:œMilvus-z3V2Gœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-parser-Z3nso{œfieldNameœ:œinput_dataœ,œidœ:œparser-Z3nsoœ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Milvus-z3V2G",
        "sourceHandle": "{œdataTypeœ:œMilvusœ,œidœ:œMilvus-z3V2Gœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
        "target": "parser-Z3nso",
        "targetHandle": "{œfieldNameœ:œinput_dataœ,œidœ:œparser-Z3nsoœ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-MjZb4",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          },
          "targetHandle": {
            "fieldName": "agent_llm",
            "id": "Agent-yAZ5W",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-9qtbB{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-9qtbBœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-Agent-WMPgY{œfieldNameœ:œagent_llmœ,œidœ:œAgent-WMPgYœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-9qtbB",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-9qtbBœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "Agent-WMPgY",
        "targetHandle": "{œfieldNameœ:œagent_llmœ,œidœ:œAgent-WMPgYœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Agent",
            "id": "Agent-WMPgY",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-fAm9M",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Agent-WMPgY{œdataTypeœ:œAgentœ,œidœ:œAgent-WMPgYœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-fAm9M{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-fAm9Mœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Agent-WMPgY",
        "sourceHandle": "{œdataTypeœ:œAgentœ,œidœ:œAgent-WMPgYœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-fAm9M",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-fAm9Mœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-G4fxK",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "Agent-WMPgY",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt-G4fxK{œdataTypeœ:œPromptœ,œidœ:œPrompt-G4fxKœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Agent-WMPgY{œfieldNameœ:œinput_valueœ,œidœ:œAgent-WMPgYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-G4fxK",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-G4fxKœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Agent-WMPgY",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œAgent-WMPgYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-EUPnO",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          },
          "targetHandle": {
            "fieldName": "agent_llm",
            "id": "Agent-Lrn6a",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-LMStudioModel-EUPnO{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-EUPnOœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-Agent-Lrn6a{œfieldNameœ:œagent_llmœ,œidœ:œAgent-Lrn6aœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-EUPnO",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-EUPnOœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "Agent-Lrn6a",
        "targetHandle": "{œfieldNameœ:œagent_llmœ,œidœ:œAgent-Lrn6aœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Agent",
            "id": "Agent-Lrn6a",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-PlODE",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Agent-Lrn6a{œdataTypeœ:œAgentœ,œidœ:œAgent-Lrn6aœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-PlODE{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-PlODEœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Agent-Lrn6a",
        "sourceHandle": "{œdataTypeœ:œAgentœ,œidœ:œAgent-Lrn6aœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-PlODE",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-PlODEœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "MCPTools",
            "id": "MCPTools-HE57I",
            "name": "component_as_tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-Lrn6a",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-MCPTools-HE57I{œdataTypeœ:œMCPToolsœ,œidœ:œMCPTools-HE57Iœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}-Agent-Lrn6a{œfieldNameœ:œtoolsœ,œidœ:œAgent-Lrn6aœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "MCPTools-HE57I",
        "sourceHandle": "{œdataTypeœ:œMCPToolsœ,œidœ:œMCPTools-HE57Iœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-Lrn6a",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-Lrn6aœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "MCPTools",
            "id": "MCPTools-n7itv",
            "name": "component_as_tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-WMPgY",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__MCPTools-n7itv{œdataTypeœ:œMCPToolsœ,œidœ:œMCPTools-n7itvœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}-Agent-WMPgY{œfieldNameœ:œtoolsœ,œidœ:œAgent-WMPgYœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "MCPTools-n7itv",
        "sourceHandle": "{œdataTypeœ:œMCPToolsœ,œidœ:œMCPTools-n7itvœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-WMPgY",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-WMPgYœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioEmbeddingsComponent",
            "id": "LMStudioEmbeddingsComponent-OcmK9",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "Milvus-mYB10",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-LMStudioEmbeddingsComponent-OcmK9{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-OcmK9œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Milvus-mYB10{œfieldNameœ:œembeddingœ,œidœ:œMilvus-mYB10œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "LMStudioEmbeddingsComponent-OcmK9",
        "sourceHandle": "{œdataTypeœ:œLMStudioEmbeddingsComponentœ,œidœ:œLMStudioEmbeddingsComponent-OcmK9œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "Milvus-mYB10",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œMilvus-mYB10œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Reranker",
            "id": "CustomComponent-LaYcW",
            "name": "output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-EbS2H",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__CustomComponent-LaYcW{œdataTypeœ:œRerankerœ,œidœ:œCustomComponent-LaYcWœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-ChatOutput-EbS2H{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-EbS2Hœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "CustomComponent-LaYcW",
        "sourceHandle": "{œdataTypeœ:œRerankerœ,œidœ:œCustomComponent-LaYcWœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}",
        "target": "ChatOutput-EbS2H",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-EbS2Hœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatOutput",
            "id": "ChatOutput-EbS2H",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "context",
            "id": "Prompt-YxJU5",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatOutput-EbS2H{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-EbS2Hœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-YxJU5{œfieldNameœ:œcontextœ,œidœ:œPrompt-YxJU5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatOutput-EbS2H",
        "sourceHandle": "{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-EbS2Hœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-YxJU5",
        "targetHandle": "{œfieldNameœ:œcontextœ,œidœ:œPrompt-YxJU5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-9Tnph",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "LMStudioModel-r484n",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-9Tnph{œdataTypeœ:œChatInputœ,œidœ:œChatInput-9Tnphœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-r484n{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-r484nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-9Tnph",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-9Tnphœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-r484n",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-r484nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "HyDEParser",
            "id": "PythonREPLComponent-gCXyv",
            "name": "keywords",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_keywords",
            "id": "PythonREPLComponent-GRKxY",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__PythonREPLComponent-gCXyv{œdataTypeœ:œHyDEParserœ,œidœ:œPythonREPLComponent-gCXyvœ,œnameœ:œkeywordsœ,œoutput_typesœ:[œMessageœ]}-PythonREPLComponent-GRKxY{œfieldNameœ:œsearch_keywordsœ,œidœ:œPythonREPLComponent-GRKxYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "PythonREPLComponent-gCXyv",
        "sourceHandle": "{œdataTypeœ:œHyDEParserœ,œidœ:œPythonREPLComponent-gCXyvœ,œnameœ:œkeywordsœ,œoutput_typesœ:[œMessageœ]}",
        "target": "PythonREPLComponent-GRKxY",
        "targetHandle": "{œfieldNameœ:œsearch_keywordsœ,œidœ:œPythonREPLComponent-GRKxYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Milvus",
            "id": "Milvus-mYB10",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-kJwhc",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Milvus-mYB10{œdataTypeœ:œMilvusœ,œidœ:œMilvus-mYB10œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ChatOutput-kJwhc{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-kJwhcœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Milvus-mYB10",
        "sourceHandle": "{œdataTypeœ:œMilvusœ,œidœ:œMilvus-mYB10œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
        "target": "ChatOutput-kJwhc",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-kJwhcœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-1Ap2N",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "LMStudioModel-MjZb4",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__TextInput-1Ap2N{œdataTypeœ:œTextInputœ,œidœ:œTextInput-1Ap2Nœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-MjZb4{œfieldNameœ:œsystem_messageœ,œidœ:œLMStudioModel-MjZb4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-1Ap2N",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-1Ap2Nœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-MjZb4",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œLMStudioModel-MjZb4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-YxJU5",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "LMStudioModel-MjZb4",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt-YxJU5{œdataTypeœ:œPromptœ,œidœ:œPrompt-YxJU5œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-MjZb4{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-MjZb4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-YxJU5",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-YxJU5œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-MjZb4",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-MjZb4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-MjZb4",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-hOiPh",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-MjZb4{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-MjZb4œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-hOiPh{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-hOiPhœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-MjZb4",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-MjZb4œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-hOiPh",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-hOiPhœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "BM25MilvusSearch",
            "id": "PythonREPLComponent-GRKxY",
            "name": "results",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "bm25_results",
            "id": "PythonREPLComponent-dY4IO",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__PythonREPLComponent-GRKxY{œdataTypeœ:œBM25MilvusSearchœ,œidœ:œPythonREPLComponent-GRKxYœ,œnameœ:œresultsœ,œoutput_typesœ:[œMessageœ]}-PythonREPLComponent-dY4IO{œfieldNameœ:œbm25_resultsœ,œidœ:œPythonREPLComponent-dY4IOœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "PythonREPLComponent-GRKxY",
        "sourceHandle": "{œdataTypeœ:œBM25MilvusSearchœ,œidœ:œPythonREPLComponent-GRKxYœ,œnameœ:œresultsœ,œoutput_typesœ:[œMessageœ]}",
        "target": "PythonREPLComponent-dY4IO",
        "targetHandle": "{œfieldNameœ:œbm25_resultsœ,œidœ:œPythonREPLComponent-dY4IOœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-YITzY",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "LMStudioModel-r484n",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__TextInput-YITzY{œdataTypeœ:œTextInputœ,œidœ:œTextInput-YITzYœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-r484n{œfieldNameœ:œsystem_messageœ,œidœ:œLMStudioModel-r484nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-YITzY",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-YITzYœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-r484n",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œLMStudioModel-r484nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-r484n",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "Milvus-mYB10",
            "inputTypes": [
              "Message"
            ],
            "type": "query"
          }
        },
        "id": "xy-edge__LMStudioModel-r484n{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-r484nœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Milvus-mYB10{œfieldNameœ:œsearch_queryœ,œidœ:œMilvus-mYB10œ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}",
        "selected": false,
        "source": "LMStudioModel-r484n",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-r484nœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Milvus-mYB10",
        "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œMilvus-mYB10œ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatOutput",
            "id": "ChatOutput-kJwhc",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "docs",
            "id": "CustomComponent-LaYcW",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatOutput-kJwhc{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-kJwhcœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-LaYcW{œfieldNameœ:œdocsœ,œidœ:œCustomComponent-LaYcWœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatOutput-kJwhc",
        "sourceHandle": "{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-kJwhcœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-LaYcW",
        "targetHandle": "{œfieldNameœ:œdocsœ,œidœ:œCustomComponent-LaYcWœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-r484n",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "hyde_output",
            "id": "HyDEParser-Sgux2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-r484n{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-r484nœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-HyDEParser-Sgux2{œfieldNameœ:œhyde_outputœ,œidœ:œHyDEParser-Sgux2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-r484n",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-r484nœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "HyDEParser-Sgux2",
        "targetHandle": "{œfieldNameœ:œhyde_outputœ,œidœ:œHyDEParser-Sgux2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "HyDEParser",
            "id": "HyDEParser-Sgux2",
            "name": "question",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "query",
            "id": "CustomComponent-LaYcW",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__HyDEParser-Sgux2{œdataTypeœ:œHyDEParserœ,œidœ:œHyDEParser-Sgux2œ,œnameœ:œquestionœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-LaYcW{œfieldNameœ:œqueryœ,œidœ:œCustomComponent-LaYcWœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "HyDEParser-Sgux2",
        "sourceHandle": "{œdataTypeœ:œHyDEParserœ,œidœ:œHyDEParser-Sgux2œ,œnameœ:œquestionœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-LaYcW",
        "targetHandle": "{œfieldNameœ:œqueryœ,œidœ:œCustomComponent-LaYcWœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-r484n",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-YxJU5",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-r484n{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-r484nœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-YxJU5{œfieldNameœ:œquestionœ,œidœ:œPrompt-YxJU5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-r484n",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-r484nœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-YxJU5",
        "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-YxJU5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "description": "Get chat inputs from the Playground.",
          "display_name": "Chat Input",
          "id": "ChatInput-9Tnph",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-input\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        background_color = self.background_color\n        text_color = self.text_color\n        icon = self.chat_icon\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\n                \"background_color\": background_color,\n                \"text_color\": text_color,\n                \"icon\": icon,\n            },\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "advanced": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "사용자 정보를 알려줘"
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            }
          },
          "selected_output": "message",
          "type": "ChatInput"
        },
        "dragging": false,
        "height": 234,
        "id": "ChatInput-9Tnph",
        "measured": {
          "height": 234,
          "width": 320
        },
        "position": {
          "x": -4025.359936853334,
          "y": 1717.035621221612
        },
        "positionAbsolute": {
          "x": 743.9745420290319,
          "y": 463.6977510207854
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-G4fxK",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "context",
                "question"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "method": "build_prompt",
                "name": "prompt",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "context": {
                "advanced": false,
                "display_name": "context",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "context",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nQuestion: {question}\n\nAnswer: "
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "height": 433,
        "id": "Prompt-G4fxK",
        "measured": {
          "height": 433,
          "width": 320
        },
        "position": {
          "x": 2085.2103441841223,
          "y": 896.6507653679114
        },
        "positionAbsolute": {
          "x": 1977.9097981422992,
          "y": 640.5656416923846
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "description": "Split text into chunks based on specified criteria.",
          "display_name": "Split Text",
          "id": "SplitText-u0gyp",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Split text into chunks based on specified criteria.",
            "display_name": "Split Text",
            "documentation": "",
            "edited": false,
            "field_order": [
              "data_inputs",
              "chunk_overlap",
              "chunk_size",
              "separator"
            ],
            "frozen": false,
            "icon": "scissors-line-dashed",
            "legacy": false,
            "lf_version": "1.1.1",
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chunks",
                "group_outputs": false,
                "method": "split_text",
                "name": "dataframe",
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "chunk_overlap": {
                "advanced": false,
                "display_name": "Chunk Overlap",
                "dynamic": false,
                "info": "Number of characters to overlap between chunks.",
                "list": false,
                "name": "chunk_overlap",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 200
              },
              "chunk_size": {
                "advanced": false,
                "display_name": "Chunk Size",
                "dynamic": false,
                "info": "The maximum length of each chunk. Text is first split by separator, then chunks are merged up to this size. Individual splits larger than this won't be further divided.",
                "list": false,
                "name": "chunk_size",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1000
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    documentation: str = \"https://docs.langflow.org/components-processing#split-text\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Input\",\n            info=\"The data with texts to split in chunks.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=(\n                \"The maximum length of each chunk. Text is first split by separator, \"\n                \"then chunks are merged up to this size. \"\n                \"Individual splits larger than this won't be further divided.\"\n            ),\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=(\n                \"The character to split on. Use \\\\n for newline. \"\n                \"Examples: \\\\n\\\\n for paragraphs, \\\\n for lines, . for sentences\"\n            ),\n            value=\"\\n\",\n        ),\n        MessageTextInput(\n            name=\"text_key\",\n            display_name=\"Text Key\",\n            info=\"The key to use for the text column.\",\n            value=\"text\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"keep_separator\",\n            display_name=\"Keep Separator\",\n            info=\"Whether to keep the separator in the output chunks and where to place it.\",\n            options=[\"False\", \"True\", \"Start\", \"End\"],\n            value=\"False\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"dataframe\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs) -> list[Data]:\n        return [Data(text=doc.page_content, data=doc.metadata) for doc in docs]\n\n    def _fix_separator(self, separator: str) -> str:\n        \"\"\"Fix common separator issues and convert to proper format.\"\"\"\n        if separator == \"/n\":\n            return \"\\n\"\n        if separator == \"/t\":\n            return \"\\t\"\n        return separator\n\n    def split_text_base(self):\n        separator = self._fix_separator(self.separator)\n        separator = unescape_string(separator)\n\n        if isinstance(self.data_inputs, DataFrame):\n            if not len(self.data_inputs):\n                msg = \"DataFrame is empty\"\n                raise TypeError(msg)\n\n            self.data_inputs.text_key = self.text_key\n            try:\n                documents = self.data_inputs.to_lc_documents()\n            except Exception as e:\n                msg = f\"Error converting DataFrame to documents: {e}\"\n                raise TypeError(msg) from e\n        elif isinstance(self.data_inputs, Message):\n            self.data_inputs = [self.data_inputs.to_data()]\n            return self.split_text_base()\n        else:\n            if not self.data_inputs:\n                msg = \"No data inputs provided\"\n                raise TypeError(msg)\n\n            documents = []\n            if isinstance(self.data_inputs, Data):\n                self.data_inputs.text_key = self.text_key\n                documents = [self.data_inputs.to_lc_document()]\n            else:\n                try:\n                    documents = [input_.to_lc_document() for input_ in self.data_inputs if isinstance(input_, Data)]\n                    if not documents:\n                        msg = f\"No valid Data inputs found in {type(self.data_inputs)}\"\n                        raise TypeError(msg)\n                except AttributeError as e:\n                    msg = f\"Invalid input type in collection: {e}\"\n                    raise TypeError(msg) from e\n        try:\n            # Convert string 'False'/'True' to boolean\n            keep_sep = self.keep_separator\n            if isinstance(keep_sep, str):\n                if keep_sep.lower() == \"false\":\n                    keep_sep = False\n                elif keep_sep.lower() == \"true\":\n                    keep_sep = True\n                # 'start' and 'end' are kept as strings\n\n            splitter = CharacterTextSplitter(\n                chunk_overlap=self.chunk_overlap,\n                chunk_size=self.chunk_size,\n                separator=separator,\n                keep_separator=keep_sep,\n            )\n            return splitter.split_documents(documents)\n        except Exception as e:\n            msg = f\"Error splitting text: {e}\"\n            raise TypeError(msg) from e\n\n    def split_text(self) -> DataFrame:\n        return DataFrame(self._docs_to_data(self.split_text_base()))\n"
              },
              "data_inputs": {
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The data with texts to split in chunks.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "name": "data_inputs",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "keep_separator": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Keep Separator",
                "dynamic": false,
                "info": "Whether to keep the separator in the output chunks and where to place it.",
                "name": "keep_separator",
                "options": [
                  "False",
                  "True",
                  "Start",
                  "End"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "False"
              },
              "separator": {
                "advanced": false,
                "display_name": "Separator",
                "dynamic": false,
                "info": "The character to split on. Use \\n for newline. Examples: \\n\\n for paragraphs, \\n for lines, . for sentences",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "separator",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              },
              "text_key": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Key",
                "dynamic": false,
                "info": "The key to use for the text column.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_key",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text"
              }
            }
          },
          "selected_output": "chunks",
          "type": "SplitText"
        },
        "dragging": false,
        "height": 475,
        "id": "SplitText-u0gyp",
        "measured": {
          "height": 475,
          "width": 320
        },
        "position": {
          "x": 1535.6659786421526,
          "y": 2735.8318601086444
        },
        "positionAbsolute": {
          "x": 1683.4543896546102,
          "y": 1350.7871623588553
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-fAm9M",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean the data",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([safe_convert(item, clean_data=self.clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "height": 234,
        "id": "ChatOutput-fAm9M",
        "measured": {
          "height": 234,
          "width": 320
        },
        "position": {
          "x": 3165.746610207051,
          "y": 811.2782670859343
        },
        "positionAbsolute": {
          "x": 2734.385670401691,
          "y": 810.6079786425926
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "id": "parser-Z3nso",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Format a DataFrame, Data object, or list of Data objects into text using a template. Enable 'Stringify' to convert input into a readable string instead.",
            "display_name": "Parser",
            "documentation": "",
            "edited": true,
            "field_order": [
              "mode",
              "pattern",
              "input_data",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Parsed Text",
                "group_outputs": false,
                "hidden": null,
                "method": "parse_combined_text",
                "name": "parsed_text",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\r\nfrom typing import Any\r\n\r\nfrom langflow.custom import Component\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    HandleInput,\r\n    MessageTextInput,\r\n    MultilineInput,\r\n    Output,\r\n    TabInput,\r\n)\r\nfrom langflow.schema import Data, DataFrame\r\nfrom langflow.schema.message import Message\r\n\r\n\r\nclass ParserComponent(Component):\r\n    name = \"parser\"\r\n    display_name = \"Parser\"\r\n    description = (\r\n        \"Format a DataFrame, Data object, or list of Data objects into text using a template. \"\r\n        \"Enable 'Stringify' to convert input into a readable string instead.\"\r\n    )\r\n    icon = \"braces\"\r\n\r\n    inputs = [\r\n        TabInput(\r\n            name=\"mode\",\r\n            display_name=\"Mode\",\r\n            options=[\"Parser\", \"Stringify\"],\r\n            value=\"Parser\",\r\n            info=\"Convert into raw string instead of using a template.\",\r\n            real_time_refresh=True,\r\n        ),\r\n        MultilineInput(\r\n            name=\"pattern\",\r\n            display_name=\"Template\",\r\n            info=(\r\n                \"Use variables within curly brackets to extract column values for DataFrames, \"\r\n                \"key values for Data, or for each Data in a list. \"\r\n                \"For example: `Name: {Name}, Age: {Age}, Country: {Country}`\"\r\n            ),\r\n            value=\"Text: {text}\",\r\n            dynamic=True,\r\n            show=True,\r\n            required=True,\r\n        ),\r\n        HandleInput(\r\n            name=\"input_data\",\r\n            display_name=\"Data, DataFrame or List of Data\",\r\n            input_types=[\"DataFrame\", \"Data\"],\r\n            info=\"Accepts DataFrame, Data object, or list of Data objects.\",\r\n            required=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"sep\",\r\n            display_name=\"Separator\",\r\n            advanced=True,\r\n            value=\"\\n\",\r\n            info=\"String used to separate rows/items.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Parsed Text\",\r\n            name=\"parsed_text\",\r\n            info=\"Formatted text output.\",\r\n            method=\"parse_combined_text\",\r\n        ),\r\n    ]\r\n\r\n    def update_build_config(self, build_config, field_value, field_name=None):\r\n        if field_name == \"mode\":\r\n            build_config[\"pattern\"][\"show\"] = self.mode == \"Parser\"\r\n            build_config[\"pattern\"][\"required\"] = self.mode == \"Parser\"\r\n            if field_value:\r\n                clean_data = BoolInput(\r\n                    name=\"clean_data\",\r\n                    display_name=\"Clean Data\",\r\n                    info=(\r\n                        \"Enable to clean the data by removing empty rows and lines \"\r\n                        \"in each cell of the DataFrame/ Data object.\"\r\n                    ),\r\n                    value=True,\r\n                    advanced=True,\r\n                    required=False,\r\n                )\r\n                build_config[\"clean_data\"] = clean_data.to_dict()\r\n            else:\r\n                build_config.pop(\"clean_data\", None)\r\n        return build_config\r\n\r\n    def _clean_args(self):\r\n        \"\"\"Prepare arguments based on input type.\"\"\"\r\n        input_data = self.input_data\r\n\r\n        match input_data:\r\n            case list() if all(isinstance(item, Data) for item in input_data):\r\n                # ✅ 리스트도 지원\r\n                return None, input_data\r\n            case DataFrame():\r\n                return input_data, None\r\n            case Data():\r\n                return None, input_data\r\n            case dict() if \"data\" in input_data:\r\n                try:\r\n                    if \"columns\" in input_data:\r\n                        return DataFrame.from_dict(input_data), None\r\n                    return None, Data(**input_data)\r\n                except (TypeError, ValueError, KeyError) as e:\r\n                    msg = f\"Invalid structured input provided: {e!s}\"\r\n                    raise ValueError(msg) from e\r\n            case _:\r\n                msg = f\"Unsupported input type: {type(input_data)}. Expected DataFrame, Data, or list of Data.\"\r\n                raise ValueError(msg)\r\n\r\n    def parse_combined_text(self) -> Message:\r\n        \"\"\"Parse rows/items into a single text or convert input to string if `stringify` is enabled.\"\"\"\r\n        if self.mode == \"Stringify\":\r\n            return self.convert_to_string()\r\n\r\n        df, data = self._clean_args()\r\n        lines = []\r\n\r\n        if df is not None:\r\n            for _, row in df.iterrows():\r\n                formatted_text = self.pattern.format(**row.to_dict())\r\n                lines.append(formatted_text)\r\n\r\n        elif isinstance(data, list):   # ✅ list of Data 처리\r\n            for item in data:\r\n                formatted_text = self.pattern.format(**item.data)\r\n                lines.append(formatted_text)\r\n\r\n        elif isinstance(data, Data):\r\n            formatted_text = self.pattern.format(**data.data)\r\n            lines.append(formatted_text)\r\n\r\n        combined_text = self.sep.join(lines)  # ✅ 합치기\r\n        self.status = combined_text\r\n        return Message(text=combined_text)\r\n\r\n    def _safe_convert(self, data: Any) -> str:\r\n        \"\"\"Safely convert input data to string.\"\"\"\r\n        try:\r\n            if isinstance(data, str):\r\n                return data\r\n            if isinstance(data, Message):\r\n                return data.get_text()\r\n            if isinstance(data, Data):\r\n                return json.dumps(data.data)\r\n            if isinstance(data, DataFrame):\r\n                if hasattr(self, \"clean_data\") and self.clean_data:\r\n                    data = data.dropna(how=\"all\")\r\n                    data = data.replace(r\"^\\s*$\", \"\", regex=True)\r\n                    data = data.replace(r\"\\n+\", \"\\n\", regex=True)\r\n                return data.to_markdown(index=False)\r\n            return str(data)\r\n        except (ValueError, TypeError, AttributeError) as e:\r\n            msg = f\"Error converting data: {e!s}\"\r\n            raise ValueError(msg) from e\r\n\r\n    def convert_to_string(self) -> Message:\r\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\r\n        result = \"\"\r\n        if isinstance(self.input_data, list):\r\n            result = \"\\n\".join([self._safe_convert(item) for item in self.input_data])\r\n        else:\r\n            result = self._safe_convert(self.input_data)\r\n        self.log(f\"Converted to string with length: {len(result)}\")\r\n\r\n        message = Message(text=result)\r\n        self.status = message\r\n        return message"
              },
              "input_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Data, DataFrame or List of Data",
                "dynamic": false,
                "info": "Accepts DataFrame, Data object, or list of Data objects.",
                "input_types": [
                  "DataFrame",
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_data",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "mode": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Mode",
                "dynamic": false,
                "info": "Convert into raw string instead of using a template.",
                "name": "mode",
                "options": [
                  "Parser",
                  "Stringify"
                ],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tab",
                "value": "Parser"
              },
              "pattern": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Template",
                "dynamic": true,
                "info": "Use variables within curly brackets to extract column values for DataFrames, key values for Data, or for each Data in a list. For example: `Name: {Name}, Age: {Age}, Country: {Country}`",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "pattern",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Text: {text}"
              },
              "sep": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "String used to separate rows/items.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sep",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              }
            },
            "tool_mode": false
          },
          "selected_output": "parsed_text",
          "showNode": true,
          "type": "parser"
        },
        "dragging": false,
        "id": "parser-Z3nso",
        "measured": {
          "height": 362,
          "width": 320
        },
        "position": {
          "x": 1550.5051391144125,
          "y": 732.0655748770347
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "File-BlymP",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Loads content from one or more files as a DataFrame.",
            "display_name": "File",
            "documentation": "",
            "edited": false,
            "field_order": [
              "path",
              "file_path",
              "separator",
              "silent_errors",
              "delete_server_file_after_processing",
              "ignore_unsupported_extensions",
              "ignore_unspecified_files",
              "use_multithreading",
              "concurrency_multithreading"
            ],
            "frozen": false,
            "icon": "file-text",
            "last_updated": "2025-10-29T16:16:15.700Z",
            "legacy": false,
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Raw Content",
                "group_outputs": false,
                "method": "load_files_message",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from copy import deepcopy\nfrom typing import Any\n\nfrom langflow.base.data.base_file import BaseFileComponent\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.io import BoolInput, FileInput, IntInput, Output\nfrom langflow.schema.data import Data\n\n\nclass FileComponent(BaseFileComponent):\n    \"\"\"Handles loading and processing of individual or zipped text files.\n\n    This component supports processing multiple valid files within a zip archive,\n    resolving paths, validating file types, and optionally using multithreading for processing.\n    \"\"\"\n\n    display_name = \"File\"\n    description = \"Loads content from one or more files.\"\n    documentation: str = \"https://docs.langflow.org/components-data#file\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    VALID_EXTENSIONS = TEXT_FILE_TYPES\n\n    _base_inputs = deepcopy(BaseFileComponent._base_inputs)\n\n    for input_item in _base_inputs:\n        if isinstance(input_item, FileInput) and input_item.name == \"path\":\n            input_item.real_time_refresh = True\n            break\n\n    inputs = [\n        *_base_inputs,\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"[Deprecated] Use Multithreading\",\n            advanced=True,\n            value=True,\n            info=\"Set 'Processing Concurrency' greater than 1 to enable multithreading.\",\n        ),\n        IntInput(\n            name=\"concurrency_multithreading\",\n            display_name=\"Processing Concurrency\",\n            advanced=True,\n            info=\"When multiple files are being processed, the number of files to process concurrently.\",\n            value=1,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Raw Content\", name=\"message\", method=\"load_files_message\"),\n    ]\n\n    def update_outputs(self, frontend_node: dict, field_name: str, field_value: Any) -> dict:\n        \"\"\"Dynamically show only the relevant output based on the number of files processed.\"\"\"\n        if field_name == \"path\":\n            # Add outputs based on the number of files in the path\n            if len(field_value) == 0:\n                return frontend_node\n\n            frontend_node[\"outputs\"] = []\n\n            if len(field_value) == 1:\n                # We need to check if the file is structured content\n                file_path = frontend_node[\"template\"][\"path\"][\"file_path\"][0]\n                if file_path.endswith((\".csv\", \".xlsx\", \".parquet\")):\n                    frontend_node[\"outputs\"].append(\n                        Output(display_name=\"Structured Content\", name=\"dataframe\", method=\"load_files_structured\"),\n                    )\n                elif file_path.endswith(\".json\"):\n                    frontend_node[\"outputs\"].append(\n                        Output(display_name=\"Structured Content\", name=\"json\", method=\"load_files_json\"),\n                    )\n\n                # All files get the raw content and path outputs\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"Raw Content\", name=\"message\", method=\"load_files_message\"),\n                )\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"File Path\", name=\"path\", method=\"load_files_path\"),\n                )\n            else:\n                # For multiple files, we only show the files output\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"Files\", name=\"dataframe\", method=\"load_files\"),\n                )\n\n        return frontend_node\n\n    def process_files(self, file_list: list[BaseFileComponent.BaseFile]) -> list[BaseFileComponent.BaseFile]:\n        \"\"\"Processes files either sequentially or in parallel, depending on concurrency settings.\n\n        Args:\n            file_list (list[BaseFileComponent.BaseFile]): List of files to process.\n\n        Returns:\n            list[BaseFileComponent.BaseFile]: Updated list of files with merged data.\n        \"\"\"\n\n        def process_file(file_path: str, *, silent_errors: bool = False) -> Data | None:\n            \"\"\"Processes a single file and returns its Data object.\"\"\"\n            try:\n                return parse_text_file_to_data(file_path, silent_errors=silent_errors)\n            except FileNotFoundError as e:\n                msg = f\"File not found: {file_path}. Error: {e}\"\n                self.log(msg)\n                if not silent_errors:\n                    raise\n                return None\n            except Exception as e:\n                msg = f\"Unexpected error processing {file_path}: {e}\"\n                self.log(msg)\n                if not silent_errors:\n                    raise\n                return None\n\n        if not file_list:\n            msg = \"No files to process.\"\n            raise ValueError(msg)\n\n        concurrency = 1 if not self.use_multithreading else max(1, self.concurrency_multithreading)\n        file_count = len(file_list)\n\n        parallel_processing_threshold = 2\n        if concurrency < parallel_processing_threshold or file_count < parallel_processing_threshold:\n            if file_count > 1:\n                self.log(f\"Processing {file_count} files sequentially.\")\n            processed_data = [process_file(str(file.path), silent_errors=self.silent_errors) for file in file_list]\n        else:\n            self.log(f\"Starting parallel processing of {file_count} files with concurrency: {concurrency}.\")\n            file_paths = [str(file.path) for file in file_list]\n            processed_data = parallel_load_data(\n                file_paths,\n                silent_errors=self.silent_errors,\n                load_function=process_file,\n                max_concurrency=concurrency,\n            )\n\n        # Use rollup_basefile_data to merge processed data with BaseFile objects\n        return self.rollup_data(file_list, processed_data)\n"
              },
              "concurrency_multithreading": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Processing Concurrency",
                "dynamic": false,
                "info": "When multiple files are being processed, the number of files to process concurrently.",
                "list": false,
                "list_add_label": "Add More",
                "name": "concurrency_multithreading",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "delete_server_file_after_processing": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Delete Server File After Processing",
                "dynamic": false,
                "info": "If true, the Server File Path will be deleted after processing.",
                "list": false,
                "list_add_label": "Add More",
                "name": "delete_server_file_after_processing",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "file_path": {
                "_input_type": "HandleInput",
                "advanced": true,
                "display_name": "Server File Path",
                "dynamic": false,
                "info": "Data object with a 'file_path' property pointing to server file or a Message object with a path to the file. Supercedes 'Path' but supports same file types.",
                "input_types": [
                  "Data",
                  "Message"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "file_path",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "ignore_unspecified_files": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Ignore Unspecified Files",
                "dynamic": false,
                "info": "If true, Data with no 'file_path' property will be ignored.",
                "list": false,
                "list_add_label": "Add More",
                "name": "ignore_unspecified_files",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "ignore_unsupported_extensions": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Ignore Unsupported Extensions",
                "dynamic": false,
                "info": "If true, files with unsupported extensions will not be processed.",
                "list": false,
                "list_add_label": "Add More",
                "name": "ignore_unsupported_extensions",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "path": {
                "_input_type": "FileInput",
                "advanced": false,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "zip",
                  "tar",
                  "tgz",
                  "bz2",
                  "gz"
                ],
                "file_path": [],
                "info": "Supported file extensions: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx; optionally bundled in file extensions: zip, tar, tgz, bz2, gz",
                "list": true,
                "list_add_label": "Add More",
                "name": "path",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "temp_file": false,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "separator": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "Specify the separator to use between multiple outputs in Message format.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "separator",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n\n"
              },
              "silent_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Silent Errors",
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "list": false,
                "list_add_label": "Add More",
                "name": "silent_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "use_multithreading": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "[Deprecated] Use Multithreading",
                "dynamic": false,
                "info": "Set 'Processing Concurrency' greater than 1 to enable multithreading.",
                "list": false,
                "list_add_label": "Add More",
                "name": "use_multithreading",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "File"
        },
        "dragging": false,
        "id": "File-BlymP",
        "measured": {
          "height": 230,
          "width": 320
        },
        "position": {
          "x": 810.2019450036391,
          "y": 2767.2875125716273
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioEmbeddingsComponent-YCNlt",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "category": "lmstudio",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate embeddings using LM Studio.",
            "display_name": "LM Studio Embeddings",
            "documentation": "",
            "edited": false,
            "field_order": [
              "model",
              "base_url",
              "api_key",
              "temperature"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "key": "LMStudioEmbeddingsComponent",
            "last_updated": "2025-10-29T16:16:15.701Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Model",
                "group_outputs": false,
                "method": "build_embeddings",
                "name": "embeddings",
                "options": null,
                "required_inputs": null,
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.000018578044550916993,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "LM Studio Base URL",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\n\n\nclass LMStudioEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"LM Studio Embeddings\"\n    description: str = \"Generate embeddings using LM Studio.\"\n    icon = \"LMStudio\"\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):  # noqa: ARG002\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            refresh_button=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"LM Studio Base URL\",\n            refresh_button=True,\n            value=\"http://localhost:1234/v1\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use LM Studio Embeddings.\"\n            raise ImportError(msg) from e\n        try:\n            output = NVIDIAEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n                nvidia_api_key=self.api_key,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to LM Studio API. Error: {e}\"\n            raise ValueError(msg) from e\n        return output\n"
              },
              "model": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "info": "",
                "name": "model",
                "options": [
                  "qwen/qwen3-4b-thinking-2507",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-nomic-embed-text-v1.5"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf"
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Model Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "LMStudioEmbeddingsComponent"
        },
        "dragging": false,
        "id": "LMStudioEmbeddingsComponent-YCNlt",
        "measured": {
          "height": 287,
          "width": 320
        },
        "position": {
          "x": 408.9442736979911,
          "y": 148.38238472136172
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Milvus-z3V2G",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Milvus vector store with search capabilities",
            "display_name": "Milvus",
            "documentation": "",
            "edited": true,
            "field_order": [
              "collection_name",
              "collection_description",
              "uri",
              "password",
              "connection_args",
              "primary_field",
              "text_field",
              "vector_field",
              "consistency_level",
              "index_params",
              "search_params",
              "drop_old",
              "timeout",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "embedding",
              "number_of_results"
            ],
            "frozen": false,
            "icon": "Milvus",
            "last_updated": "2025-09-30T04:30:49.033Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search_documents",
                "name": "search_results",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "hidden": null,
                "method": "as_dataframe",
                "name": "dataframe",
                "options": null,
                "required_inputs": null,
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\r\nfrom langflow.helpers.data import docs_to_data\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    DictInput,\r\n    DropdownInput,\r\n    FloatInput,\r\n    HandleInput,\r\n    IntInput,\r\n    SecretStrInput,\r\n    StrInput,\r\n)\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass MilvusVectorStoreComponent(LCVectorStoreComponent):\r\n    \"\"\"Milvus vector store with search capabilities.\"\"\"\r\n\r\n    display_name: str = \"Milvus\"\r\n    description: str = \"Milvus vector store with search capabilities\"\r\n    name = \"Milvus\"\r\n    icon = \"Milvus\"\r\n\r\n    inputs = [\r\n        StrInput(name=\"collection_name\", display_name=\"Collection Name\", value=\"langflow\"),\r\n        StrInput(name=\"collection_description\", display_name=\"Collection Description\", value=\"\"),\r\n        StrInput(\r\n            name=\"uri\",\r\n            display_name=\"Connection URI\",\r\n            value=\"http://localhost:19530\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"password\",\r\n            display_name=\"Milvus Token\",\r\n            value=\"\",\r\n            info=\"Ignore this field if no token is required to make connection.\",\r\n        ),\r\n        DictInput(name=\"connection_args\", display_name=\"Other Connection Arguments\", advanced=True),\r\n        StrInput(name=\"primary_field\", display_name=\"Primary Field Name\", value=\"pk\"),\r\n        StrInput(name=\"text_field\", display_name=\"Text Field Name\", value=\"text\"),\r\n        StrInput(name=\"vector_field\", display_name=\"Vector Field Name\", value=\"vector\"),\r\n        DropdownInput(\r\n            name=\"consistency_level\",\r\n            display_name=\"Consistencey Level\",\r\n            options=[\"Bounded\", \"Session\", \"Strong\", \"Eventual\"],\r\n            value=\"Session\",\r\n            advanced=True,\r\n        ),\r\n        DictInput(name=\"index_params\", display_name=\"Index Parameters\", advanced=True),\r\n        DictInput(name=\"search_params\", display_name=\"Search Parameters\", advanced=True),\r\n        BoolInput(name=\"drop_old\", display_name=\"Drop Old Collection\", value=False, advanced=True),\r\n        FloatInput(name=\"timeout\", display_name=\"Timeout\", advanced=True),\r\n        *LCVectorStoreComponent.inputs,\r\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\r\n        IntInput(\r\n            name=\"number_of_results\",\r\n            display_name=\"Number of Results\",\r\n            info=\"Number of results to return.\",\r\n            value=4,\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    @check_cached_vector_store\r\n    def build_vector_store(self):\r\n        print(\"\\n--- Milvus LOG 1: build_vector_store 함수 시작 ---\")\r\n        \r\n        try:\r\n            from langchain_milvus.vectorstores import Milvus as LangchainMilvus\r\n        except ImportError as e:\r\n            msg = \"Could not import Milvus integration package. Please install it with `pip install langchain-milvus`.\"\r\n            raise ImportError(msg) from e\r\n\r\n        self.connection_args.update(uri=self.uri, token=self.password)\r\n    \r\n        milvus_store = LangchainMilvus(\r\n            embedding_function=self.embedding,\r\n            collection_name=self.collection_name,\r\n            collection_description=self.collection_description,\r\n            connection_args=self.connection_args,\r\n            consistency_level=self.consistency_level,\r\n            index_params=self.index_params,\r\n            search_params=self.search_params,\r\n            drop_old=self.drop_old,\r\n            auto_id=True,\r\n            primary_field=self.primary_field,\r\n            text_field=self.text_field,\r\n            vector_field=self.vector_field,\r\n          \r\n        )\r\n\r\n        self.ingest_data = self._prepare_ingest_data()\r\n\r\n        documents = []\r\n        for _input in self.ingest_data or []:\r\n            if isinstance(_input, Data):\r\n                documents.append(_input.to_lc_document())\r\n            else:\r\n                documents.append(_input)\r\n\r\n        print(f\"--- Milvus LOG 2: 입력으로부터 {len(documents)}개의 문서를 받았습니다.\")\r\n        \r\n        print(\"--- Milvus LOG 3: 필터링 전 각 문서의 내용:\")\r\n        for i, doc in enumerate(documents):\r\n            content = getattr(doc, 'page_content', 'NO_PAGE_CONTENT_ATTRIBUTE_ERROR')\r\n            print(f\"  - 문서 #{i}: '{content}'\")\r\n\r\n        if documents:\r\n            filtered_documents = [\r\n                doc for doc in documents\r\n                if hasattr(doc, \"page_content\")\r\n                and doc.page_content\r\n                and doc.page_content.strip()\r\n            ]\r\n            \r\n            print(f\"--- Milvus LOG 4: 빈 문서 필터링 후 {len(filtered_documents)}개가 남았습니다.\")\r\n\r\n            if not filtered_documents:\r\n                print(\"--- Milvus LOG 5: 유효한 문서가 없어 저장을 중단합니다.\")\r\n            else:\r\n                try:\r\n                    texts_to_embed = [doc.page_content for doc in filtered_documents]\r\n                    print(f\"--- Milvus LOG 7.1: 임베딩 모델에 {len(texts_to_embed)}개의 텍스트를 전달합니다.\")\r\n\r\n                    print(\"--- Milvus LOG 7.2: 임베딩 모델을 호출하여 벡터 변환을 시작합니다...\")\r\n                    \r\n                    # milvus_store.embedding_function -> self.embedding 으로 수정됨\r\n                    embeddings = self.embedding.embed_documents(texts_to_embed)\r\n\r\n                    print(f\"--- Milvus LOG 7.3: 임베딩 모델이 {len(embeddings)}개의 벡터를 반환했습니다.\")\r\n\r\n                    if len(texts_to_embed) != len(embeddings):\r\n                        print(f\"--- Milvus LOG 7.4 FATAL: 텍스트 개수({len(texts_to_embed)})와 벡터 개수({len(embeddings)})가 일치하지 않아 저장할 수 없습니다!\")\r\n                        raise ValueError(f\"Mismatched lengths: {len(texts_to_embed)} texts vs {len(embeddings)} embeddings\")\r\n\r\n                    print(\"--- Milvus LOG 7.4: 텍스트와 벡터 개수가 일치합니다. Milvus에 저장을 시작합니다...\")\r\n                    milvus_store.add_embeddings(texts=texts_to_embed, embeddings=embeddings)\r\n                    print(\"--- Milvus LOG 7.5: 문서 추가에 성공했습니다.\")\r\n                    \r\n                except Exception as e:\r\n                    print(f\"--- Milvus LOG 8: 작업 중 에러 발생: {e}\")\r\n                    raise e\r\n\r\n        return milvus_store\r\n\r\n    def search_documents(self) -> list[Data]:\r\n        vector_store = self.build_vector_store()\r\n\r\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\r\n            docs = vector_store.similarity_search(\r\n                query=self.search_query,\r\n                k=self.number_of_results,\r\n            )\r\n\r\n            data = docs_to_data(docs)\r\n            self.status = data\r\n            return data\r\n        return []"
              },
              "collection_description": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Description",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "collection_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qqqq"
              },
              "connection_args": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Other Connection Arguments",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "connection_args",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "consistency_level": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Consistencey Level",
                "dynamic": false,
                "info": "",
                "name": "consistency_level",
                "options": [
                  "Bounded",
                  "Session",
                  "Strong",
                  "Eventual"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Session"
              },
              "drop_old": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Drop Old Collection",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "drop_old",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "index_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Index Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "index_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "password": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Milvus Token",
                "dynamic": false,
                "info": "Ignore this field if no token is required to make connection.",
                "input_types": [],
                "load_from_db": false,
                "name": "password",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "primary_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Primary Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "primary_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "pk"
              },
              "search_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Search Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "search_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Text Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text"
              },
              "timeout": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": ""
              },
              "uri": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Connection URI",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "uri",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:19530"
              },
              "vector_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Vector Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "vector_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "vector"
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "Milvus"
        },
        "dragging": false,
        "id": "Milvus-z3V2G",
        "measured": {
          "height": 875,
          "width": 320
        },
        "position": {
          "x": 941.5754079839578,
          "y": -84.53554640448621
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioEmbeddingsComponent-Ilrp9",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "category": "lmstudio",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate embeddings using LM Studio.",
            "display_name": "LM Studio Embeddings",
            "documentation": "",
            "edited": false,
            "field_order": [
              "model",
              "base_url",
              "api_key",
              "temperature"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "key": "LMStudioEmbeddingsComponent",
            "last_updated": "2025-10-29T16:16:15.702Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Model",
                "group_outputs": false,
                "method": "build_embeddings",
                "name": "embeddings",
                "options": null,
                "required_inputs": null,
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.000018578044550916993,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "LM Studio Base URL",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\n\n\nclass LMStudioEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"LM Studio Embeddings\"\n    description: str = \"Generate embeddings using LM Studio.\"\n    icon = \"LMStudio\"\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):  # noqa: ARG002\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            refresh_button=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"LM Studio Base URL\",\n            refresh_button=True,\n            value=\"http://localhost:1234/v1\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use LM Studio Embeddings.\"\n            raise ImportError(msg) from e\n        try:\n            output = NVIDIAEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n                nvidia_api_key=self.api_key,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to LM Studio API. Error: {e}\"\n            raise ValueError(msg) from e\n        return output\n"
              },
              "model": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "info": "",
                "name": "model",
                "options": [
                  "qwen/qwen3-4b-thinking-2507",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-nomic-embed-text-v1.5"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text-embedding-paraphrase-multilingual-minilm-l12-v2"
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Model Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "LMStudioEmbeddingsComponent"
        },
        "dragging": false,
        "id": "LMStudioEmbeddingsComponent-Ilrp9",
        "measured": {
          "height": 287,
          "width": 320
        },
        "position": {
          "x": 1094.1766821341953,
          "y": 3159.162997427643
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Milvus-3RBiF",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Milvus vector store with search capabilities",
            "display_name": "Milvus",
            "documentation": "",
            "edited": true,
            "field_order": [
              "collection_name",
              "collection_description",
              "uri",
              "password",
              "connection_args",
              "primary_field",
              "text_field",
              "vector_field",
              "consistency_level",
              "index_params",
              "search_params",
              "drop_old",
              "timeout",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "embedding",
              "number_of_results"
            ],
            "frozen": false,
            "icon": "Milvus",
            "legacy": false,
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search_documents",
                "name": "search_results",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "hidden": null,
                "method": "as_dataframe",
                "name": "dataframe",
                "options": null,
                "required_inputs": null,
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\r\nfrom langflow.helpers.data import docs_to_data\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    DictInput,\r\n    DropdownInput,\r\n    FloatInput,\r\n    HandleInput,\r\n    IntInput,\r\n    SecretStrInput,\r\n    StrInput,\r\n)\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass MilvusVectorStoreComponent(LCVectorStoreComponent):\r\n    \"\"\"Milvus vector store with search capabilities.\"\"\"\r\n\r\n    display_name: str = \"Milvus\"\r\n    description: str = \"Milvus vector store with search capabilities\"\r\n    name = \"Milvus\"\r\n    icon = \"Milvus\"\r\n\r\n    inputs = [\r\n        StrInput(name=\"collection_name\", display_name=\"Collection Name\", value=\"langflow\"),\r\n        StrInput(name=\"collection_description\", display_name=\"Collection Description\", value=\"\"),\r\n        StrInput(\r\n            name=\"uri\",\r\n            display_name=\"Connection URI\",\r\n            value=\"http://localhost:19530\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"password\",\r\n            display_name=\"Milvus Token\",\r\n            value=\"\",\r\n            info=\"Ignore this field if no token is required to make connection.\",\r\n        ),\r\n        DictInput(name=\"connection_args\", display_name=\"Other Connection Arguments\", advanced=True),\r\n        StrInput(name=\"primary_field\", display_name=\"Primary Field Name\", value=\"pk\"),\r\n        StrInput(name=\"text_field\", display_name=\"Text Field Name\", value=\"text\"),\r\n        StrInput(name=\"vector_field\", display_name=\"Vector Field Name\", value=\"vector\"),\r\n        DropdownInput(\r\n            name=\"consistency_level\",\r\n            display_name=\"Consistencey Level\",\r\n            options=[\"Bounded\", \"Session\", \"Strong\", \"Eventual\"],\r\n            value=\"Session\",\r\n            advanced=True,\r\n        ),\r\n        DictInput(name=\"index_params\", display_name=\"Index Parameters\", advanced=True),\r\n        DictInput(name=\"search_params\", display_name=\"Search Parameters\", advanced=True),\r\n        BoolInput(name=\"drop_old\", display_name=\"Drop Old Collection\", value=False, advanced=True),\r\n        FloatInput(name=\"timeout\", display_name=\"Timeout\", advanced=True),\r\n        *LCVectorStoreComponent.inputs,\r\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\r\n        IntInput(\r\n            name=\"number_of_results\",\r\n            display_name=\"Number of Results\",\r\n            info=\"Number of results to return.\",\r\n            value=4,\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    @check_cached_vector_store\r\n    def build_vector_store(self):\r\n        print(\"\\n--- Milvus LOG 1: build_vector_store 함수 시작 ---\")\r\n        \r\n        try:\r\n            from langchain_milvus.vectorstores import Milvus as LangchainMilvus\r\n        except ImportError as e:\r\n            msg = \"Could not import Milvus integration package. Please install it with `pip install langchain-milvus`.\"\r\n            raise ImportError(msg) from e\r\n\r\n        self.connection_args.update(uri=self.uri, token=self.password)\r\n    \r\n        milvus_store = LangchainMilvus(\r\n            embedding_function=self.embedding,\r\n            collection_name=self.collection_name,\r\n            collection_description=self.collection_description,\r\n            connection_args=self.connection_args,\r\n            consistency_level=self.consistency_level,\r\n            index_params=self.index_params,\r\n            search_params=self.search_params,\r\n            drop_old=self.drop_old,\r\n            auto_id=True,\r\n            primary_field=self.primary_field,\r\n            text_field=self.text_field,\r\n            vector_field=self.vector_field,\r\n          \r\n        )\r\n\r\n        self.ingest_data = self._prepare_ingest_data()\r\n\r\n        documents = []\r\n        for _input in self.ingest_data or []:\r\n            if isinstance(_input, Data):\r\n                documents.append(_input.to_lc_document())\r\n            else:\r\n                documents.append(_input)\r\n\r\n        print(f\"--- Milvus LOG 2: 입력으로부터 {len(documents)}개의 문서를 받았습니다.\")\r\n        \r\n        print(\"--- Milvus LOG 3: 필터링 전 각 문서의 내용:\")\r\n        for i, doc in enumerate(documents):\r\n            content = getattr(doc, 'page_content', 'NO_PAGE_CONTENT_ATTRIBUTE_ERROR')\r\n            print(f\"  - 문서 #{i}: '{content}'\")\r\n\r\n        if documents:\r\n            filtered_documents = [\r\n                doc for doc in documents\r\n                if hasattr(doc, \"page_content\")\r\n                and doc.page_content\r\n                and doc.page_content.strip()\r\n            ]\r\n            \r\n            print(f\"--- Milvus LOG 4: 빈 문서 필터링 후 {len(filtered_documents)}개가 남았습니다.\")\r\n\r\n            if not filtered_documents:\r\n                print(\"--- Milvus LOG 5: 유효한 문서가 없어 저장을 중단합니다.\")\r\n            else:\r\n                try:\r\n                    texts_to_embed = [doc.page_content for doc in filtered_documents]\r\n                    print(f\"--- Milvus LOG 7.1: 임베딩 모델에 {len(texts_to_embed)}개의 텍스트를 전달합니다.\")\r\n\r\n                    print(\"--- Milvus LOG 7.2: 임베딩 모델을 호출하여 벡터 변환을 시작합니다...\")\r\n                    \r\n                    # milvus_store.embedding_function -> self.embedding 으로 수정됨\r\n                    embeddings = self.embedding.embed_documents(texts_to_embed)\r\n\r\n                    print(f\"--- Milvus LOG 7.3: 임베딩 모델이 {len(embeddings)}개의 벡터를 반환했습니다.\")\r\n\r\n                    if len(texts_to_embed) != len(embeddings):\r\n                        print(f\"--- Milvus LOG 7.4 FATAL: 텍스트 개수({len(texts_to_embed)})와 벡터 개수({len(embeddings)})가 일치하지 않아 저장할 수 없습니다!\")\r\n                        raise ValueError(f\"Mismatched lengths: {len(texts_to_embed)} texts vs {len(embeddings)} embeddings\")\r\n\r\n                    print(\"--- Milvus LOG 7.4: 텍스트와 벡터 개수가 일치합니다. Milvus에 저장을 시작합니다...\")\r\n                    milvus_store.add_embeddings(texts=texts_to_embed, embeddings=embeddings)\r\n                    print(\"--- Milvus LOG 7.5: 문서 추가에 성공했습니다.\")\r\n                    \r\n                except Exception as e:\r\n                    print(f\"--- Milvus LOG 8: 작업 중 에러 발생: {e}\")\r\n                    raise e\r\n\r\n        return milvus_store\r\n\r\n    def search_documents(self) -> list[Data]:\r\n        vector_store = self.build_vector_store()\r\n\r\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\r\n            docs = vector_store.similarity_search(\r\n                query=self.search_query,\r\n                k=self.number_of_results,\r\n            )\r\n\r\n            data = docs_to_data(docs)\r\n            self.status = data\r\n            return data\r\n        return []"
              },
              "collection_description": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Description",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "collection_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "kfc"
              },
              "connection_args": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Other Connection Arguments",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "connection_args",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "consistency_level": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Consistencey Level",
                "dynamic": false,
                "info": "",
                "name": "consistency_level",
                "options": [
                  "Bounded",
                  "Session",
                  "Strong",
                  "Eventual"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Session"
              },
              "drop_old": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Drop Old Collection",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "drop_old",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "index_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Index Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "index_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 4
              },
              "password": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Milvus Token",
                "dynamic": false,
                "info": "Ignore this field if no token is required to make connection.",
                "input_types": [],
                "load_from_db": false,
                "name": "password",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "primary_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Primary Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "primary_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "pk"
              },
              "search_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Search Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "search_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Text Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text"
              },
              "timeout": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": ""
              },
              "uri": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Connection URI",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "uri",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:19530"
              },
              "vector_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Vector Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "vector_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "vector"
              }
            },
            "tool_mode": false
          },
          "selected_output": "dataframe",
          "showNode": true,
          "type": "Milvus"
        },
        "dragging": false,
        "id": "Milvus-3RBiF",
        "measured": {
          "height": 875,
          "width": 320
        },
        "position": {
          "x": 1828.1346299344857,
          "y": 3316.259414612203
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-WMPgY",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "n_messages",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "last_updated": "2025-10-28T17:31:54.441Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Language Model",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "LanguageModel"
                ],
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "Groq",
                  "OpenAI",
                  "Custom"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "Groq"
                  },
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "brain"
                  }
                ],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langchain_core.tools import StructuredTool\n\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.agents.events import ExceptionWithMessageError\nfrom langflow.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS,\n    MODEL_PROVIDERS_DICT,\n    MODELS_METADATA,\n)\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers.current_date import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import ToolCallingAgentComponent\nfrom langflow.custom.custom_component.component import _get_component_toolkit\nfrom langflow.custom.utils import update_component_build_config\nfrom langflow.field_typing import Tool\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MultilineInput, Output\nfrom langflow.logging import logger\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nMODEL_PROVIDERS_LIST = [\"Anthropic\", \"Google Generative AI\", \"Groq\", \"OpenAI\"]\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST, \"Custom\"],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST] + [{\"icon\": \"brain\"}],\n        ),\n        *MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"],\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        *LCToolsAgentComponent._base_inputs,\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [Output(name=\"response\", display_name=\"Response\", method=\"message_response\")]\n\n    async def message_response(self) -> Message:\n        try:\n            # Get LLM model and validate\n            llm_model, display_name = self.get_llm()\n            if llm_model is None:\n                msg = \"No language model selected. Please choose a model to proceed.\"\n                raise ValueError(msg)\n            self.model_name = get_model_name(llm_model, display_name=display_name)\n\n            # Get memory data\n            self.chat_history = await self.get_memory_data()\n            if isinstance(self.chat_history, Message):\n                self.chat_history = [self.chat_history]\n\n            # Add current date tool if enabled\n            if self.add_current_date_tool:\n                if not isinstance(self.tools, list):  # type: ignore[has-type]\n                    self.tools = []\n                current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n                if not isinstance(current_date_tool, StructuredTool):\n                    msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                    raise TypeError(msg)\n                self.tools.append(current_date_tool)\n            # note the tools are not required to run the agent, hence the validation removed.\n\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            return await self.run_agent(agent)\n\n        except (ValueError, TypeError, KeyError) as e:\n            logger.error(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            logger.error(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e!s}\")\n            raise\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(session_id=self.graph.session_id, order=\"Ascending\", n_messages=self.n_messages)\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except Exception as e:\n            logger.error(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            model_kwargs = {input_.name: getattr(self, f\"{prefix}{input_.name}\") for input_ in inputs}\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n            elif field_value == \"Custom\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    options=[*sorted(MODEL_PROVIDERS), \"Custom\"],\n                    value=\"Custom\",\n                    real_time_refresh=True,\n                    input_types=[\"LanguageModel\"],\n                    options_metadata=[MODELS_METADATA[key] for key in sorted(MODELS_METADATA.keys())]\n                    + [{\"icon\": \"brain\"}],\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name = field_name.replace(prefix, \"\")\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = _get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\", tool_description=description, callbacks=self.get_langchain_callbacks()\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n        return tools\n"
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 15
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "dragging": false,
        "id": "Agent-WMPgY",
        "measured": {
          "height": 431,
          "width": 320
        },
        "position": {
          "x": 2575.8263747942055,
          "y": 523.5033601823549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-9qtbB",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "last_updated": "2025-09-30T04:27:49.337Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "name": "model_name",
                "options": [
                  "qwen/qwen3-4b-thinking-2507",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-nomic-embed-text-v1.5"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qwen/qwen3-4b-thinking-2507"
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "model_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-9qtbB",
        "measured": {
          "height": 453,
          "width": 320
        },
        "position": {
          "x": 2344.6098025004294,
          "y": -42.06352898851785
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-PlODE",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean the data",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([safe_convert(item, clean_data=self.clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-PlODE",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": 4353.764332701606,
          "y": 2201.1089520531727
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-Lrn6a",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "n_messages",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "last_updated": "2025-10-28T17:31:54.442Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Language Model",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "LanguageModel"
                ],
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "Groq",
                  "OpenAI",
                  "Custom"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "Groq"
                  },
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "brain"
                  }
                ],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langchain_core.tools import StructuredTool\n\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.agents.events import ExceptionWithMessageError\nfrom langflow.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS,\n    MODEL_PROVIDERS_DICT,\n    MODELS_METADATA,\n)\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers.current_date import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import ToolCallingAgentComponent\nfrom langflow.custom.custom_component.component import _get_component_toolkit\nfrom langflow.custom.utils import update_component_build_config\nfrom langflow.field_typing import Tool\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MultilineInput, Output\nfrom langflow.logging import logger\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nMODEL_PROVIDERS_LIST = [\"Anthropic\", \"Google Generative AI\", \"Groq\", \"OpenAI\"]\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST, \"Custom\"],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST] + [{\"icon\": \"brain\"}],\n        ),\n        *MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"],\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        *LCToolsAgentComponent._base_inputs,\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [Output(name=\"response\", display_name=\"Response\", method=\"message_response\")]\n\n    async def message_response(self) -> Message:\n        try:\n            # Get LLM model and validate\n            llm_model, display_name = self.get_llm()\n            if llm_model is None:\n                msg = \"No language model selected. Please choose a model to proceed.\"\n                raise ValueError(msg)\n            self.model_name = get_model_name(llm_model, display_name=display_name)\n\n            # Get memory data\n            self.chat_history = await self.get_memory_data()\n            if isinstance(self.chat_history, Message):\n                self.chat_history = [self.chat_history]\n\n            # Add current date tool if enabled\n            if self.add_current_date_tool:\n                if not isinstance(self.tools, list):  # type: ignore[has-type]\n                    self.tools = []\n                current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n                if not isinstance(current_date_tool, StructuredTool):\n                    msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                    raise TypeError(msg)\n                self.tools.append(current_date_tool)\n            # note the tools are not required to run the agent, hence the validation removed.\n\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            return await self.run_agent(agent)\n\n        except (ValueError, TypeError, KeyError) as e:\n            logger.error(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            logger.error(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e!s}\")\n            raise\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(session_id=self.graph.session_id, order=\"Ascending\", n_messages=self.n_messages)\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except Exception as e:\n            logger.error(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            model_kwargs = {input_.name: getattr(self, f\"{prefix}{input_.name}\") for input_ in inputs}\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n            elif field_value == \"Custom\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    options=[*sorted(MODEL_PROVIDERS), \"Custom\"],\n                    value=\"Custom\",\n                    real_time_refresh=True,\n                    input_types=[\"LanguageModel\"],\n                    options_metadata=[MODELS_METADATA[key] for key in sorted(MODELS_METADATA.keys())]\n                    + [{\"icon\": \"brain\"}],\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name = field_name.replace(prefix, \"\")\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = _get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\", tool_description=description, callbacks=self.get_langchain_callbacks()\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n        return tools\n"
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 15
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "id": "Agent-Lrn6a",
        "measured": {
          "height": 431,
          "width": 320
        },
        "position": {
          "x": 3787.2770217592733,
          "y": 1930.4609833527406
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-EUPnO",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "last_updated": "2025-09-30T04:27:49.337Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "name": "model_name",
                "options": [
                  "qwen/qwen3-4b-thinking-2507",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-nomic-embed-text-v1.5"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qwen/qwen3-4b-thinking-2507"
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "model_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-EUPnO",
        "measured": {
          "height": 453,
          "width": 320
        },
        "position": {
          "x": 3099.014977985825,
          "y": 1313.0275005296894
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "MCPTools-HE57I",
          "node": {
            "base_classes": [
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Connect to an MCP server to use its tools.",
            "display_name": "MCP Tools",
            "documentation": "https://docs.langflow.org/mcp-client",
            "edited": false,
            "field_order": [
              "mcp_server",
              "tool",
              "tool_placeholder"
            ],
            "frozen": false,
            "icon": "Mcp",
            "last_updated": "2025-10-28T17:31:53.882Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Toolset",
                "group_outputs": false,
                "hidden": null,
                "method": "to_toolkit",
                "name": "component_as_tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from __future__ import annotations\n\nimport asyncio\nimport uuid\nfrom typing import Any\n\nfrom langchain_core.tools import StructuredTool  # noqa: TC002\n\nfrom langflow.api.v2.mcp import get_server\nfrom langflow.base.agents.utils import maybe_unflatten_dict, safe_cache_get, safe_cache_set\nfrom langflow.base.mcp.util import (\n    MCPSseClient,\n    MCPStdioClient,\n    create_input_schema_from_json_schema,\n    update_tools,\n)\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs.inputs import InputTypes  # noqa: TC001\nfrom langflow.io import DropdownInput, McpInput, MessageTextInput, Output\nfrom langflow.io.schema import flatten_schema, schema_to_langflow_inputs\nfrom langflow.logging import logger\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.services.auth.utils import create_user_longterm_token\n\n# Import get_server from the backend API\nfrom langflow.services.database.models.user.crud import get_user_by_id\nfrom langflow.services.deps import get_session, get_settings_service, get_storage_service\n\n\nclass MCPToolsComponent(ComponentWithCache):\n    schema_inputs: list = []\n    tools: list[StructuredTool] = []\n    _not_load_actions: bool = False\n    _tool_cache: dict = {}\n    _last_selected_server: str | None = None  # Cache for the last selected server\n\n    def __init__(self, **data) -> None:\n        super().__init__(**data)\n        # Initialize cache keys to avoid CacheMiss when accessing them\n        self._ensure_cache_structure()\n\n        # Initialize clients with access to the component cache\n        self.stdio_client: MCPStdioClient = MCPStdioClient(component_cache=self._shared_component_cache)\n        self.sse_client: MCPSseClient = MCPSseClient(component_cache=self._shared_component_cache)\n\n    def _ensure_cache_structure(self):\n        \"\"\"Ensure the cache has the required structure.\"\"\"\n        # Check if servers key exists and is not CacheMiss\n        servers_value = safe_cache_get(self._shared_component_cache, \"servers\")\n        if servers_value is None:\n            safe_cache_set(self._shared_component_cache, \"servers\", {})\n\n        # Check if last_selected_server key exists and is not CacheMiss\n        last_server_value = safe_cache_get(self._shared_component_cache, \"last_selected_server\")\n        if last_server_value is None:\n            safe_cache_set(self._shared_component_cache, \"last_selected_server\", \"\")\n\n    default_keys: list[str] = [\n        \"code\",\n        \"_type\",\n        \"tool_mode\",\n        \"tool_placeholder\",\n        \"mcp_server\",\n        \"tool\",\n    ]\n\n    display_name = \"MCP Tools\"\n    description = \"Connect to an MCP server to use its tools.\"\n    documentation: str = \"https://docs.langflow.org/mcp-client\"\n    icon = \"Mcp\"\n    name = \"MCPTools\"\n\n    inputs = [\n        McpInput(\n            name=\"mcp_server\",\n            display_name=\"MCP Server\",\n            info=\"Select the MCP Server that will be used by this component\",\n            real_time_refresh=True,\n        ),\n        DropdownInput(\n            name=\"tool\",\n            display_name=\"Tool\",\n            options=[],\n            value=\"\",\n            info=\"Select the tool to execute\",\n            show=False,\n            required=True,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            info=\"Placeholder for the tool\",\n            value=\"\",\n            show=False,\n            tool_mode=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Response\", name=\"response\", method=\"build_output\"),\n    ]\n\n    async def _validate_schema_inputs(self, tool_obj) -> list[InputTypes]:\n        \"\"\"Validate and process schema inputs for a tool.\"\"\"\n        try:\n            if not tool_obj or not hasattr(tool_obj, \"args_schema\"):\n                msg = \"Invalid tool object or missing input schema\"\n                raise ValueError(msg)\n\n            flat_schema = flatten_schema(tool_obj.args_schema.schema())\n            input_schema = create_input_schema_from_json_schema(flat_schema)\n            if not input_schema:\n                msg = f\"Empty input schema for tool '{tool_obj.name}'\"\n                raise ValueError(msg)\n\n            schema_inputs = schema_to_langflow_inputs(input_schema)\n            if not schema_inputs:\n                msg = f\"No input parameters defined for tool '{tool_obj.name}'\"\n                logger.warning(msg)\n                return []\n\n        except Exception as e:\n            msg = f\"Error validating schema inputs: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n        else:\n            return schema_inputs\n\n    async def update_tool_list(self, mcp_server_value=None):\n        # Accepts mcp_server_value as dict {name, config} or uses self.mcp_server\n        mcp_server = mcp_server_value if mcp_server_value is not None else getattr(self, \"mcp_server\", None)\n        server_name = None\n        server_config_from_value = None\n        if isinstance(mcp_server, dict):\n            server_name = mcp_server.get(\"name\")\n            server_config_from_value = mcp_server.get(\"config\")\n        else:\n            server_name = mcp_server\n        if not server_name:\n            self.tools = []\n            return [], {\"name\": server_name, \"config\": server_config_from_value}\n\n        # Use shared cache if available\n        servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n        cached = servers_cache.get(server_name) if isinstance(servers_cache, dict) else None\n\n        if cached is not None:\n            self.tools = cached[\"tools\"]\n            self.tool_names = cached[\"tool_names\"]\n            self._tool_cache = cached[\"tool_cache\"]\n            server_config_from_value = cached[\"config\"]\n            return self.tools, {\"name\": server_name, \"config\": server_config_from_value}\n\n        try:\n            async for db in get_session():\n                user_id, _ = await create_user_longterm_token(db)\n                current_user = await get_user_by_id(db, user_id)\n\n                # Try to get server config from DB/API\n                server_config = await get_server(\n                    server_name,\n                    current_user,\n                    db,\n                    storage_service=get_storage_service(),\n                    settings_service=get_settings_service(),\n                )\n\n                # If get_server returns empty but we have a config, use it\n                if not server_config and server_config_from_value:\n                    server_config = server_config_from_value\n\n                if not server_config:\n                    self.tools = []\n                    return [], {\"name\": server_name, \"config\": server_config}\n\n                _, tool_list, tool_cache = await update_tools(\n                    server_name=server_name,\n                    server_config=server_config,\n                    mcp_stdio_client=self.stdio_client,\n                    mcp_sse_client=self.sse_client,\n                )\n\n                self.tool_names = [tool.name for tool in tool_list if hasattr(tool, \"name\")]\n                self._tool_cache = tool_cache\n                self.tools = tool_list\n                # Cache the result using shared cache\n                cache_data = {\n                    \"tools\": tool_list,\n                    \"tool_names\": self.tool_names,\n                    \"tool_cache\": tool_cache,\n                    \"config\": server_config,\n                }\n\n                # Safely update the servers cache\n                current_servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n                if isinstance(current_servers_cache, dict):\n                    current_servers_cache[server_name] = cache_data\n                    safe_cache_set(self._shared_component_cache, \"servers\", current_servers_cache)\n\n                return tool_list, {\"name\": server_name, \"config\": server_config}\n        except (TimeoutError, asyncio.TimeoutError) as e:\n            msg = f\"Timeout updating tool list: {e!s}\"\n            logger.exception(msg)\n            raise TimeoutError(msg) from e\n        except Exception as e:\n            msg = f\"Error updating tool list: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n\n    async def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:\n        \"\"\"Toggle the visibility of connection-specific fields based on the selected mode.\"\"\"\n        try:\n            if field_name == \"tool\":\n                try:\n                    if len(self.tools) == 0:\n                        try:\n                            self.tools, build_config[\"mcp_server\"][\"value\"] = await self.update_tool_list()\n                            build_config[\"tool\"][\"options\"] = [tool.name for tool in self.tools]\n                            build_config[\"tool\"][\"placeholder\"] = \"Select a tool\"\n                        except (TimeoutError, asyncio.TimeoutError) as e:\n                            msg = f\"Timeout updating tool list: {e!s}\"\n                            logger.exception(msg)\n                            if not build_config[\"tools_metadata\"][\"show\"]:\n                                build_config[\"tool\"][\"show\"] = True\n                                build_config[\"tool\"][\"options\"] = []\n                                build_config[\"tool\"][\"value\"] = \"\"\n                                build_config[\"tool\"][\"placeholder\"] = \"Timeout on MCP server\"\n                            else:\n                                build_config[\"tool\"][\"show\"] = False\n                        except ValueError:\n                            if not build_config[\"tools_metadata\"][\"show\"]:\n                                build_config[\"tool\"][\"show\"] = True\n                                build_config[\"tool\"][\"options\"] = []\n                                build_config[\"tool\"][\"value\"] = \"\"\n                                build_config[\"tool\"][\"placeholder\"] = \"Error on MCP Server\"\n                            else:\n                                build_config[\"tool\"][\"show\"] = False\n\n                    if field_value == \"\":\n                        return build_config\n                    tool_obj = None\n                    for tool in self.tools:\n                        if tool.name == field_value:\n                            tool_obj = tool\n                            break\n                    if tool_obj is None:\n                        msg = f\"Tool {field_value} not found in available tools: {self.tools}\"\n                        logger.warning(msg)\n                        return build_config\n                    await self._update_tool_config(build_config, field_value)\n                except Exception as e:\n                    build_config[\"tool\"][\"options\"] = []\n                    msg = f\"Failed to update tools: {e!s}\"\n                    raise ValueError(msg) from e\n                else:\n                    return build_config\n            elif field_name == \"mcp_server\":\n                if not field_value:\n                    build_config[\"tool\"][\"show\"] = False\n                    build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"value\"] = \"\"\n                    build_config[\"tool\"][\"placeholder\"] = \"\"\n                    build_config[\"tool_placeholder\"][\"tool_mode\"] = False\n                    self.remove_non_default_keys(build_config)\n                    return build_config\n\n                build_config[\"tool_placeholder\"][\"tool_mode\"] = True\n\n                current_server_name = field_value.get(\"name\") if isinstance(field_value, dict) else field_value\n                _last_selected_server = safe_cache_get(self._shared_component_cache, \"last_selected_server\", \"\")\n\n                # To avoid unnecessary updates, only proceed if the server has actually changed\n                if (_last_selected_server in (current_server_name, \"\")) and build_config[\"tool\"][\"show\"]:\n                    return build_config\n\n                # Determine if \"Tool Mode\" is active by checking if the tool dropdown is hidden.\n                is_in_tool_mode = build_config[\"tools_metadata\"][\"show\"]\n                safe_cache_set(self._shared_component_cache, \"last_selected_server\", current_server_name)\n\n                # Check if tools are already cached for this server before clearing\n                cached_tools = None\n                if current_server_name:\n                    servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n                    if isinstance(servers_cache, dict):\n                        cached = servers_cache.get(current_server_name)\n                        if cached is not None:\n                            cached_tools = cached[\"tools\"]\n                            self.tools = cached_tools\n                            self.tool_names = cached[\"tool_names\"]\n                            self._tool_cache = cached[\"tool_cache\"]\n\n                # Only clear tools if we don't have cached tools for the current server\n                if not cached_tools:\n                    self.tools = []  # Clear previous tools only if no cache\n\n                self.remove_non_default_keys(build_config)  # Clear previous tool inputs\n\n                # Only show the tool dropdown if not in tool_mode\n                if not is_in_tool_mode:\n                    build_config[\"tool\"][\"show\"] = True\n                    if cached_tools:\n                        # Use cached tools to populate options immediately\n                        build_config[\"tool\"][\"options\"] = [tool.name for tool in cached_tools]\n                        build_config[\"tool\"][\"placeholder\"] = \"Select a tool\"\n                    else:\n                        # Show loading state only when we need to fetch tools\n                        build_config[\"tool\"][\"placeholder\"] = \"Loading tools...\"\n                        build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"value\"] = uuid.uuid4()\n                else:\n                    # Keep the tool dropdown hidden if in tool_mode\n                    self._not_load_actions = True\n                    build_config[\"tool\"][\"show\"] = False\n\n            elif field_name == \"tool_mode\":\n                build_config[\"tool\"][\"placeholder\"] = \"\"\n                build_config[\"tool\"][\"show\"] = not bool(field_value) and bool(build_config[\"mcp_server\"])\n                self.remove_non_default_keys(build_config)\n                self.tool = build_config[\"tool\"][\"value\"]\n                if field_value:\n                    self._not_load_actions = True\n                else:\n                    build_config[\"tool\"][\"value\"] = uuid.uuid4()\n                    build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"show\"] = True\n                    build_config[\"tool\"][\"placeholder\"] = \"Loading tools...\"\n            elif field_name == \"tools_metadata\":\n                self._not_load_actions = False\n\n        except Exception as e:\n            msg = f\"Error in update_build_config: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n        else:\n            return build_config\n\n    def get_inputs_for_all_tools(self, tools: list) -> dict:\n        \"\"\"Get input schemas for all tools.\"\"\"\n        inputs = {}\n        for tool in tools:\n            if not tool or not hasattr(tool, \"name\"):\n                continue\n            try:\n                flat_schema = flatten_schema(tool.args_schema.schema())\n                input_schema = create_input_schema_from_json_schema(flat_schema)\n                langflow_inputs = schema_to_langflow_inputs(input_schema)\n                inputs[tool.name] = langflow_inputs\n            except (AttributeError, ValueError, TypeError, KeyError) as e:\n                msg = f\"Error getting inputs for tool {getattr(tool, 'name', 'unknown')}: {e!s}\"\n                logger.exception(msg)\n                continue\n        return inputs\n\n    def remove_input_schema_from_build_config(\n        self, build_config: dict, tool_name: str, input_schema: dict[list[InputTypes], Any]\n    ):\n        \"\"\"Remove the input schema for the tool from the build config.\"\"\"\n        # Keep only schemas that don't belong to the current tool\n        input_schema = {k: v for k, v in input_schema.items() if k != tool_name}\n        # Remove all inputs from other tools\n        for value in input_schema.values():\n            for _input in value:\n                if _input.name in build_config:\n                    build_config.pop(_input.name)\n\n    def remove_non_default_keys(self, build_config: dict) -> None:\n        \"\"\"Remove non-default keys from the build config.\"\"\"\n        for key in list(build_config.keys()):\n            if key not in self.default_keys:\n                build_config.pop(key)\n\n    async def _update_tool_config(self, build_config: dict, tool_name: str) -> None:\n        \"\"\"Update tool configuration with proper error handling.\"\"\"\n        if not self.tools:\n            self.tools, build_config[\"mcp_server\"][\"value\"] = await self.update_tool_list()\n\n        if not tool_name:\n            return\n\n        tool_obj = next((tool for tool in self.tools if tool.name == tool_name), None)\n        if not tool_obj:\n            msg = f\"Tool {tool_name} not found in available tools: {self.tools}\"\n            self.remove_non_default_keys(build_config)\n            build_config[\"tool\"][\"value\"] = \"\"\n            logger.warning(msg)\n            return\n\n        try:\n            # Store current values before removing inputs\n            current_values = {}\n            for key, value in build_config.items():\n                if key not in self.default_keys and isinstance(value, dict) and \"value\" in value:\n                    current_values[key] = value[\"value\"]\n\n            # Get all tool inputs and remove old ones\n            input_schema_for_all_tools = self.get_inputs_for_all_tools(self.tools)\n            self.remove_input_schema_from_build_config(build_config, tool_name, input_schema_for_all_tools)\n\n            # Get and validate new inputs\n            self.schema_inputs = await self._validate_schema_inputs(tool_obj)\n            if not self.schema_inputs:\n                msg = f\"No input parameters to configure for tool '{tool_name}'\"\n                logger.info(msg)\n                return\n\n            # Add new inputs to build config\n            for schema_input in self.schema_inputs:\n                if not schema_input or not hasattr(schema_input, \"name\"):\n                    msg = \"Invalid schema input detected, skipping\"\n                    logger.warning(msg)\n                    continue\n\n                try:\n                    name = schema_input.name\n                    input_dict = schema_input.to_dict()\n                    input_dict.setdefault(\"value\", None)\n                    input_dict.setdefault(\"required\", True)\n\n                    build_config[name] = input_dict\n\n                    # Preserve existing value if the parameter name exists in current_values\n                    if name in current_values:\n                        build_config[name][\"value\"] = current_values[name]\n\n                except (AttributeError, KeyError, TypeError) as e:\n                    msg = f\"Error processing schema input {schema_input}: {e!s}\"\n                    logger.exception(msg)\n                    continue\n        except ValueError as e:\n            msg = f\"Schema validation error for tool {tool_name}: {e!s}\"\n            logger.exception(msg)\n            self.schema_inputs = []\n            return\n        except (AttributeError, KeyError, TypeError) as e:\n            msg = f\"Error updating tool config: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n\n    async def build_output(self) -> DataFrame:\n        \"\"\"Build output with improved error handling and validation.\"\"\"\n        try:\n            self.tools, _ = await self.update_tool_list()\n            if self.tool != \"\":\n                # Set session context for persistent MCP sessions using Langflow session ID\n                session_context = self._get_session_context()\n                if session_context:\n                    self.stdio_client.set_session_context(session_context)\n                    self.sse_client.set_session_context(session_context)\n\n                exec_tool = self._tool_cache[self.tool]\n                tool_args = self.get_inputs_for_all_tools(self.tools)[self.tool]\n                kwargs = {}\n                for arg in tool_args:\n                    value = getattr(self, arg.name, None)\n                    if value:\n                        if isinstance(value, Message):\n                            kwargs[arg.name] = value.text\n                        else:\n                            kwargs[arg.name] = value\n\n                unflattened_kwargs = maybe_unflatten_dict(kwargs)\n\n                output = await exec_tool.coroutine(**unflattened_kwargs)\n\n                tool_content = []\n                for item in output.content:\n                    item_dict = item.model_dump()\n                    tool_content.append(item_dict)\n                return DataFrame(data=tool_content)\n            return DataFrame(data=[{\"error\": \"You must select a tool\"}])\n        except Exception as e:\n            msg = f\"Error in build_output: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n\n    def _get_session_context(self) -> str | None:\n        \"\"\"Get the Langflow session ID for MCP session caching.\"\"\"\n        # Try to get session ID from the component's execution context\n        if hasattr(self, \"graph\") and hasattr(self.graph, \"session_id\"):\n            session_id = self.graph.session_id\n            # Include server name to ensure different servers get different sessions\n            server_name = \"\"\n            mcp_server = getattr(self, \"mcp_server\", None)\n            if isinstance(mcp_server, dict):\n                server_name = mcp_server.get(\"name\", \"\")\n            elif mcp_server:\n                server_name = str(mcp_server)\n            return f\"{session_id}_{server_name}\" if session_id else None\n        return None\n\n    async def _get_tools(self):\n        \"\"\"Get cached tools or update if necessary.\"\"\"\n        mcp_server = getattr(self, \"mcp_server\", None)\n        if not self._not_load_actions:\n            tools, _ = await self.update_tool_list(mcp_server)\n            return tools\n        return []\n"
              },
              "mcp_server": {
                "_input_type": "McpInput",
                "advanced": false,
                "display_name": "MCP Server",
                "dynamic": false,
                "info": "Select the MCP Server that will be used by this component",
                "name": "mcp_server",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "mcp",
                "value": {
                  "config": {},
                  "name": "m123123"
                }
              },
              "tool": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Tool",
                "dynamic": false,
                "info": "Select the tool to execute",
                "name": "tool",
                "options": [],
                "options_metadata": [],
                "placeholder": "",
                "real_time_refresh": true,
                "required": true,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "Placeholder for the tool",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools_metadata": {
                "_input_type": "ToolsInput",
                "advanced": false,
                "display_name": "Actions",
                "dynamic": false,
                "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "tools_metadata",
                "placeholder": "Error loading actions",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tools",
                "value": []
              }
            },
            "tool_mode": true
          },
          "showNode": true,
          "type": "MCPTools"
        },
        "dragging": false,
        "id": "MCPTools-HE57I",
        "measured": {
          "height": 289,
          "width": 320
        },
        "position": {
          "x": 3193.8862944745015,
          "y": 1847.519386625851
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "MCPTools-n7itv",
          "node": {
            "base_classes": [
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Connect to an MCP server to use its tools.",
            "display_name": "MCP Tools",
            "documentation": "https://docs.langflow.org/mcp-client",
            "edited": false,
            "field_order": [
              "mcp_server",
              "tool",
              "tool_placeholder"
            ],
            "frozen": false,
            "icon": "Mcp",
            "last_updated": "2025-10-28T17:31:53.883Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Toolset",
                "group_outputs": false,
                "hidden": null,
                "method": "to_toolkit",
                "name": "component_as_tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from __future__ import annotations\n\nimport asyncio\nimport uuid\nfrom typing import Any\n\nfrom langchain_core.tools import StructuredTool  # noqa: TC002\n\nfrom langflow.api.v2.mcp import get_server\nfrom langflow.base.agents.utils import maybe_unflatten_dict, safe_cache_get, safe_cache_set\nfrom langflow.base.mcp.util import (\n    MCPSseClient,\n    MCPStdioClient,\n    create_input_schema_from_json_schema,\n    update_tools,\n)\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs.inputs import InputTypes  # noqa: TC001\nfrom langflow.io import DropdownInput, McpInput, MessageTextInput, Output\nfrom langflow.io.schema import flatten_schema, schema_to_langflow_inputs\nfrom langflow.logging import logger\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.services.auth.utils import create_user_longterm_token\n\n# Import get_server from the backend API\nfrom langflow.services.database.models.user.crud import get_user_by_id\nfrom langflow.services.deps import get_session, get_settings_service, get_storage_service\n\n\nclass MCPToolsComponent(ComponentWithCache):\n    schema_inputs: list = []\n    tools: list[StructuredTool] = []\n    _not_load_actions: bool = False\n    _tool_cache: dict = {}\n    _last_selected_server: str | None = None  # Cache for the last selected server\n\n    def __init__(self, **data) -> None:\n        super().__init__(**data)\n        # Initialize cache keys to avoid CacheMiss when accessing them\n        self._ensure_cache_structure()\n\n        # Initialize clients with access to the component cache\n        self.stdio_client: MCPStdioClient = MCPStdioClient(component_cache=self._shared_component_cache)\n        self.sse_client: MCPSseClient = MCPSseClient(component_cache=self._shared_component_cache)\n\n    def _ensure_cache_structure(self):\n        \"\"\"Ensure the cache has the required structure.\"\"\"\n        # Check if servers key exists and is not CacheMiss\n        servers_value = safe_cache_get(self._shared_component_cache, \"servers\")\n        if servers_value is None:\n            safe_cache_set(self._shared_component_cache, \"servers\", {})\n\n        # Check if last_selected_server key exists and is not CacheMiss\n        last_server_value = safe_cache_get(self._shared_component_cache, \"last_selected_server\")\n        if last_server_value is None:\n            safe_cache_set(self._shared_component_cache, \"last_selected_server\", \"\")\n\n    default_keys: list[str] = [\n        \"code\",\n        \"_type\",\n        \"tool_mode\",\n        \"tool_placeholder\",\n        \"mcp_server\",\n        \"tool\",\n    ]\n\n    display_name = \"MCP Tools\"\n    description = \"Connect to an MCP server to use its tools.\"\n    documentation: str = \"https://docs.langflow.org/mcp-client\"\n    icon = \"Mcp\"\n    name = \"MCPTools\"\n\n    inputs = [\n        McpInput(\n            name=\"mcp_server\",\n            display_name=\"MCP Server\",\n            info=\"Select the MCP Server that will be used by this component\",\n            real_time_refresh=True,\n        ),\n        DropdownInput(\n            name=\"tool\",\n            display_name=\"Tool\",\n            options=[],\n            value=\"\",\n            info=\"Select the tool to execute\",\n            show=False,\n            required=True,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            info=\"Placeholder for the tool\",\n            value=\"\",\n            show=False,\n            tool_mode=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Response\", name=\"response\", method=\"build_output\"),\n    ]\n\n    async def _validate_schema_inputs(self, tool_obj) -> list[InputTypes]:\n        \"\"\"Validate and process schema inputs for a tool.\"\"\"\n        try:\n            if not tool_obj or not hasattr(tool_obj, \"args_schema\"):\n                msg = \"Invalid tool object or missing input schema\"\n                raise ValueError(msg)\n\n            flat_schema = flatten_schema(tool_obj.args_schema.schema())\n            input_schema = create_input_schema_from_json_schema(flat_schema)\n            if not input_schema:\n                msg = f\"Empty input schema for tool '{tool_obj.name}'\"\n                raise ValueError(msg)\n\n            schema_inputs = schema_to_langflow_inputs(input_schema)\n            if not schema_inputs:\n                msg = f\"No input parameters defined for tool '{tool_obj.name}'\"\n                logger.warning(msg)\n                return []\n\n        except Exception as e:\n            msg = f\"Error validating schema inputs: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n        else:\n            return schema_inputs\n\n    async def update_tool_list(self, mcp_server_value=None):\n        # Accepts mcp_server_value as dict {name, config} or uses self.mcp_server\n        mcp_server = mcp_server_value if mcp_server_value is not None else getattr(self, \"mcp_server\", None)\n        server_name = None\n        server_config_from_value = None\n        if isinstance(mcp_server, dict):\n            server_name = mcp_server.get(\"name\")\n            server_config_from_value = mcp_server.get(\"config\")\n        else:\n            server_name = mcp_server\n        if not server_name:\n            self.tools = []\n            return [], {\"name\": server_name, \"config\": server_config_from_value}\n\n        # Use shared cache if available\n        servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n        cached = servers_cache.get(server_name) if isinstance(servers_cache, dict) else None\n\n        if cached is not None:\n            self.tools = cached[\"tools\"]\n            self.tool_names = cached[\"tool_names\"]\n            self._tool_cache = cached[\"tool_cache\"]\n            server_config_from_value = cached[\"config\"]\n            return self.tools, {\"name\": server_name, \"config\": server_config_from_value}\n\n        try:\n            async for db in get_session():\n                user_id, _ = await create_user_longterm_token(db)\n                current_user = await get_user_by_id(db, user_id)\n\n                # Try to get server config from DB/API\n                server_config = await get_server(\n                    server_name,\n                    current_user,\n                    db,\n                    storage_service=get_storage_service(),\n                    settings_service=get_settings_service(),\n                )\n\n                # If get_server returns empty but we have a config, use it\n                if not server_config and server_config_from_value:\n                    server_config = server_config_from_value\n\n                if not server_config:\n                    self.tools = []\n                    return [], {\"name\": server_name, \"config\": server_config}\n\n                _, tool_list, tool_cache = await update_tools(\n                    server_name=server_name,\n                    server_config=server_config,\n                    mcp_stdio_client=self.stdio_client,\n                    mcp_sse_client=self.sse_client,\n                )\n\n                self.tool_names = [tool.name for tool in tool_list if hasattr(tool, \"name\")]\n                self._tool_cache = tool_cache\n                self.tools = tool_list\n                # Cache the result using shared cache\n                cache_data = {\n                    \"tools\": tool_list,\n                    \"tool_names\": self.tool_names,\n                    \"tool_cache\": tool_cache,\n                    \"config\": server_config,\n                }\n\n                # Safely update the servers cache\n                current_servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n                if isinstance(current_servers_cache, dict):\n                    current_servers_cache[server_name] = cache_data\n                    safe_cache_set(self._shared_component_cache, \"servers\", current_servers_cache)\n\n                return tool_list, {\"name\": server_name, \"config\": server_config}\n        except (TimeoutError, asyncio.TimeoutError) as e:\n            msg = f\"Timeout updating tool list: {e!s}\"\n            logger.exception(msg)\n            raise TimeoutError(msg) from e\n        except Exception as e:\n            msg = f\"Error updating tool list: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n\n    async def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:\n        \"\"\"Toggle the visibility of connection-specific fields based on the selected mode.\"\"\"\n        try:\n            if field_name == \"tool\":\n                try:\n                    if len(self.tools) == 0:\n                        try:\n                            self.tools, build_config[\"mcp_server\"][\"value\"] = await self.update_tool_list()\n                            build_config[\"tool\"][\"options\"] = [tool.name for tool in self.tools]\n                            build_config[\"tool\"][\"placeholder\"] = \"Select a tool\"\n                        except (TimeoutError, asyncio.TimeoutError) as e:\n                            msg = f\"Timeout updating tool list: {e!s}\"\n                            logger.exception(msg)\n                            if not build_config[\"tools_metadata\"][\"show\"]:\n                                build_config[\"tool\"][\"show\"] = True\n                                build_config[\"tool\"][\"options\"] = []\n                                build_config[\"tool\"][\"value\"] = \"\"\n                                build_config[\"tool\"][\"placeholder\"] = \"Timeout on MCP server\"\n                            else:\n                                build_config[\"tool\"][\"show\"] = False\n                        except ValueError:\n                            if not build_config[\"tools_metadata\"][\"show\"]:\n                                build_config[\"tool\"][\"show\"] = True\n                                build_config[\"tool\"][\"options\"] = []\n                                build_config[\"tool\"][\"value\"] = \"\"\n                                build_config[\"tool\"][\"placeholder\"] = \"Error on MCP Server\"\n                            else:\n                                build_config[\"tool\"][\"show\"] = False\n\n                    if field_value == \"\":\n                        return build_config\n                    tool_obj = None\n                    for tool in self.tools:\n                        if tool.name == field_value:\n                            tool_obj = tool\n                            break\n                    if tool_obj is None:\n                        msg = f\"Tool {field_value} not found in available tools: {self.tools}\"\n                        logger.warning(msg)\n                        return build_config\n                    await self._update_tool_config(build_config, field_value)\n                except Exception as e:\n                    build_config[\"tool\"][\"options\"] = []\n                    msg = f\"Failed to update tools: {e!s}\"\n                    raise ValueError(msg) from e\n                else:\n                    return build_config\n            elif field_name == \"mcp_server\":\n                if not field_value:\n                    build_config[\"tool\"][\"show\"] = False\n                    build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"value\"] = \"\"\n                    build_config[\"tool\"][\"placeholder\"] = \"\"\n                    build_config[\"tool_placeholder\"][\"tool_mode\"] = False\n                    self.remove_non_default_keys(build_config)\n                    return build_config\n\n                build_config[\"tool_placeholder\"][\"tool_mode\"] = True\n\n                current_server_name = field_value.get(\"name\") if isinstance(field_value, dict) else field_value\n                _last_selected_server = safe_cache_get(self._shared_component_cache, \"last_selected_server\", \"\")\n\n                # To avoid unnecessary updates, only proceed if the server has actually changed\n                if (_last_selected_server in (current_server_name, \"\")) and build_config[\"tool\"][\"show\"]:\n                    return build_config\n\n                # Determine if \"Tool Mode\" is active by checking if the tool dropdown is hidden.\n                is_in_tool_mode = build_config[\"tools_metadata\"][\"show\"]\n                safe_cache_set(self._shared_component_cache, \"last_selected_server\", current_server_name)\n\n                # Check if tools are already cached for this server before clearing\n                cached_tools = None\n                if current_server_name:\n                    servers_cache = safe_cache_get(self._shared_component_cache, \"servers\", {})\n                    if isinstance(servers_cache, dict):\n                        cached = servers_cache.get(current_server_name)\n                        if cached is not None:\n                            cached_tools = cached[\"tools\"]\n                            self.tools = cached_tools\n                            self.tool_names = cached[\"tool_names\"]\n                            self._tool_cache = cached[\"tool_cache\"]\n\n                # Only clear tools if we don't have cached tools for the current server\n                if not cached_tools:\n                    self.tools = []  # Clear previous tools only if no cache\n\n                self.remove_non_default_keys(build_config)  # Clear previous tool inputs\n\n                # Only show the tool dropdown if not in tool_mode\n                if not is_in_tool_mode:\n                    build_config[\"tool\"][\"show\"] = True\n                    if cached_tools:\n                        # Use cached tools to populate options immediately\n                        build_config[\"tool\"][\"options\"] = [tool.name for tool in cached_tools]\n                        build_config[\"tool\"][\"placeholder\"] = \"Select a tool\"\n                    else:\n                        # Show loading state only when we need to fetch tools\n                        build_config[\"tool\"][\"placeholder\"] = \"Loading tools...\"\n                        build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"value\"] = uuid.uuid4()\n                else:\n                    # Keep the tool dropdown hidden if in tool_mode\n                    self._not_load_actions = True\n                    build_config[\"tool\"][\"show\"] = False\n\n            elif field_name == \"tool_mode\":\n                build_config[\"tool\"][\"placeholder\"] = \"\"\n                build_config[\"tool\"][\"show\"] = not bool(field_value) and bool(build_config[\"mcp_server\"])\n                self.remove_non_default_keys(build_config)\n                self.tool = build_config[\"tool\"][\"value\"]\n                if field_value:\n                    self._not_load_actions = True\n                else:\n                    build_config[\"tool\"][\"value\"] = uuid.uuid4()\n                    build_config[\"tool\"][\"options\"] = []\n                    build_config[\"tool\"][\"show\"] = True\n                    build_config[\"tool\"][\"placeholder\"] = \"Loading tools...\"\n            elif field_name == \"tools_metadata\":\n                self._not_load_actions = False\n\n        except Exception as e:\n            msg = f\"Error in update_build_config: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n        else:\n            return build_config\n\n    def get_inputs_for_all_tools(self, tools: list) -> dict:\n        \"\"\"Get input schemas for all tools.\"\"\"\n        inputs = {}\n        for tool in tools:\n            if not tool or not hasattr(tool, \"name\"):\n                continue\n            try:\n                flat_schema = flatten_schema(tool.args_schema.schema())\n                input_schema = create_input_schema_from_json_schema(flat_schema)\n                langflow_inputs = schema_to_langflow_inputs(input_schema)\n                inputs[tool.name] = langflow_inputs\n            except (AttributeError, ValueError, TypeError, KeyError) as e:\n                msg = f\"Error getting inputs for tool {getattr(tool, 'name', 'unknown')}: {e!s}\"\n                logger.exception(msg)\n                continue\n        return inputs\n\n    def remove_input_schema_from_build_config(\n        self, build_config: dict, tool_name: str, input_schema: dict[list[InputTypes], Any]\n    ):\n        \"\"\"Remove the input schema for the tool from the build config.\"\"\"\n        # Keep only schemas that don't belong to the current tool\n        input_schema = {k: v for k, v in input_schema.items() if k != tool_name}\n        # Remove all inputs from other tools\n        for value in input_schema.values():\n            for _input in value:\n                if _input.name in build_config:\n                    build_config.pop(_input.name)\n\n    def remove_non_default_keys(self, build_config: dict) -> None:\n        \"\"\"Remove non-default keys from the build config.\"\"\"\n        for key in list(build_config.keys()):\n            if key not in self.default_keys:\n                build_config.pop(key)\n\n    async def _update_tool_config(self, build_config: dict, tool_name: str) -> None:\n        \"\"\"Update tool configuration with proper error handling.\"\"\"\n        if not self.tools:\n            self.tools, build_config[\"mcp_server\"][\"value\"] = await self.update_tool_list()\n\n        if not tool_name:\n            return\n\n        tool_obj = next((tool for tool in self.tools if tool.name == tool_name), None)\n        if not tool_obj:\n            msg = f\"Tool {tool_name} not found in available tools: {self.tools}\"\n            self.remove_non_default_keys(build_config)\n            build_config[\"tool\"][\"value\"] = \"\"\n            logger.warning(msg)\n            return\n\n        try:\n            # Store current values before removing inputs\n            current_values = {}\n            for key, value in build_config.items():\n                if key not in self.default_keys and isinstance(value, dict) and \"value\" in value:\n                    current_values[key] = value[\"value\"]\n\n            # Get all tool inputs and remove old ones\n            input_schema_for_all_tools = self.get_inputs_for_all_tools(self.tools)\n            self.remove_input_schema_from_build_config(build_config, tool_name, input_schema_for_all_tools)\n\n            # Get and validate new inputs\n            self.schema_inputs = await self._validate_schema_inputs(tool_obj)\n            if not self.schema_inputs:\n                msg = f\"No input parameters to configure for tool '{tool_name}'\"\n                logger.info(msg)\n                return\n\n            # Add new inputs to build config\n            for schema_input in self.schema_inputs:\n                if not schema_input or not hasattr(schema_input, \"name\"):\n                    msg = \"Invalid schema input detected, skipping\"\n                    logger.warning(msg)\n                    continue\n\n                try:\n                    name = schema_input.name\n                    input_dict = schema_input.to_dict()\n                    input_dict.setdefault(\"value\", None)\n                    input_dict.setdefault(\"required\", True)\n\n                    build_config[name] = input_dict\n\n                    # Preserve existing value if the parameter name exists in current_values\n                    if name in current_values:\n                        build_config[name][\"value\"] = current_values[name]\n\n                except (AttributeError, KeyError, TypeError) as e:\n                    msg = f\"Error processing schema input {schema_input}: {e!s}\"\n                    logger.exception(msg)\n                    continue\n        except ValueError as e:\n            msg = f\"Schema validation error for tool {tool_name}: {e!s}\"\n            logger.exception(msg)\n            self.schema_inputs = []\n            return\n        except (AttributeError, KeyError, TypeError) as e:\n            msg = f\"Error updating tool config: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n\n    async def build_output(self) -> DataFrame:\n        \"\"\"Build output with improved error handling and validation.\"\"\"\n        try:\n            self.tools, _ = await self.update_tool_list()\n            if self.tool != \"\":\n                # Set session context for persistent MCP sessions using Langflow session ID\n                session_context = self._get_session_context()\n                if session_context:\n                    self.stdio_client.set_session_context(session_context)\n                    self.sse_client.set_session_context(session_context)\n\n                exec_tool = self._tool_cache[self.tool]\n                tool_args = self.get_inputs_for_all_tools(self.tools)[self.tool]\n                kwargs = {}\n                for arg in tool_args:\n                    value = getattr(self, arg.name, None)\n                    if value:\n                        if isinstance(value, Message):\n                            kwargs[arg.name] = value.text\n                        else:\n                            kwargs[arg.name] = value\n\n                unflattened_kwargs = maybe_unflatten_dict(kwargs)\n\n                output = await exec_tool.coroutine(**unflattened_kwargs)\n\n                tool_content = []\n                for item in output.content:\n                    item_dict = item.model_dump()\n                    tool_content.append(item_dict)\n                return DataFrame(data=tool_content)\n            return DataFrame(data=[{\"error\": \"You must select a tool\"}])\n        except Exception as e:\n            msg = f\"Error in build_output: {e!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n\n    def _get_session_context(self) -> str | None:\n        \"\"\"Get the Langflow session ID for MCP session caching.\"\"\"\n        # Try to get session ID from the component's execution context\n        if hasattr(self, \"graph\") and hasattr(self.graph, \"session_id\"):\n            session_id = self.graph.session_id\n            # Include server name to ensure different servers get different sessions\n            server_name = \"\"\n            mcp_server = getattr(self, \"mcp_server\", None)\n            if isinstance(mcp_server, dict):\n                server_name = mcp_server.get(\"name\", \"\")\n            elif mcp_server:\n                server_name = str(mcp_server)\n            return f\"{session_id}_{server_name}\" if session_id else None\n        return None\n\n    async def _get_tools(self):\n        \"\"\"Get cached tools or update if necessary.\"\"\"\n        mcp_server = getattr(self, \"mcp_server\", None)\n        if not self._not_load_actions:\n            tools, _ = await self.update_tool_list(mcp_server)\n            return tools\n        return []\n"
              },
              "mcp_server": {
                "_input_type": "McpInput",
                "advanced": false,
                "display_name": "MCP Server",
                "dynamic": false,
                "info": "Select the MCP Server that will be used by this component",
                "name": "mcp_server",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "mcp",
                "value": {
                  "config": {},
                  "name": "m123123"
                }
              },
              "tool": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Tool",
                "dynamic": false,
                "info": "Select the tool to execute",
                "name": "tool",
                "options": [],
                "options_metadata": [],
                "placeholder": "",
                "real_time_refresh": true,
                "required": true,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "Placeholder for the tool",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools_metadata": {
                "_input_type": "ToolsInput",
                "advanced": false,
                "display_name": "Actions",
                "dynamic": false,
                "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "tools_metadata",
                "placeholder": "Error loading actions",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tools",
                "value": []
              }
            },
            "tool_mode": true
          },
          "showNode": true,
          "type": "MCPTools"
        },
        "dragging": false,
        "id": "MCPTools-n7itv",
        "measured": {
          "height": 289,
          "width": 320
        },
        "position": {
          "x": 2008.3256948235407,
          "y": 442.91936039512063
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioEmbeddingsComponent-OcmK9",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "category": "lmstudio",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate embeddings using LM Studio.",
            "display_name": "LM Studio Embeddings",
            "documentation": "",
            "edited": false,
            "field_order": [
              "model",
              "base_url",
              "api_key",
              "temperature"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "key": "LMStudioEmbeddingsComponent",
            "last_updated": "2025-10-28T17:31:53.885Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Model",
                "group_outputs": false,
                "method": "build_embeddings",
                "name": "embeddings",
                "options": null,
                "required_inputs": null,
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.000018578044550916993,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "LM Studio Base URL",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\n\n\nclass LMStudioEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"LM Studio Embeddings\"\n    description: str = \"Generate embeddings using LM Studio.\"\n    icon = \"LMStudio\"\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):  # noqa: ARG002\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            refresh_button=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"LM Studio Base URL\",\n            refresh_button=True,\n            value=\"http://localhost:1234/v1\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use LM Studio Embeddings.\"\n            raise ImportError(msg) from e\n        try:\n            output = NVIDIAEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n                nvidia_api_key=self.api_key,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to LM Studio API. Error: {e}\"\n            raise ValueError(msg) from e\n        return output\n"
              },
              "model": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "info": "",
                "name": "model",
                "options": [
                  "text-embedding-bge-reranker-v2-m3",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "qwen/qwen3-4b-thinking-2507",
                  "qwen/qwen3-4b-2507",
                  "text-embedding-sentence-transformers_all-minilm-l12-v2",
                  "text-embedding-nomic-embed-text-v1.5",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": true,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf"
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Model Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "LMStudioEmbeddingsComponent"
        },
        "dragging": false,
        "id": "LMStudioEmbeddingsComponent-OcmK9",
        "measured": {
          "height": 287,
          "width": 320
        },
        "position": {
          "x": -2834.685832721252,
          "y": 1194.9015193465118
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Milvus-mYB10",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Milvus vector store with search capabilities",
            "display_name": "Milvus",
            "documentation": "",
            "edited": true,
            "field_order": [
              "collection_name",
              "collection_description",
              "uri",
              "password",
              "connection_args",
              "primary_field",
              "text_field",
              "vector_field",
              "consistency_level",
              "index_params",
              "search_params",
              "drop_old",
              "timeout",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "embedding",
              "number_of_results"
            ],
            "frozen": false,
            "icon": "Milvus",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search_documents",
                "name": "search_results",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "hidden": null,
                "method": "as_dataframe",
                "name": "dataframe",
                "options": null,
                "required_inputs": null,
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\r\nfrom langflow.helpers.data import docs_to_data\r\nfrom langflow.io import (\r\n    BoolInput,\r\n    DictInput,\r\n    DropdownInput,\r\n    FloatInput,\r\n    HandleInput,\r\n    IntInput,\r\n    SecretStrInput,\r\n    StrInput,\r\n)\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass MilvusVectorStoreComponent(LCVectorStoreComponent):\r\n    \"\"\"Milvus vector store with search capabilities.\"\"\"\r\n\r\n    display_name: str = \"Milvus\"\r\n    description: str = \"Milvus vector store with search capabilities\"\r\n    name = \"Milvus\"\r\n    icon = \"Milvus\"\r\n\r\n    inputs = [\r\n        StrInput(name=\"collection_name\", display_name=\"Collection Name\", value=\"langflow\"),\r\n        StrInput(name=\"collection_description\", display_name=\"Collection Description\", value=\"\"),\r\n        StrInput(\r\n            name=\"uri\",\r\n            display_name=\"Connection URI\",\r\n            value=\"http://localhost:19530\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"password\",\r\n            display_name=\"Milvus Token\",\r\n            value=\"\",\r\n            info=\"Ignore this field if no token is required to make connection.\",\r\n        ),\r\n        DictInput(name=\"connection_args\", display_name=\"Other Connection Arguments\", advanced=True),\r\n        StrInput(name=\"primary_field\", display_name=\"Primary Field Name\", value=\"pk\"),\r\n        StrInput(name=\"text_field\", display_name=\"Text Field Name\", value=\"text\"),\r\n        StrInput(name=\"vector_field\", display_name=\"Vector Field Name\", value=\"vector\"),\r\n        DropdownInput(\r\n            name=\"consistency_level\",\r\n            display_name=\"Consistencey Level\",\r\n            options=[\"Bounded\", \"Session\", \"Strong\", \"Eventual\"],\r\n            value=\"Session\",\r\n            advanced=True,\r\n        ),\r\n        DictInput(name=\"index_params\", display_name=\"Index Parameters\", advanced=True),\r\n        DictInput(name=\"search_params\", display_name=\"Search Parameters\", advanced=True),\r\n        BoolInput(name=\"drop_old\", display_name=\"Drop Old Collection\", value=False, advanced=True),\r\n        FloatInput(name=\"timeout\", display_name=\"Timeout\", advanced=True),\r\n        *LCVectorStoreComponent.inputs,\r\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\r\n        IntInput(\r\n            name=\"number_of_results\",\r\n            display_name=\"Number of Results\",\r\n            info=\"Number of results to return.\",\r\n            value=4,\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    @check_cached_vector_store\r\n    def build_vector_store(self):\r\n        print(\"\\n--- Milvus LOG 1: build_vector_store 함수 시작 ---\")\r\n        \r\n        try:\r\n            from langchain_milvus.vectorstores import Milvus as LangchainMilvus\r\n        except ImportError as e:\r\n            msg = \"Could not import Milvus integration package. Please install it with `pip install langchain-milvus`.\"\r\n            raise ImportError(msg) from e\r\n\r\n        self.connection_args.update(uri=self.uri, token=self.password)\r\n    \r\n        milvus_store = LangchainMilvus(\r\n            embedding_function=self.embedding,\r\n            collection_name=self.collection_name,\r\n            collection_description=self.collection_description,\r\n            connection_args=self.connection_args,\r\n            consistency_level=self.consistency_level,\r\n            index_params=self.index_params,\r\n            search_params=self.search_params,\r\n            drop_old=self.drop_old,\r\n            auto_id=True,\r\n            primary_field=self.primary_field,\r\n            text_field=self.text_field,\r\n            vector_field=self.vector_field,\r\n          \r\n        )\r\n\r\n        self.ingest_data = self._prepare_ingest_data()\r\n\r\n        documents = []\r\n        for _input in self.ingest_data or []:\r\n            if isinstance(_input, Data):\r\n                documents.append(_input.to_lc_document())\r\n            else:\r\n                documents.append(_input)\r\n\r\n        print(f\"--- Milvus LOG 2: 입력으로부터 {len(documents)}개의 문서를 받았습니다.\")\r\n        \r\n        print(\"--- Milvus LOG 3: 필터링 전 각 문서의 내용:\")\r\n        for i, doc in enumerate(documents):\r\n            content = getattr(doc, 'page_content', 'NO_PAGE_CONTENT_ATTRIBUTE_ERROR')\r\n            print(f\"  - 문서 #{i}: '{content}'\")\r\n\r\n        if documents:\r\n            filtered_documents = [\r\n                doc for doc in documents\r\n                if hasattr(doc, \"page_content\")\r\n                and doc.page_content\r\n                and doc.page_content.strip()\r\n            ]\r\n            \r\n            print(f\"--- Milvus LOG 4: 빈 문서 필터링 후 {len(filtered_documents)}개가 남았습니다.\")\r\n\r\n            if not filtered_documents:\r\n                print(\"--- Milvus LOG 5: 유효한 문서가 없어 저장을 중단합니다.\")\r\n            else:\r\n                try:\r\n                    texts_to_embed = [doc.page_content for doc in filtered_documents]\r\n                    print(f\"--- Milvus LOG 7.1: 임베딩 모델에 {len(texts_to_embed)}개의 텍스트를 전달합니다.\")\r\n\r\n                    print(\"--- Milvus LOG 7.2: 임베딩 모델을 호출하여 벡터 변환을 시작합니다...\")\r\n                    \r\n                    # milvus_store.embedding_function -> self.embedding 으로 수정됨\r\n                    embeddings = self.embedding.embed_documents(texts_to_embed)\r\n\r\n                    print(f\"--- Milvus LOG 7.3: 임베딩 모델이 {len(embeddings)}개의 벡터를 반환했습니다.\")\r\n\r\n                    if len(texts_to_embed) != len(embeddings):\r\n                        print(f\"--- Milvus LOG 7.4 FATAL: 텍스트 개수({len(texts_to_embed)})와 벡터 개수({len(embeddings)})가 일치하지 않아 저장할 수 없습니다!\")\r\n                        raise ValueError(f\"Mismatched lengths: {len(texts_to_embed)} texts vs {len(embeddings)} embeddings\")\r\n\r\n                    print(\"--- Milvus LOG 7.4: 텍스트와 벡터 개수가 일치합니다. Milvus에 저장을 시작합니다...\")\r\n                    milvus_store.add_embeddings(texts=texts_to_embed, embeddings=embeddings)\r\n                    print(\"--- Milvus LOG 7.5: 문서 추가에 성공했습니다.\")\r\n                    \r\n                except Exception as e:\r\n                    print(f\"--- Milvus LOG 8: 작업 중 에러 발생: {e}\")\r\n                    raise e\r\n\r\n        return milvus_store\r\n\r\n    def search_documents(self) -> list[Data]:\r\n        vector_store = self.build_vector_store()\r\n\r\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\r\n            docs_with_scores = vector_store.similarity_search_with_score(\r\n                query=self.search_query,\r\n                k=self.number_of_results,\r\n            )\r\n\r\n            # 문서만 추출하여 Data로 변환\r\n            docs = [doc for doc, score in docs_with_scores]\r\n            data = docs_to_data(docs)\r\n            \r\n            # 각 데이터에 유사도 점수 추가\r\n            for i, (doc, score) in enumerate(docs_with_scores):\r\n                if i < len(data):\r\n                    data[i].data[\"similarity_score\"] = float(score)\r\n            \r\n            self.status = data\r\n            return data\r\n        return []"
              },
              "collection_description": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Description",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "collection_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "test_system"
              },
              "connection_args": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Other Connection Arguments",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "connection_args",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "consistency_level": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Consistencey Level",
                "dynamic": false,
                "info": "",
                "name": "consistency_level",
                "options": [
                  "Bounded",
                  "Session",
                  "Strong",
                  "Eventual"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Session"
              },
              "drop_old": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Drop Old Collection",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "drop_old",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "index_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Index Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "index_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1000
              },
              "password": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Milvus Token",
                "dynamic": false,
                "info": "Ignore this field if no token is required to make connection.",
                "input_types": [],
                "load_from_db": false,
                "name": "password",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "primary_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Primary Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "primary_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "pk"
              },
              "search_params": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Search Parameters",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "search_params",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Text Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "text"
              },
              "timeout": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": ""
              },
              "uri": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Connection URI",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "uri",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:19530"
              },
              "vector_field": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Vector Field Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "vector_field",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "vector"
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "Milvus"
        },
        "dragging": false,
        "id": "Milvus-mYB10",
        "measured": {
          "height": 958,
          "width": 320
        },
        "position": {
          "x": -2293.0151974885453,
          "y": 924.1842314024204
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "note-J7n8t",
          "node": {
            "description": "rerank test",
            "display_name": "",
            "documentation": "",
            "template": {}
          },
          "type": "note"
        },
        "dragging": false,
        "id": "note-J7n8t",
        "measured": {
          "height": 324,
          "width": 339
        },
        "position": {
          "x": -2206.8707240757294,
          "y": 349.34725525057723
        },
        "selected": false,
        "type": "noteNode"
      },
      {
        "data": {
          "id": "note-qE6lp",
          "node": {
            "description": "입력부터 출력까지\n",
            "display_name": "",
            "documentation": "",
            "template": {}
          },
          "type": "note"
        },
        "dragging": false,
        "id": "note-qE6lp",
        "measured": {
          "height": 324,
          "width": 339
        },
        "position": {
          "x": 1689.890307958656,
          "y": -310.5602967391667
        },
        "selected": false,
        "type": "noteNode"
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-EbS2H",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean the data",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([safe_convert(item, clean_data=self.clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-EbS2H",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": -663.9622024785381,
          "y": 1861.9180896098756
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-LaYcW",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Rerank documents using LM Studio embeddings (api/v0).",
            "display_name": "Reranker",
            "documentation": "",
            "edited": true,
            "field_order": [
              "docs",
              "query",
              "top_k"
            ],
            "frozen": false,
            "icon": "sort",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output",
                "group_outputs": false,
                "hidden": null,
                "method": "rerank",
                "name": "output",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import requests\r\nimport numpy as np\r\nimport json\r\nimport logging\r\nimport re\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import MessageTextInput, Output, IntInput\r\nfrom langflow.schema.data import Data\r\n\r\n# 로깅 설정\r\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s [%(levelname)s] %(message)s\")\r\n\r\n\r\nclass Reranker(Component):\r\n    display_name = \"Reranker\"\r\n    description = \"Rerank documents using LM Studio embeddings (api/v0).\"\r\n    icon = \"sort\"\r\n    name = \"Reranker\"\r\n\r\n    # ===== 입력 =====\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"docs\",\r\n            display_name=\"Documents\",\r\n            info=\"List of docs (string list or list of JSON objects with 'text').\",\r\n            value=\"[]\",\r\n            tool_mode=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"query\",\r\n            display_name=\"Query\",\r\n            info=\"Search query text\",\r\n            value=\"칼바람\",\r\n            tool_mode=True,\r\n        ),\r\n        IntInput(\r\n            name=\"top_k\",\r\n            display_name=\"Number of Results\",\r\n            info=\"상위 몇 개 문서를 반환할지 지정합니다.\",\r\n            value=50,   # 기본값은 50개\r\n        ),\r\n    ]\r\n\r\n    # ===== 출력 =====\r\n    outputs = [\r\n        Output(display_name=\"Output\", name=\"output\", method=\"rerank\"),\r\n    ]\r\n\r\n    # ===== Embedding 호출 =====\r\n    def get_embedding(self, text: str, model: str = \"text-embedding-bge-reranker-v2-m3\"):\r\n        url = \"http://127.0.0.1:1234/api/v0/embeddings\"\r\n        payload = {\"model\": model, \"input\": [text]}\r\n        response = requests.post(url, json=payload)\r\n        response.raise_for_status()\r\n        result = response.json()\r\n        return np.array(result[\"data\"][0][\"embedding\"], dtype=np.float32)\r\n\r\n    def cosine_similarity(self, a, b):\r\n        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\r\n\r\n    # ===== Rerank 로직 =====\r\n    def rerank(self) -> Data:\r\n        docs = self.docs\r\n        query_text = self.query if hasattr(self, \"query\") else \"query\"\r\n        top_k = int(self.top_k) if hasattr(self, \"top_k\") else 50\r\n\r\n        logging.debug(\"Raw docs input: %s\", str(docs)[:500])\r\n\r\n        # 문자열로 들어왔을 때 여러 JSON 블록(````json ... ````) 분리\r\n        if isinstance(docs, str):\r\n            parts = re.split(r\"```json|```\", docs)\r\n            docs = []\r\n            for part in parts:\r\n                part = part.strip()\r\n                if not part:\r\n                    continue\r\n                try:\r\n                    parsed = json.loads(part)\r\n                    docs.append(parsed)\r\n                except Exception:\r\n                    logging.debug(\"Skip non-JSON part: %s\", part[:100])\r\n        elif isinstance(docs, list):\r\n            pass\r\n        else:\r\n            docs = [docs]\r\n\r\n        logging.debug(\"Parsed %d documents\", len(docs))\r\n\r\n        # 쿼리 임베딩\r\n        query_emb = self.get_embedding(query_text)\r\n\r\n        reranked_docs = []\r\n        for i, doc in enumerate(docs):\r\n            if isinstance(doc, dict):\r\n                text = doc.get(\"text\", \"\")\r\n            else:\r\n                text = str(doc)\r\n\r\n            if not text.strip():\r\n                continue\r\n\r\n            logging.debug(\"Processing doc #%d text preview: %s\", i, text[:200])\r\n\r\n            doc_emb = self.get_embedding(text)\r\n            score = self.cosine_similarity(query_emb, doc_emb)\r\n\r\n            reranked_docs.append({\r\n                \"text\": text,\r\n                \"score\": score\r\n            })\r\n\r\n        # 스코어 높은 순으로 정렬\r\n        reranked_docs.sort(key=lambda x: x[\"score\"], reverse=True)\r\n\r\n        # top_k 만큼만 선택\r\n        top_n = reranked_docs[:top_k]\r\n\r\n        logging.debug(\"Final top_n docs count=%d\", len(top_n))\r\n\r\n        # 최종 출력은 text만\r\n        only_texts = [{\"text\": doc[\"text\"]} for doc in top_n]\r\n\r\n        return Data(value=only_texts)\r\n"
              },
              "docs": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Documents",
                "dynamic": false,
                "info": "List of docs (string list or list of JSON objects with 'text').",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "docs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "query": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Query",
                "dynamic": false,
                "info": "Search query text",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "query",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "top_k": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "상위 몇 개 문서를 반환할지 지정합니다.",
                "list": false,
                "list_add_label": "Add More",
                "name": "top_k",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 30
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Reranker"
        },
        "dragging": false,
        "id": "CustomComponent-LaYcW",
        "measured": {
          "height": 386,
          "width": 320
        },
        "position": {
          "x": -1190.0790556056359,
          "y": 1635.0807808941504
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-oIuy5",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-oIuy5",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": 1715.6148531197528,
          "y": 227.32753198672188
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-6kM40",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "내 질문을 2가지로 나눠주라\n\n1번 내 질문을 깔끔하게 정리해줘(구조는 바꾸지마)\n\n2번 키워드를 선정해줘, 키워드는 영어, 한국어 두개로 출력해줘\n그리고 답은\n질문 : \n키워드 : \n이런식으로 해줘"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-6kM40",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": -4109.964046261952,
          "y": 2009.75481703883
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-YxJU5",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "context",
                "question"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "hidden": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "context": {
                "advanced": false,
                "display_name": "context",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "context",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nQuestion: {question}\n\nAnswer: "
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "id": "Prompt-YxJU5",
        "measured": {
          "height": 450,
          "width": 320
        },
        "position": {
          "x": -1459.2582916568447,
          "y": 2746.2967535156035
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-hOiPh",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean the data",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([safe_convert(item, clean_data=self.clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-hOiPh",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": -142.90365276798158,
          "y": 3082.972031242704
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-yAZ5W",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "n_messages",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "last_updated": "2025-10-10T03:58:43.650Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Language Model",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "LanguageModel"
                ],
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "Groq",
                  "OpenAI",
                  "Custom"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "Groq"
                  },
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "brain"
                  }
                ],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langchain_core.tools import StructuredTool\n\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.agents.events import ExceptionWithMessageError\nfrom langflow.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS,\n    MODEL_PROVIDERS_DICT,\n    MODELS_METADATA,\n)\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers.current_date import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import ToolCallingAgentComponent\nfrom langflow.custom.custom_component.component import _get_component_toolkit\nfrom langflow.custom.utils import update_component_build_config\nfrom langflow.field_typing import Tool\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MultilineInput, Output\nfrom langflow.logging import logger\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nMODEL_PROVIDERS_LIST = [\"Anthropic\", \"Google Generative AI\", \"Groq\", \"OpenAI\"]\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST, \"Custom\"],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST] + [{\"icon\": \"brain\"}],\n        ),\n        *MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"],\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        *LCToolsAgentComponent._base_inputs,\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [Output(name=\"response\", display_name=\"Response\", method=\"message_response\")]\n\n    async def message_response(self) -> Message:\n        try:\n            # Get LLM model and validate\n            llm_model, display_name = self.get_llm()\n            if llm_model is None:\n                msg = \"No language model selected. Please choose a model to proceed.\"\n                raise ValueError(msg)\n            self.model_name = get_model_name(llm_model, display_name=display_name)\n\n            # Get memory data\n            self.chat_history = await self.get_memory_data()\n            if isinstance(self.chat_history, Message):\n                self.chat_history = [self.chat_history]\n\n            # Add current date tool if enabled\n            if self.add_current_date_tool:\n                if not isinstance(self.tools, list):  # type: ignore[has-type]\n                    self.tools = []\n                current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n                if not isinstance(current_date_tool, StructuredTool):\n                    msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                    raise TypeError(msg)\n                self.tools.append(current_date_tool)\n            # note the tools are not required to run the agent, hence the validation removed.\n\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            return await self.run_agent(agent)\n\n        except (ValueError, TypeError, KeyError) as e:\n            logger.error(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            logger.error(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e!s}\")\n            raise\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(session_id=self.graph.session_id, order=\"Ascending\", n_messages=self.n_messages)\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except Exception as e:\n            logger.error(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            model_kwargs = {input_.name: getattr(self, f\"{prefix}{input_.name}\") for input_ in inputs}\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n            elif field_value == \"Custom\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    options=[*sorted(MODEL_PROVIDERS), \"Custom\"],\n                    value=\"Custom\",\n                    real_time_refresh=True,\n                    input_types=[\"LanguageModel\"],\n                    options_metadata=[MODELS_METADATA[key] for key in sorted(MODELS_METADATA.keys())]\n                    + [{\"icon\": \"brain\"}],\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name = field_name.replace(prefix, \"\")\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = _get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\", tool_description=description, callbacks=self.get_langchain_callbacks()\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n        return tools\n"
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 15
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "dragging": false,
        "id": "Agent-yAZ5W",
        "measured": {
          "height": 431,
          "width": 320
        },
        "position": {
          "x": 381.26873003596313,
          "y": 2697.3822322050482
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-MjZb4",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "last_updated": "2025-10-10T07:14:01.769Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "name": "model_name",
                "options": [
                  "text-embedding-bge-reranker-v2-m3",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "qwen/qwen3-4b-thinking-2507",
                  "qwen/qwen3-4b-2507",
                  "text-embedding-sentence-transformers_all-minilm-l12-v2",
                  "text-embedding-nomic-embed-text-v1.5",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qwen/qwen3-4b-thinking-2507"
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-MjZb4",
        "measured": {
          "height": 453,
          "width": 320
        },
        "position": {
          "x": -285.5030629515558,
          "y": 2384.532011930078
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-1Ap2N",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are Agent Forensics, a digital forensic investigator.\n\nAll the information provided here constitutes evidence.\n\nBase your answers and reasoning strictly on the evidence.\n\nDo not lie.\n\nIf you do not know the answer, say “I don’t know.”"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-1Ap2N",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": -753.8374900940581,
          "y": 2804.54396399582
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-r484n",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "last_updated": "2025-10-05T19:05:46.155Z",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "name": "model_name",
                "options": [
                  "qwen/qwen3-4b-thinking-2507",
                  "qwen/qwen3-4b-2507",
                  "text-embedding-bge-reranker-v2-m3",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2.gguf",
                  "text-embedding-nomic-embed-text-v1.5",
                  "text-embedding-trotr-paraphrase-multilingual-minilm-l12-v2",
                  "text-embedding-paraphrase-multilingual-minilm-l12-v2"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qwen/qwen3-4b-thinking-2507"
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-r484n",
        "measured": {
          "height": 453,
          "width": 320
        },
        "position": {
          "x": -3406.373265094604,
          "y": 1790.4600593188347
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "PythonREPLComponent-gCXyv",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Parse HyDE LLM output into question and keywords.",
            "display_name": "HyDE Output Parser",
            "documentation": "https://docs.langflow.org/components-processing#hyde-parser",
            "edited": true,
            "field_order": [
              "hyde_output"
            ],
            "frozen": false,
            "icon": "square-terminal",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Question",
                "group_outputs": false,
                "hidden": null,
                "method": "question_response",
                "name": "question",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Keywords",
                "group_outputs": false,
                "hidden": null,
                "method": "keywords_response",
                "name": "keywords",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import re\r\nimport json\r\nfrom langflow.base.io.text import TextComponent\r\nfrom langflow.io import MessageInput, Output\r\nfrom langflow.schema.message import Message\r\n\r\nclass HyDEParser(TextComponent):\r\n    display_name = \"HyDE Output Parser\"\r\n    description = \"Parse HyDE LLM output into question and keywords.\"\r\n    documentation: str = \"https://docs.langflow.org/components-processing#hyde-parser\"\r\n    icon = \"square-terminal\"\r\n\r\n    inputs = [\r\n        MessageInput(\r\n            name=\"hyde_output\",\r\n            display_name=\"HyDE Output\",\r\n            info=\"Raw text or AIMessage output from the HyDE LLM\",\r\n            input_types=[\"Message\"],\r\n            tool_mode=True,\r\n            required=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Question\", name=\"question\", method=\"question_response\"),\r\n        Output(display_name=\"Keywords\", name=\"keywords\", method=\"keywords_response\"),\r\n    ]\r\n\r\n    def extract_text(self, data):\r\n        \"\"\"\r\n        HyDE 출력이 AIMessage, Message, dict, 또는 str 중 어떤 형식이든 텍스트만 추출.\r\n        \"\"\"\r\n        # 1️ Langflow Message 객체 (가장 우선)\r\n        if hasattr(data, 'text'):\r\n            return str(data.text)\r\n        if hasattr(data, 'content'):\r\n            return str(data.content)\r\n        \r\n        # 2️ 문자열인 경우\r\n        if isinstance(data, str):\r\n            return data\r\n        \r\n        # 3️ dict형 (Langflow Data 객체일 수 있음)\r\n        if isinstance(data, dict):\r\n            # data.data.text 형태\r\n            if \"data\" in data and isinstance(data[\"data\"], dict) and \"text\" in data[\"data\"]:\r\n                return str(data[\"data\"][\"text\"])\r\n            # data 자체가 text 키를 가질 경우\r\n            if \"text\" in data:\r\n                return str(data[\"text\"])\r\n            # content 필드 있을 수도 있음\r\n            if \"content\" in data:\r\n                return str(data[\"content\"])\r\n\r\n        # 4️ Langchain 메시지 객체형 (AIMessage 등) - str 변환 후 파싱\r\n        text = str(data)\r\n        if \"content='\" in text or 'content=\"' in text:\r\n            # AIMessage(content='...') 또는 content=\"...\" 구조 추출\r\n            match = re.search(r\"content=['\\\"](.+?)['\\\"]\", text, re.DOTALL)\r\n            if match:\r\n                return match.group(1)\r\n\r\n        # 5️ 그냥 문자열로 변환\r\n        return text\r\n\r\n    def parse_text(self, text: str):\r\n        question = None\r\n        keywords = []\r\n\r\n        # 텍스트가 한 줄로 되어 있을 수도 있으므로 공백으로 분리\r\n        parts = text.strip().split()\r\n        \r\n        # \"질문\" 찾기\r\n        if \"질문\" in parts:\r\n            q_idx = parts.index(\"질문\")\r\n            # \"질문 :\" 다음부터 \"키워드\" 전까지가 질문 내용\r\n            if q_idx + 2 < len(parts):\r\n                question_parts = []\r\n                for i in range(q_idx + 2, len(parts)):\r\n                    if parts[i] == \"키워드\":\r\n                        break\r\n                    question_parts.append(parts[i])\r\n                question = \" \".join(question_parts).replace(\"'\", \"\").replace('\"', \"\")\r\n        \r\n        # \"키워드\" 찾기\r\n        if \"키워드\" in parts:\r\n            k_idx = parts.index(\"키워드\")\r\n            if k_idx + 1 < len(parts):\r\n                keyword_parts = parts[k_idx + 1:]\r\n                keywords = [k.replace(\":\", \"\").strip() for k in keyword_parts if k.strip()]\r\n        \r\n        keyword_text = \" \".join(keywords)\r\n        return question, keyword_text\r\n\r\n    def get_parsed_data(self):\r\n        \"\"\"공통 파싱 로직\"\"\"\r\n        try:\r\n            # HyDE 출력에서 텍스트만 추출\r\n            raw_data = self.hyde_output\r\n            \r\n            # 디버깅: LLM이 생성한 전체 내용을 로그로 남기기\r\n            self.log(f\"[DEBUG 1] 받은 데이터 타입: {type(raw_data)}\")\r\n            self.log(f\"[DEBUG 2] 받은 데이터 전체 내용: {raw_data}\")\r\n            self.log(f\"[DEBUG 3] raw_data.text 직접 출력: {raw_data.text}\")\r\n            self.log(f\"[DEBUG 4] raw_data.text 길이: {len(str(raw_data.text))}\")\r\n            self.log(f\"[DEBUG 5] raw_data.text 전체 내용: '{str(raw_data.text)}'\")\r\n            \r\n            text = self.extract_text(raw_data)\r\n            self.log(f\"[DEBUG 6] 추출된 텍스트: '{text}'\")\r\n            \r\n            question, keyword_text = self.parse_text(text)\r\n            self.log(f\"[DEBUG 7] 파싱된 질문: '{question}'\")\r\n            self.log(f\"[DEBUG 8] 파싱된 키워드: '{keyword_text}'\")\r\n\r\n            return question or \"\", keyword_text or \"\"\r\n\r\n        except Exception as e:\r\n            error_message = f\"Error during parsing: {str(e)}\"\r\n            self.log(error_message)\r\n            return \"\", \"\"\r\n\r\n    def question_response(self) -> Message:\r\n        question, _ = self.get_parsed_data()\r\n        return Message(text=question)\r\n\r\n    def keywords_response(self) -> Message:\r\n        _, keywords = self.get_parsed_data()\r\n        return Message(text=keywords)"
              },
              "hyde_output": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "HyDE Output",
                "dynamic": false,
                "info": "Raw text or AIMessage output from the HyDE LLM",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "hyde_output",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "keywords",
          "showNode": true,
          "type": "HyDEParser"
        },
        "dragging": false,
        "id": "PythonREPLComponent-gCXyv",
        "measured": {
          "height": 220,
          "width": 320
        },
        "position": {
          "x": -2831.751281805325,
          "y": 2099.623358375858
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-kJwhc",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/components-io#chat-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color",
              "clean_data"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "hidden": null,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean the data",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([safe_convert(item, clean_data=self.clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-kJwhc",
        "measured": {
          "height": 166,
          "width": 320
        },
        "position": {
          "x": -1776.4588707853948,
          "y": 1663.9942378147223
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "HyDEParser-Sgux2",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Parse HyDE LLM output into question and keywords.",
            "display_name": "HyDE Output Parser",
            "documentation": "https://docs.langflow.org/components-processing#hyde-parser",
            "edited": true,
            "field_order": [
              "hyde_output"
            ],
            "frozen": false,
            "icon": "square-terminal",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Question",
                "group_outputs": false,
                "hidden": null,
                "method": "question_response",
                "name": "question",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Keywords",
                "group_outputs": false,
                "hidden": null,
                "method": "keywords_response",
                "name": "keywords",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import re\r\nimport json\r\nfrom langflow.base.io.text import TextComponent\r\nfrom langflow.io import MessageInput, Output\r\nfrom langflow.schema.message import Message\r\n\r\nclass HyDEParser(TextComponent):\r\n    display_name = \"HyDE Output Parser\"\r\n    description = \"Parse HyDE LLM output into question and keywords.\"\r\n    documentation: str = \"https://docs.langflow.org/components-processing#hyde-parser\"\r\n    icon = \"square-terminal\"\r\n\r\n    inputs = [\r\n        MessageInput(\r\n            name=\"hyde_output\",\r\n            display_name=\"HyDE Output\",\r\n            info=\"Raw text or AIMessage output from the HyDE LLM\",\r\n            input_types=[\"Message\"],\r\n            tool_mode=True,\r\n            required=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Question\", name=\"question\", method=\"question_response\"),\r\n        Output(display_name=\"Keywords\", name=\"keywords\", method=\"keywords_response\"),\r\n    ]\r\n\r\n    def extract_text(self, data):\r\n        \"\"\"\r\n        HyDE 출력이 AIMessage, Message, dict, 또는 str 중 어떤 형식이든 텍스트만 추출.\r\n        \"\"\"\r\n        # 1️ Langflow Message 객체 (가장 우선)\r\n        if hasattr(data, 'text'):\r\n            return str(data.text)\r\n        if hasattr(data, 'content'):\r\n            return str(data.content)\r\n        \r\n        # 2️ 문자열인 경우\r\n        if isinstance(data, str):\r\n            return data\r\n        \r\n        # 3️ dict형 (Langflow Data 객체일 수 있음)\r\n        if isinstance(data, dict):\r\n            # data.data.text 형태\r\n            if \"data\" in data and isinstance(data[\"data\"], dict) and \"text\" in data[\"data\"]:\r\n                return str(data[\"data\"][\"text\"])\r\n            # data 자체가 text 키를 가질 경우\r\n            if \"text\" in data:\r\n                return str(data[\"text\"])\r\n            # content 필드 있을 수도 있음\r\n            if \"content\" in data:\r\n                return str(data[\"content\"])\r\n\r\n        # 4️ Langchain 메시지 객체형 (AIMessage 등) - str 변환 후 파싱\r\n        text = str(data)\r\n        if \"content='\" in text or 'content=\"' in text:\r\n            # AIMessage(content='...') 또는 content=\"...\" 구조 추출\r\n            match = re.search(r\"content=['\\\"](.+?)['\\\"]\", text, re.DOTALL)\r\n            if match:\r\n                return match.group(1)\r\n\r\n        # 5️ 그냥 문자열로 변환\r\n        return text\r\n\r\n    def parse_text(self, text: str):\r\n        question = None\r\n        keywords = []\r\n\r\n        # 텍스트가 한 줄로 되어 있을 수도 있으므로 공백으로 분리\r\n        parts = text.strip().split()\r\n        \r\n        # \"질문\" 찾기\r\n        if \"질문\" in parts:\r\n            q_idx = parts.index(\"질문\")\r\n            # \"질문 :\" 다음부터 \"키워드\" 전까지가 질문 내용\r\n            if q_idx + 2 < len(parts):\r\n                question_parts = []\r\n                for i in range(q_idx + 2, len(parts)):\r\n                    if parts[i] == \"키워드\":\r\n                        break\r\n                    question_parts.append(parts[i])\r\n                question = \" \".join(question_parts).replace(\"'\", \"\").replace('\"', \"\")\r\n        \r\n        # \"키워드\" 찾기\r\n        if \"키워드\" in parts:\r\n            k_idx = parts.index(\"키워드\")\r\n            if k_idx + 1 < len(parts):\r\n                keyword_parts = parts[k_idx + 1:]\r\n                keywords = [k.replace(\":\", \"\").strip() for k in keyword_parts if k.strip()]\r\n        \r\n        keyword_text = \" \".join(keywords)\r\n        return question, keyword_text\r\n\r\n    def get_parsed_data(self):\r\n        \"\"\"공통 파싱 로직\"\"\"\r\n        try:\r\n            # HyDE 출력에서 텍스트만 추출\r\n            raw_data = self.hyde_output\r\n            \r\n            # 디버깅: LLM이 생성한 전체 내용을 로그로 남기기\r\n            self.log(f\"[DEBUG 1] 받은 데이터 타입: {type(raw_data)}\")\r\n            self.log(f\"[DEBUG 2] 받은 데이터 전체 내용: {raw_data}\")\r\n            self.log(f\"[DEBUG 3] raw_data.text 직접 출력: {raw_data.text}\")\r\n            self.log(f\"[DEBUG 4] raw_data.text 길이: {len(str(raw_data.text))}\")\r\n            self.log(f\"[DEBUG 5] raw_data.text 전체 내용: '{str(raw_data.text)}'\")\r\n            \r\n            text = self.extract_text(raw_data)\r\n            self.log(f\"[DEBUG 6] 추출된 텍스트: '{text}'\")\r\n            \r\n            question, keyword_text = self.parse_text(text)\r\n            self.log(f\"[DEBUG 7] 파싱된 질문: '{question}'\")\r\n            self.log(f\"[DEBUG 8] 파싱된 키워드: '{keyword_text}'\")\r\n\r\n            return question or \"\", keyword_text or \"\"\r\n\r\n        except Exception as e:\r\n            error_message = f\"Error during parsing: {str(e)}\"\r\n            self.log(error_message)\r\n            return \"\", \"\"\r\n\r\n    def question_response(self) -> Message:\r\n        question, _ = self.get_parsed_data()\r\n        return Message(text=question)\r\n\r\n    def keywords_response(self) -> Message:\r\n        _, keywords = self.get_parsed_data()\r\n        return Message(text=keywords)"
              },
              "hyde_output": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "HyDE Output",
                "dynamic": false,
                "info": "Raw text or AIMessage output from the HyDE LLM",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "hyde_output",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "question",
          "showNode": true,
          "type": "HyDEParser"
        },
        "dragging": false,
        "id": "HyDEParser-Sgux2",
        "measured": {
          "height": 220,
          "width": 320
        },
        "position": {
          "x": -2590.0086323491946,
          "y": 1951.3865501933537
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "PythonREPLComponent-GRKxY",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Milvus에서 BM25 알고리즘을 사용하여 텍스트 키워드 검색",
            "display_name": "BM25 Milvus Search",
            "documentation": "https://docs.langflow.org/components-retrievers",
            "edited": true,
            "field_order": [
              "milvus_uri",
              "collection_name",
              "search_keywords",
              "top_k"
            ],
            "frozen": false,
            "icon": "search",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Results",
                "group_outputs": false,
                "hidden": null,
                "method": "search_results",
                "name": "results",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom.custom_component.component import Component\r\nfrom langflow.io import MessageInput, StrInput, IntInput, Output\r\nfrom langflow.schema.message import Message\r\nfrom pymilvus import connections, Collection, utility\r\nfrom typing import List, Dict\r\n\r\n\r\nclass BM25MilvusSearch(Component):\r\n    display_name = \"BM25 Milvus Search\"\r\n    description = \"Milvus에서 BM25 알고리즘을 사용하여 텍스트 키워드 검색\"\r\n    documentation: str = \"https://docs.langflow.org/components-retrievers\"\r\n    icon = \"search\"\r\n\r\n    inputs = [\r\n        StrInput(\r\n            name=\"milvus_uri\",\r\n            display_name=\"Milvus URI\",\r\n            info=\"Milvus 서버 주소\",\r\n            value=\"http://localhost:19530\",\r\n            required=True,\r\n        ),\r\n        StrInput(\r\n            name=\"collection_name\",\r\n            display_name=\"Collection Name\",\r\n            info=\"검색할 Milvus 컬렉션 이름\",\r\n            value=\"qqqq\",\r\n            required=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"search_keywords\",\r\n            display_name=\"Search Keywords\",\r\n            info=\"검색할 키워드 텍스트\",\r\n            input_types=[\"Message\"],\r\n            required=True,\r\n        ),\r\n        IntInput(\r\n            name=\"top_k\",\r\n            display_name=\"Top K Results\",\r\n            info=\"반환할 검색 결과 개수\",\r\n            value=10,\r\n            required=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Results\", name=\"results\", method=\"search_results\"),\r\n    ]\r\n\r\n    def connect_milvus(self):\r\n        \"\"\"Milvus 서버에 연결\"\"\"\r\n        try:\r\n            # 기존 연결이 있으면 해제\r\n            try:\r\n                connections.disconnect(\"default\")\r\n            except:\r\n                pass\r\n            \r\n            # 새로운 연결 생성\r\n            connections.connect(\r\n                alias=\"default\",\r\n                uri=self.milvus_uri\r\n            )\r\n            self.log(f\"Milvus 연결 성공: {self.milvus_uri}\")\r\n            return True\r\n        except Exception as e:\r\n            self.log(f\"Milvus 연결 실패: {str(e)}\")\r\n            return False\r\n\r\n    def check_collection_exists(self):\r\n        \"\"\"컬렉션 존재 여부 확인\"\"\"\r\n        try:\r\n            exists = utility.has_collection(self.collection_name)\r\n            if exists:\r\n                self.log(f\"컬렉션 '{self.collection_name}' 존재 확인\")\r\n            else:\r\n                self.log(f\"컬렉션 '{self.collection_name}'이 존재하지 않습니다\")\r\n            return exists\r\n        except Exception as e:\r\n            self.log(f\"컬렉션 확인 중 오류: {str(e)}\")\r\n            return False\r\n\r\n    def extract_keywords(self, data):\r\n        \"\"\"입력 데이터에서 키워드 텍스트 추출\"\"\"\r\n        if hasattr(data, 'text'):\r\n            return str(data.text)\r\n        if hasattr(data, 'content'):\r\n            return str(data.content)\r\n        if isinstance(data, str):\r\n            return data\r\n        if isinstance(data, dict):\r\n            if \"text\" in data:\r\n                return str(data[\"text\"])\r\n            if \"content\" in data:\r\n                return str(data[\"content\"])\r\n        return str(data)\r\n\r\n    def bm25_search(self, keywords: str, top_k: int = 10) -> List[Dict]:\r\n        \"\"\"\r\n        BM25 알고리즘을 사용한 Milvus 검색\r\n        Milvus의 query()를 사용하여 텍스트 필드 검색 후 BM25 스코어링\r\n        \"\"\"\r\n        try:\r\n            # 컬렉션 로드\r\n            collection = Collection(self.collection_name)\r\n            \r\n            # 컬렉션 로드 (검색을 위해 메모리에 로드)\r\n            collection.load()\r\n            self.log(f\"📚 컬렉션 '{self.collection_name}' 로드 완료\")\r\n            \r\n            # 컬렉션 스키마 확인\r\n            schema = collection.schema\r\n            text_field = None\r\n            for field in schema.fields:\r\n                if field.dtype.name in ['VARCHAR', 'STRING']:\r\n                    text_field = field.name\r\n                    self.log(f\"📝 텍스트 필드 발견: {text_field}\")\r\n                    break\r\n            \r\n            if not text_field:\r\n                self.log(\"❌ 텍스트 필드를 찾을 수 없습니다\")\r\n                return []\r\n            \r\n            # 키워드 분리\r\n            keyword_list = keywords.strip().split()\r\n            self.log(f\"🔑 분리된 키워드: {keyword_list}\")\r\n            \r\n            # 각 키워드에 대해 OR 검색 수행\r\n            # Milvus query에서는 expr을 사용한 필터링\r\n            all_results = []\r\n            \r\n            # 전체 데이터 가져오기 (정확한 BM25 계산을 위해 모든 데이터 조회)\r\n            try:\r\n                # expr 없이 전체 쿼리 (limit 최대값 사용)\r\n                query_results = collection.query(\r\n                    expr=\"\",\r\n                    output_fields=[\"*\"],\r\n                    limit=16384  # Milvus 최대 limit (전체 데이터 조회)\r\n                )\r\n                self.log(f\"📊 쿼리 결과: {len(query_results)}개 문서 (전체 데이터 대상)\")\r\n            except Exception as e:\r\n                # expr=\"\" 이 안되면 전체 조회 시도\r\n                self.log(f\"⚠️ 전체 쿼리 실패, 대체 방법 시도: {str(e)}\")\r\n                query_results = collection.query(\r\n                    expr=f\"{text_field} != ''\",\r\n                    output_fields=[\"*\"],\r\n                    limit=16384  # Milvus 최대 limit\r\n                )\r\n            \r\n            # BM25 스코어 계산 (간단한 TF-IDF 기반 스코어링)\r\n            from collections import Counter\r\n            import math\r\n            \r\n            scored_results = []\r\n            for doc in query_results:\r\n                if text_field not in doc:\r\n                    continue\r\n                \r\n                doc_text = str(doc[text_field]).lower()\r\n                doc_terms = doc_text.split()\r\n                \r\n                # BM25 파라미터\r\n                k1 = 1.5\r\n                b = 0.75\r\n                avgdl = sum(len(str(d.get(text_field, \"\")).split()) for d in query_results) / len(query_results)\r\n                doc_len = len(doc_terms)\r\n                \r\n                score = 0.0\r\n                doc_term_freq = Counter(doc_terms)\r\n                \r\n                for keyword in keyword_list:\r\n                    keyword_lower = keyword.lower()\r\n                    if keyword_lower in doc_term_freq:\r\n                        # TF (Term Frequency)\r\n                        tf = doc_term_freq[keyword_lower]\r\n                        \r\n                        # IDF 계산 (전체 문서에서 키워드 포함 개수)\r\n                        df = sum(1 for d in query_results if keyword_lower in str(d.get(text_field, \"\")).lower())\r\n                        idf = math.log((len(query_results) - df + 0.5) / (df + 0.5) + 1.0)\r\n                        \r\n                        # BM25 스코어\r\n                        numerator = tf * (k1 + 1)\r\n                        denominator = tf + k1 * (1 - b + b * (doc_len / avgdl))\r\n                        score += idf * (numerator / denominator)\r\n                \r\n                if score > 0:\r\n                    result_dict = {\r\n                        \"score\": score,\r\n                        **doc\r\n                    }\r\n                    scored_results.append(result_dict)\r\n            \r\n            # 스코어 기준으로 정렬\r\n            scored_results.sort(key=lambda x: x['score'], reverse=True)\r\n            \r\n            # Top K 결과만 반환\r\n            final_results = scored_results[:top_k]\r\n            \r\n            self.log(f\"🔍 BM25 검색 완료: {len(final_results)} 개의 결과\")\r\n            \r\n            return final_results\r\n            \r\n        except Exception as e:\r\n            self.log(f\"❌ 검색 중 오류 발생: {str(e)}\")\r\n            import traceback\r\n            self.log(f\"📋 상세 오류: {traceback.format_exc()}\")\r\n            return []\r\n\r\n    def search_results(self) -> Message:\r\n        \"\"\"검색 실행 및 결과 반환\"\"\"\r\n        try:\r\n            # 1. Milvus 연결\r\n            if not self.connect_milvus():\r\n                return Message(text=\"Milvus 연결 실패\")\r\n            \r\n            # 2. 컬렉션 존재 확인\r\n            if not self.check_collection_exists():\r\n                return Message(text=f\"컬렉션 '{self.collection_name}'이 존재하지 않습니다\")\r\n            \r\n            # 3. 키워드 추출\r\n            keywords = self.extract_keywords(self.search_keywords)\r\n            self.log(f\"🔑 검색 키워드: '{keywords}'\")\r\n            \r\n            # 4. BM25 검색 실행\r\n            top_k = self.top_k if hasattr(self, 'top_k') and self.top_k else 10\r\n            results = self.bm25_search(keywords, top_k)\r\n            \r\n            # 5. 결과 포맷팅\r\n            if not results:\r\n                return Message(text=\"검색 결과가 없습니다\")\r\n            \r\n            # 결과를 텍스트로 변환 (벡터 필드 제외)\r\n            result_text = f\"🔍 BM25 검색 결과 (총 {len(results)}개)\\n\\n\"\r\n            for idx, result in enumerate(results, 1):\r\n                result_text += f\"[{idx}] Score: {result.get('score', 'N/A'):.4f}\\n\"\r\n                for key, value in result.items():\r\n                    # 벡터 필드(리스트), id, score, distance 제외\r\n                    if key not in ['id', 'score', 'distance', 'vector'] and not isinstance(value, list):\r\n                        result_text += f\"  {key}: {value}\\n\"\r\n                result_text += \"\\n\"\r\n            \r\n            self.log(f\"✅ 검색 완료: {len(results)}개 결과 반환\")\r\n            \r\n            return Message(text=result_text)\r\n\r\n        except Exception as e:\r\n            error_msg = f\"❌ 오류 발생: {str(e)}\"\r\n            self.log(error_msg)\r\n            return Message(text=error_msg)\r\n        \r\n        finally:\r\n            # 연결 정리\r\n            try:\r\n                connections.disconnect(\"default\")\r\n            except:\r\n                pass\r\n\r\n    def build(self) -> Message:\r\n        \"\"\"빌드 메서드\"\"\"\r\n        return self.search_results()\r\n\r\n"
              },
              "collection_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Collection Name",
                "dynamic": false,
                "info": "검색할 Milvus 컬렉션 이름",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "collection_name",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qqqq"
              },
              "milvus_uri": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Milvus URI",
                "dynamic": false,
                "info": "Milvus 서버 주소",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "milvus_uri",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://localhost:19530"
              },
              "search_keywords": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Search Keywords",
                "dynamic": false,
                "info": "검색할 키워드 텍스트",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_keywords",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "top_k": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Top K Results",
                "dynamic": false,
                "info": "반환할 검색 결과 개수",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "top_k",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1000
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "BM25MilvusSearch"
        },
        "dragging": false,
        "id": "PythonREPLComponent-GRKxY",
        "measured": {
          "height": 469,
          "width": 320
        },
        "position": {
          "x": -2356.1044049871584,
          "y": 2211.267215017494
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "PythonREPLComponent-dY4IO",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Milvus 벡터 검색과 BM25 검색 결과를 결합하여 최종 순위 생성",
            "display_name": "Hybrid Search Fusion",
            "documentation": "https://docs.langflow.org/components-retrievers",
            "edited": true,
            "field_order": [
              "milvus_results",
              "bm25_results",
              "milvus_weight",
              "bm25_weight"
            ],
            "frozen": false,
            "icon": "merge",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Fused Results",
                "group_outputs": false,
                "hidden": null,
                "method": "fuse_results",
                "name": "fused_results",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "bm25_results": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "BM25 Results",
                "dynamic": false,
                "info": "BM25 검색 결과 (텍스트 형식)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "bm25_results",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "bm25_weight": {
                "_input_type": "FloatInput",
                "advanced": false,
                "display_name": "BM25 Weight",
                "dynamic": false,
                "info": "BM25 점수 가중치 (0~1)",
                "list": false,
                "list_add_label": "Add More",
                "name": "bm25_weight",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.5
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom.custom_component.component import Component\r\nfrom langflow.io import MessageInput, FloatInput, Output\r\nfrom langflow.schema.message import Message\r\nfrom typing import List, Dict\r\nimport json\r\n\r\n\r\nclass HybridSearchFusion(Component):\r\n    display_name = \"Hybrid Search Fusion\"\r\n    description = \"Milvus 벡터 검색과 BM25 검색 결과를 결합하여 최종 순위 생성\"\r\n    documentation: str = \"https://docs.langflow.org/components-retrievers\"\r\n    icon = \"merge\"\r\n\r\n    inputs = [\r\n        MessageInput(\r\n            name=\"milvus_results\",\r\n            display_name=\"Milvus Results\",\r\n            info=\"Milvus 벡터 유사도 검색 결과 (JSON 형식)\",\r\n            input_types=[\"Message\"],\r\n            required=True,\r\n        ),\r\n        MessageInput(\r\n            name=\"bm25_results\",\r\n            display_name=\"BM25 Results\",\r\n            info=\"BM25 검색 결과 (텍스트 형식)\",\r\n            input_types=[\"Message\"],\r\n            required=True,\r\n        ),\r\n        FloatInput(\r\n            name=\"milvus_weight\",\r\n            display_name=\"Milvus Weight\",\r\n            info=\"Milvus 점수 가중치 (0~1)\",\r\n            value=0.5,\r\n            required=False,\r\n        ),\r\n        FloatInput(\r\n            name=\"bm25_weight\",\r\n            display_name=\"BM25 Weight\",\r\n            info=\"BM25 점수 가중치 (0~1)\",\r\n            value=0.5,\r\n            required=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Fused Results\", name=\"fused_results\", method=\"fuse_results\"),\r\n    ]\r\n\r\n    def extract_text(self, data):\r\n        \"\"\"입력 데이터에서 텍스트 추출\"\"\"\r\n        if hasattr(data, 'text'):\r\n            return str(data.text)\r\n        if hasattr(data, 'content'):\r\n            return str(data.content)\r\n        if isinstance(data, str):\r\n            return data\r\n        if isinstance(data, dict):\r\n            if \"text\" in data:\r\n                return str(data[\"text\"])\r\n            if \"content\" in data:\r\n                return str(data[\"content\"])\r\n        return str(data)\r\n\r\n    def parse_milvus_results(self, text: str) -> List[Dict]:\r\n        \"\"\"Milvus JSON 결과 파싱\"\"\"\r\n        results = []\r\n        try:\r\n            # 먼저 전체를 JSON으로 파싱 시도 (배열 형태)\r\n            try:\r\n                parsed = json.loads(text)\r\n                if isinstance(parsed, list):\r\n                    results = parsed\r\n                    self.log(f\"Milvus 결과 파싱 완료 (JSON 배열): {len(results)}개\")\r\n                    return results\r\n                elif isinstance(parsed, dict):\r\n                    results = [parsed]\r\n                    self.log(f\"Milvus 결과 파싱 완료 (JSON 객체): {len(results)}개\")\r\n                    return results\r\n            except json.JSONDecodeError:\r\n                pass\r\n            \r\n            # JSON 배열 파싱 실패시 줄바꿈으로 구분된 JSON 파싱\r\n            lines = text.strip().split('\\n')\r\n            for line in lines:\r\n                line = line.strip()\r\n                if line.startswith('```json'):\r\n                    continue\r\n                if line.startswith('```'):\r\n                    continue\r\n                if not line:\r\n                    continue\r\n                \r\n                try:\r\n                    result = json.loads(line)\r\n                    if isinstance(result, dict):\r\n                        results.append(result)\r\n                    elif isinstance(result, list):\r\n                        results.extend(result)\r\n                except json.JSONDecodeError:\r\n                    continue\r\n            \r\n            self.log(f\"Milvus 결과 파싱 완료 (줄바꿈 구분): {len(results)}개\")\r\n            return results\r\n        except Exception as e:\r\n            self.log(f\"Milvus 결과 파싱 오류: {str(e)}\")\r\n            return []\r\n\r\n    def parse_bm25_results(self, text: str) -> List[Dict]:\r\n        \"\"\"BM25 텍스트 결과 파싱\"\"\"\r\n        results = []\r\n        try:\r\n            lines = text.strip().split('\\n')\r\n            current_result = None\r\n            \r\n            for line in lines:\r\n                line = line.strip()\r\n                \r\n                # 결과 시작 (예: [1] Score: 6.5163)\r\n                if line.startswith('[') and '] Score:' in line:\r\n                    if current_result:\r\n                        results.append(current_result)\r\n                    \r\n                    # Score 추출\r\n                    score_str = line.split('Score:')[1].strip()\r\n                    current_result = {\r\n                        'score': float(score_str),\r\n                        'text': '',\r\n                        'id': None\r\n                    }\r\n                \r\n                # text 필드\r\n                elif current_result and line.startswith('text:'):\r\n                    current_result['text'] = line[5:].strip()\r\n                \r\n                # id 필드 (선택적)\r\n                elif current_result and line.startswith('id:'):\r\n                    try:\r\n                        current_result['id'] = int(line[3:].strip())\r\n                    except:\r\n                        pass\r\n            \r\n            # 마지막 결과 추가\r\n            if current_result:\r\n                results.append(current_result)\r\n            \r\n            self.log(f\"BM25 결과 파싱 완료: {len(results)}개\")\r\n            return results\r\n        except Exception as e:\r\n            self.log(f\"BM25 결과 파싱 오류: {str(e)}\")\r\n            return []\r\n\r\n    def normalize_scores(self, scores: List[float]) -> List[float]:\r\n        \"\"\"점수를 0~1 범위로 정규화\"\"\"\r\n        if not scores:\r\n            return []\r\n        \r\n        min_score = min(scores)\r\n        max_score = max(scores)\r\n        \r\n        if max_score == min_score:\r\n            return [1.0] * len(scores)\r\n        \r\n        return [(s - min_score) / (max_score - min_score) for s in scores]\r\n\r\n    def fuse_results(self) -> Message:\r\n        \"\"\"두 검색 결과를 결합\"\"\"\r\n        try:\r\n            # 1. 입력 데이터 추출\r\n            milvus_text = self.extract_text(self.milvus_results)\r\n            bm25_text = self.extract_text(self.bm25_results)\r\n            \r\n            self.log(\"입력 데이터 추출 완료\")\r\n            \r\n            # 2. 결과 파싱\r\n            milvus_results = self.parse_milvus_results(milvus_text)\r\n            bm25_results = self.parse_bm25_results(bm25_text)\r\n            \r\n            self.log(f\"DEBUG - Milvus 결과 개수: {len(milvus_results)}\")\r\n            self.log(f\"DEBUG - BM25 결과 개수: {len(bm25_results)}\")\r\n            \r\n            if milvus_results:\r\n                self.log(f\"DEBUG - Milvus 첫 번째 결과 샘플: {milvus_results[0]}\")\r\n            if bm25_results:\r\n                self.log(f\"DEBUG - BM25 첫 번째 결과 샘플: {bm25_results[0]}\")\r\n            \r\n            if not milvus_results and not bm25_results:\r\n                return Message(text=\"검색 결과가 없습니다\")\r\n            \r\n            # 3. 문서별 점수 매핑 (text를 키로 사용)\r\n            doc_scores = {}\r\n            \r\n            # Milvus 점수 처리 (거리 기반이므로 낮을수록 좋음 -> 역수 사용)\r\n            if milvus_results:\r\n                milvus_scores = [1.0 / (1.0 + r.get('similarity_score', 1.0)) for r in milvus_results]\r\n                normalized_milvus = self.normalize_scores(milvus_scores)\r\n                \r\n                for i, result in enumerate(milvus_results):\r\n                    text = result.get('text', '')\r\n                    if text:\r\n                        doc_scores[text] = {\r\n                            'text': text,\r\n                            'id': result.get('id'),\r\n                            'milvus_score': normalized_milvus[i],\r\n                            'bm25_score': 0.0,\r\n                            'similarity_score': result.get('similarity_score', 0),\r\n                        }\r\n            \r\n            # BM25 점수 처리 (높을수록 좋음)\r\n            if bm25_results:\r\n                bm25_scores = [r.get('score', 0.0) for r in bm25_results]\r\n                normalized_bm25 = self.normalize_scores(bm25_scores)\r\n                \r\n                for i, result in enumerate(bm25_results):\r\n                    text = result.get('text', '')\r\n                    if text:\r\n                        if text in doc_scores:\r\n                            doc_scores[text]['bm25_score'] = normalized_bm25[i]\r\n                            doc_scores[text]['bm25_raw_score'] = bm25_scores[i]\r\n                        else:\r\n                            doc_scores[text] = {\r\n                                'text': text,\r\n                                'id': result.get('id'),\r\n                                'milvus_score': 0.0,\r\n                                'bm25_score': normalized_bm25[i],\r\n                                'bm25_raw_score': bm25_scores[i],\r\n                            }\r\n            \r\n            # 4. 최종 점수 계산 (가중 합)\r\n            milvus_weight = self.milvus_weight if hasattr(self, 'milvus_weight') else 0.5\r\n            bm25_weight = self.bm25_weight if hasattr(self, 'bm25_weight') else 0.5\r\n            \r\n            # 겹치는 문서 개수 확인\r\n            both_found = sum(1 for scores in doc_scores.values() if scores['milvus_score'] > 0 and scores['bm25_score'] > 0)\r\n            only_milvus = sum(1 for scores in doc_scores.values() if scores['milvus_score'] > 0 and scores['bm25_score'] == 0)\r\n            only_bm25 = sum(1 for scores in doc_scores.values() if scores['milvus_score'] == 0 and scores['bm25_score'] > 0)\r\n            \r\n            self.log(f\"DEBUG - 두 검색 모두에서 발견: {both_found}개\")\r\n            self.log(f\"DEBUG - Milvus에만 있음: {only_milvus}개\")\r\n            self.log(f\"DEBUG - BM25에만 있음: {only_bm25}개\")\r\n            \r\n            for text, scores in doc_scores.items():\r\n                scores['final_score'] = (\r\n                    scores['milvus_score'] * milvus_weight + \r\n                    scores['bm25_score'] * bm25_weight\r\n                )\r\n            \r\n            # 5. 최종 점수로 정렬\r\n            sorted_results = sorted(\r\n                doc_scores.values(), \r\n                key=lambda x: x['final_score'], \r\n                reverse=True\r\n            )\r\n            \r\n            # 6. 결과 포맷팅\r\n            result_text = f\"Hybrid Search Results (총 {len(sorted_results)}개)\\n\"\r\n            result_text += f\"Milvus 가중치: {milvus_weight}, BM25 가중치: {bm25_weight}\\n\\n\"\r\n            \r\n            for idx, result in enumerate(sorted_results, 1):\r\n                result_text += f\"[{idx}] Final Score: {result['final_score']:.4f}\\n\"\r\n                result_text += f\"  Milvus: {result['milvus_score']:.4f} | BM25: {result['bm25_score']:.4f}\\n\"\r\n                \r\n                if 'similarity_score' in result:\r\n                    result_text += f\"  원본 유사도: {result['similarity_score']:.4f}\\n\"\r\n                if 'bm25_raw_score' in result:\r\n                    result_text += f\"  원본 BM25: {result['bm25_raw_score']:.4f}\\n\"\r\n                \r\n                if result.get('id'):\r\n                    result_text += f\"  ID: {result['id']}\\n\"\r\n                \r\n                text_preview = result['text'][:200] + '...' if len(result['text']) > 200 else result['text']\r\n                result_text += f\"  Text: {text_preview}\\n\\n\"\r\n            \r\n            self.log(f\"하이브리드 검색 완료: {len(sorted_results)}개 결과\")\r\n            \r\n            return Message(text=result_text)\r\n            \r\n        except Exception as e:\r\n            error_msg = f\"오류 발생: {str(e)}\"\r\n            self.log(error_msg)\r\n            import traceback\r\n            self.log(f\"상세 오류: {traceback.format_exc()}\")\r\n            return Message(text=error_msg)\r\n\r\n    def build(self) -> Message:\r\n        \"\"\"빌드 메서드\"\"\"\r\n        return self.fuse_results()\r\n\r\n"
              },
              "milvus_results": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Milvus Results",
                "dynamic": false,
                "info": "Milvus 벡터 유사도 검색 결과 (JSON 형식)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "milvus_results",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "milvus_weight": {
                "_input_type": "FloatInput",
                "advanced": false,
                "display_name": "Milvus Weight",
                "dynamic": false,
                "info": "Milvus 점수 가중치 (0~1)",
                "list": false,
                "list_add_label": "Add More",
                "name": "milvus_weight",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.5
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "HybridSearchFusion"
        },
        "dragging": false,
        "id": "PythonREPLComponent-dY4IO",
        "measured": {
          "height": 469,
          "width": 320
        },
        "position": {
          "x": -1644.9295383677947,
          "y": 2050.384121332286
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-YITzY",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are a digital forensic classification assistant.\nYou will receive a forensic-related question or statement in Korean.\nYour task is to:\n1. Translate the question into natural English.\n2. Determine which category (tag) the question belongs to, based on the tag descriptions below.\n3. Output both the original Korean question, its English translation, and the selected tag.\n\nSelect exactly ONE tag that best represents the topic of the question.\n\n---\n\n### 🏷️ Available Tags and Descriptions\n\n1️⃣ **System**\n   - Relates to operating system components, user account management, host configuration, registry data, or system-level settings and logs.\n   - Focuses on OS-level information and system structure rather than user interaction or specific programs.\n\n2️⃣ **Execution**\n   - Concerns evidence of program or process execution.\n   - Includes Prefetch, Run keys, process creation, script execution, or any record showing a program being launched or run on the system.\n\n3️⃣ **UserActivity**\n   - Refers to traces of user actions and interactions with the system.\n   - Includes file access, recent documents, browser activity, USB usage, or shellbag artifacts indicating user behavior.\n\n4️⃣ **Installation**\n   - Focuses on evidence of software or component installation.\n   - Includes installed applications, extensions, program setup traces, and configuration changes made during installation.\n\n5️⃣ **Metadata**\n   - Relates to file system metadata and structural information.\n   - Includes timestamps (creation, modification, access), NTFS MFT data, file attributes, and general file object details.\n\n6️⃣ **Exfiltration**\n   - Pertains to evidence of data leakage or transfer outside the host system.\n   - Includes uploads, network transmissions, email attachments, or any activity suggesting data exfiltration via protocols like HTTP, FTP, SMTP, or IMAP.\n\n---\n\n### 🧩 Output Format\n\nRespond **only in JSON** using this structure:\n\n{\n  \"original_question\": \"<the original Korean question>\",\n  \"translated_question\": \"<the question translated into English>\",\n  \"tag\": \"<one of [System, Execution, UserActivity, Installation, Metadata, Exfiltration]>\"\n}\n\n---\n\nNow translate and classify the following question based on the above tag descriptions:\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-YITzY",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": -3895.501791637671,
          "y": 2269.8636938193486
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "SQLComponent-ybWaK",
          "node": {
            "base_classes": [
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Executes SQL queries on SQLAlchemy-compatible databases.",
            "display_name": "SQL Database",
            "documentation": "https://docs.langflow.org/components-data#sql-database",
            "edited": false,
            "field_order": [
              "database_url",
              "query",
              "include_columns",
              "add_error"
            ],
            "frozen": false,
            "icon": "database",
            "legacy": false,
            "metadata": {
              "keywords": [
                "sql",
                "database",
                "query",
                "db",
                "fetch"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Result Table",
                "group_outputs": false,
                "method": "run_sql_query",
                "name": "run_sql_query",
                "selected": "DataFrame",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "add_error": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Add Error",
                "dynamic": false,
                "info": "If True, the error will be added to the result",
                "list": false,
                "list_add_label": "Add More",
                "name": "add_error",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import TYPE_CHECKING, Any\n\nfrom langchain_community.utilities import SQLDatabase\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.io import BoolInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.services.cache.utils import CacheMiss\n\nif TYPE_CHECKING:\n    from sqlalchemy.engine import Result\n\n\nclass SQLComponent(ComponentWithCache):\n    \"\"\"A sql component.\"\"\"\n\n    display_name = \"SQL Database\"\n    description = \"Executes SQL queries on SQLAlchemy-compatible databases.\"\n    documentation: str = \"https://docs.langflow.org/components-data#sql-database\"\n    icon = \"database\"\n    name = \"SQLComponent\"\n    metadata = {\"keywords\": [\"sql\", \"database\", \"query\", \"db\", \"fetch\"]}\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.db: SQLDatabase = None\n\n    def maybe_create_db(self):\n        if self.database_url != \"\":\n            cached_db = self._shared_component_cache.get(self.database_url)\n            if not isinstance(cached_db, CacheMiss):\n                self.db = cached_db\n                return\n            self.log(\"Connecting to database\")\n            try:\n                self.db = SQLDatabase.from_uri(self.database_url)\n            except Exception as e:\n                msg = f\"An error occurred while connecting to the database: {e}\"\n                raise ValueError(msg) from e\n            self._shared_component_cache.set(self.database_url, self.db)\n\n    inputs = [\n        MessageTextInput(name=\"database_url\", display_name=\"Database URL\", required=True),\n        MultilineInput(name=\"query\", display_name=\"SQL Query\", tool_mode=True, required=True),\n        BoolInput(name=\"include_columns\", display_name=\"Include Columns\", value=True, tool_mode=True, advanced=True),\n        BoolInput(\n            name=\"add_error\",\n            display_name=\"Add Error\",\n            value=False,\n            tool_mode=True,\n            info=\"If True, the error will be added to the result\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Result Table\", name=\"run_sql_query\", method=\"run_sql_query\"),\n    ]\n\n    def build_component(\n        self,\n    ) -> Message:\n        error = None\n        self.maybe_create_db()\n        try:\n            result = self.db.run(self.query, include_columns=self.include_columns)\n            self.status = result\n        except SQLAlchemyError as e:\n            msg = f\"An error occurred while running the SQL Query: {e}\"\n            self.log(msg)\n            result = str(e)\n            self.status = result\n            error = repr(e)\n\n        if self.add_error and error is not None:\n            result = f\"{result}\\n\\nError: {error}\\n\\nQuery: {self.query}\"\n        elif error is not None:\n            # Then we won't add the error to the result\n            result = self.query\n\n        return Message(text=result)\n\n    def __execute_query(self) -> list[dict[str, Any]]:\n        self.maybe_create_db()\n        try:\n            cursor: Result[Any] = self.db.run(self.query, fetch=\"cursor\")\n            return [x._asdict() for x in cursor.fetchall()]\n        except SQLAlchemyError as e:\n            msg = f\"An error occurred while running the SQL Query: {e}\"\n            self.log(msg)\n            raise ValueError(msg) from e\n\n    def run_sql_query(self) -> DataFrame:\n        result = self.__execute_query()\n        df_result = DataFrame(result)\n        self.status = df_result\n        return df_result\n"
              },
              "database_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Database URL",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "database_url",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "include_columns": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Include Columns",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "include_columns",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "query": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "SQL Query",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "query",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "SQLComponent"
        },
        "id": "SQLComponent-ybWaK",
        "measured": {
          "height": 303,
          "width": 320
        },
        "position": {
          "x": -3639.2587007528205,
          "y": 903.3758015910222
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "HyDEParser-wA8XI",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Parse HyDE LLM output into question and tags.",
            "display_name": "HyDE Tag Parser",
            "documentation": "https://docs.langflow.org/components-processing#hyde-parser",
            "edited": true,
            "field_order": [
              "hyde_output"
            ],
            "frozen": false,
            "icon": "square-terminal",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Question",
                "group_outputs": false,
                "hidden": null,
                "method": "question_response",
                "name": "question",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Tags",
                "group_outputs": false,
                "hidden": null,
                "method": "tags_response",
                "name": "tags",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import re\r\nimport json\r\nfrom langflow.base.io.text import TextComponent\r\nfrom langflow.io import MessageInput, Output\r\nfrom langflow.schema.message import Message\r\n\r\nclass HyDEParser(TextComponent):\r\n    display_name = \"HyDE Tag Parser\"\r\n    description = \"Parse HyDE LLM output into question and tags.\"\r\n    documentation: str = \"https://docs.langflow.org/components-processing#hyde-parser\"\r\n    icon = \"square-terminal\"\r\n\r\n    inputs = [\r\n        MessageInput(\r\n            name=\"hyde_output\",\r\n            display_name=\"HyDE Output\",\r\n            info=\"Raw text or AIMessage output from the HyDE LLM\",\r\n            input_types=[\"Message\"],\r\n            tool_mode=True,\r\n            required=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Question\", name=\"question\", method=\"question_response\"),\r\n        Output(display_name=\"Tags\", name=\"tags\", method=\"tags_response\"),\r\n    ]\r\n\r\n    def extract_text(self, data):\r\n        \"\"\"HyDE 출력이 AIMessage, Message, dict, 또는 str 중 어떤 형식이든 텍스트만 추출.\"\"\"\r\n        if hasattr(data, 'text'):\r\n            return str(data.text)\r\n        if hasattr(data, 'content'):\r\n            return str(data.content)\r\n        if isinstance(data, str):\r\n            return data\r\n        if isinstance(data, dict):\r\n            if \"data\" in data and isinstance(data[\"data\"], dict) and \"text\" in data[\"data\"]:\r\n                return str(data[\"data\"][\"text\"])\r\n            if \"text\" in data:\r\n                return str(data[\"text\"])\r\n            if \"content\" in data:\r\n                return str(data[\"content\"])\r\n        text = str(data)\r\n        if \"content='\" in text or 'content=\"' in text:\r\n            match = re.search(r\"content=['\\\"](.+?)['\\\"]\", text, re.DOTALL)\r\n            if match:\r\n                return match.group(1)\r\n        return text\r\n\r\n    def parse_text(self, text: str):\r\n        question = \"\"\r\n        tags = \"\"\r\n\r\n        # ✅ 1️⃣ 영어 포맷 (Question / tag:)\r\n        match_q = re.search(r\"(?i)question\\s*(?:\\(.*?\\))?\\s*:\\s*(.+?)(?:tag|$)\", text, re.DOTALL)\r\n        match_t = re.search(r\"(?i)tag\\s*[:：]\\s*(.+)\", text, re.DOTALL)\r\n\r\n        if match_q:\r\n            question = match_q.group(1).strip()\r\n        if match_t:\r\n            tags = match_t.group(1).strip()\r\n\r\n        # ✅ 2️⃣ 한국어 포맷 (질문 / 태그)\r\n        if not question:\r\n            match_q2 = re.search(r\"질문\\s*[:：]\\s*(.+?)(?:태그|$)\", text)\r\n            if match_q2:\r\n                question = match_q2.group(1).strip()\r\n        if not tags:\r\n            match_t2 = re.search(r\"태그\\s*[:：]\\s*(.+)\", text)\r\n            if match_t2:\r\n                tags = match_t2.group(1).strip()\r\n\r\n        return question, tags\r\n\r\n    \r\n\r\n    def get_parsed_data(self):\r\n        \"\"\"공통 파싱 로직\"\"\"\r\n        try:\r\n            raw_data = self.hyde_output\r\n            self.log(f\"[DEBUG 1] 받은 데이터 타입: {type(raw_data)}\")\r\n            self.log(f\"[DEBUG 2] 받은 데이터 전체 내용: {raw_data}\")\r\n\r\n            text = self.extract_text(raw_data)\r\n            self.log(f\"[DEBUG 3] 추출된 텍스트: '{text}'\")\r\n            \r\n            question, tag_text = self.parse_text(text)\r\n            self.log(f\"[DEBUG 4] 파싱된 질문: '{question}'\")\r\n            self.log(f\"[DEBUG 5] 파싱된 태그: '{tag_text}'\")\r\n\r\n            return question or \"\", tag_text or \"\"\r\n\r\n        except Exception as e:\r\n            error_message = f\"Error during parsing: {str(e)}\"\r\n            self.log(error_message)\r\n            return \"\", \"\"\r\n\r\n    def question_response(self) -> Message:\r\n        question, _ = self.get_parsed_data()\r\n        return Message(text=question)\r\n\r\n    def tags_response(self) -> Message:\r\n        _, tags = self.get_parsed_data()\r\n        return Message(text=tags)\r\n"
              },
              "hyde_output": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "HyDE Output",
                "dynamic": false,
                "info": "Raw text or AIMessage output from the HyDE LLM",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "hyde_output",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "tags",
          "showNode": true,
          "type": "HyDEParser"
        },
        "dragging": false,
        "id": "HyDEParser-wA8XI",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": -3162.138481746303,
          "y": 3248.0479123917685
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-e1CpC",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "https://docs.langflow.org/components-io#text-input",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are a digital forensics classification assistant.\n\nYour task is to analyze a user's question (written in Korean or English) and produce two outputs:\n1. The question translated into clear English.\n2. One or more relevant forensic tags logically inferred from the question.\n\n---\n\n### Tag Inference Guidelines\nEach tag represents a forensic evidence domain.  \nSelect tags by reasoning about **the type of digital behavior or event** being investigated — not by literal words.\n\n- **System** → Used **only** when the question asks about normal operating system configurations, user accounts, or registry settings — not when describing attacks or suspicious activity.  \n- **Execution** → Evidence of program or process execution, including malware runs, script activity, or command execution.  \n- **UserActivity** → Human interaction traces such as file access, browsing, or device usage.  \n- **Installation** → Software setup, configuration change, or malicious installation evidence.  \n- **Metadata** → Timestamps, file attributes, or contextual information about artifacts.  \n- **Exfiltration** → Any case involving data leakage, unauthorized network transfer, or information theft.\n\nYou may select **multiple tags** if the question implies more than one forensic domain.  \nInfer conceptually which domains are relevant to **the action or behavior** described.\n\n---\n\n### Output Format\nReturn the result **strictly** in this format, without explanations:\n\nQuestion : <translated question>  \ntag: <comma-separated tag list>\n\n---\n\n### Behavioral Instructions\n- Focus on **what digital behavior or evidence** the question refers to (e.g., execution, exfiltration, user action).  \n- Use **System** only for questions about environment configuration, not for suspicious events or attacks.  \n- Prefer **Execution** and **Exfiltration** for hacking attempts, malware runs, or data theft questions.  \n- Ensure outputs are concise and logically inferred for forensic query generation.\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-e1CpC",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": -3635.5998567801726,
          "y": 3212.8495589365184
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ForensicMilvusEmbedderUnified-28zYl",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Embed all forensic DB tables (by tag) into a single Milvus collection.",
            "display_name": "Forensic Milvus Embedder (Unified)",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "edited": true,
            "field_order": [
              "tags",
              "dbname",
              "user",
              "password",
              "host",
              "port",
              "milvus_host",
              "milvus_port"
            ],
            "frozen": false,
            "icon": "database",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Summary",
                "group_outputs": false,
                "hidden": null,
                "method": "run_embedder",
                "name": "output",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import os\r\nimport gc\r\nimport time\r\nimport json\r\nimport logging\r\nimport pandas as pd\r\nimport psycopg2\r\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\r\nfrom pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom langflow.custom.custom_component.component import Component\r\nfrom langflow.io import MessageTextInput, Output\r\nfrom langflow.schema.data import Data\r\n\r\n\r\nclass ForensicMilvusEmbedder(Component):\r\n    display_name = \"Forensic Milvus Embedder (Unified)\"\r\n    description = \"Embed all forensic DB tables (by tag) into a single Milvus collection.\"\r\n    documentation = \"https://docs.langflow.org/components-custom-components\"\r\n    icon = \"database\"\r\n    name = \"ForensicMilvusEmbedderUnified\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"tags\",\r\n            display_name=\"Tags (comma-separated)\",\r\n            info=\"Tags detected by HyDE Tag Parser, e.g., 'System, UserActivity'\",\r\n            value=\"System\",\r\n            tool_mode=True,\r\n        ),\r\n        MessageTextInput(name=\"dbname\", display_name=\"Database Name\", value=\"forensic_db\", tool_mode=True),\r\n        MessageTextInput(name=\"user\", display_name=\"DB Username\", value=\"postgres\", tool_mode=True),\r\n        MessageTextInput(name=\"password\", display_name=\"DB Password\", value=\"admin123\", tool_mode=True),\r\n        MessageTextInput(name=\"host\", display_name=\"DB Host\", value=\"localhost\", tool_mode=True),\r\n        MessageTextInput(name=\"port\", display_name=\"DB Port\", value=\"5432\", tool_mode=True),\r\n        MessageTextInput(name=\"milvus_host\", display_name=\"Milvus Host\", value=\"localhost\", tool_mode=True),\r\n        MessageTextInput(name=\"milvus_port\", display_name=\"Milvus Port\", value=\"19530\", tool_mode=True),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Embedding Summary\", name=\"output\", method=\"run_embedder\"),\r\n    ]\r\n\r\n    def run_embedder(self) -> Data:\r\n        tags = [t.strip().lower() for t in self.tags.split(\",\") if t.strip()]\r\n        if not tags:\r\n            return Data(value=\"❌ No tags provided.\")\r\n\r\n        db_info = dict(\r\n            dbname=self.dbname,\r\n            user=self.user,\r\n            password=self.password,\r\n            host=self.host,\r\n            port=self.port\r\n        )\r\n\r\n        log_messages = []\r\n        start_total = time.time()\r\n        DIM = 384\r\n        BATCH_SIZE = 200\r\n        THREADS = 5\r\n        collection_name = \"forensic_artifacts\"\r\n\r\n        # 1️⃣ Milvus 연결\r\n        try:\r\n            connections.connect(\"default\", host=self.milvus_host, port=self.milvus_port)\r\n            log_messages.append(f\"✅ Connected to Milvus ({self.milvus_host}:{self.milvus_port})\")\r\n        except Exception as e:\r\n            return Data(value=f\"❌ Milvus 연결 실패: {e}\")\r\n\r\n        # 기존 콜렉션 제거\r\n        if utility.has_collection(collection_name):\r\n            utility.drop_collection(collection_name)\r\n\r\n        # 2️⃣ 모델 로드\r\n        try:\r\n            model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\r\n            log_messages.append(\"✅ Embedding model loaded (MiniLM-L12-v2)\")\r\n        except Exception as e:\r\n            return Data(value=f\"❌ 모델 로드 실패: {e}\")\r\n\r\n        # 3️⃣ Milvus 콜렉션 생성 (단일)\r\n        try:\r\n            fields = [\r\n                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\r\n                FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=DIM),\r\n                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\r\n                FieldSchema(name=\"tag\", dtype=DataType.VARCHAR, max_length=50),\r\n            ]\r\n            schema = CollectionSchema(fields, description=\"Unified forensic artifacts\")\r\n            collection = Collection(name=collection_name, schema=schema)\r\n            collection.create_index(\r\n                field_name=\"vector\",\r\n                index_params={\"metric_type\": \"IP\", \"index_type\": \"HNSW\", \"params\": {\"M\": 8, \"efConstruction\": 64}}\r\n            )\r\n            log_messages.append(f\"✅ Milvus collection '{collection_name}' created.\")\r\n        except Exception as e:\r\n            return Data(value=f\"❌ Milvus 콜렉션 생성 실패: {e}\")\r\n\r\n        # 4️⃣ PostgreSQL 연결\r\n        try:\r\n            conn = psycopg2.connect(**db_info)\r\n            cur = conn.cursor()\r\n            log_messages.append(f\"✅ Connected to PostgreSQL ({self.dbname})\")\r\n        except Exception as e:\r\n            return Data(value=f\"❌ DB 연결 실패: {e}\")\r\n\r\n        # 5️⃣ 모든 태그 테이블 데이터를 하나로 임베딩\r\n        def embed_table(table_name, tag_label):\r\n            try:\r\n                query = f'SELECT id, source, artifact_name, file_name, full_description, tag FROM \"{table_name}\"'\r\n                df = pd.read_sql(query, conn)\r\n                if df.empty:\r\n                    return f\"⚠️ {table_name}: 데이터 없음, 건너뜀.\"\r\n\r\n                df[\"tag\"] = tag_label\r\n                df[\"text\"] = df.astype(str).apply(\r\n                    lambda row: \" | \".join([f\"{col}: {val}\" for col, val in row.items()]),\r\n                    axis=1\r\n                )\r\n\r\n                texts = df[\"text\"].tolist()\r\n                tags_col = df[\"tag\"].tolist()\r\n\r\n                # 배치 분할\r\n                batches = [texts[i:i+BATCH_SIZE] for i in range(0, len(texts), BATCH_SIZE)]\r\n\r\n                def process_batch(batch_id, text_list, tag_list):\r\n                    try:\r\n                        start = time.perf_counter()\r\n                        vectors = model.encode(text_list, batch_size=32, convert_to_numpy=True, show_progress_bar=False)\r\n                        collection.insert([vectors, text_list, tag_list])\r\n                        collection.flush()\r\n                        elapsed = time.perf_counter() - start\r\n                        return f\"✅ [{table_name}] Batch-{batch_id} ({len(text_list)} rows, {elapsed:.2f}s)\"\r\n                    except Exception as e:\r\n                        return f\"❌ [{table_name}] Batch-{batch_id} 오류: {e}\"\r\n\r\n                with ThreadPoolExecutor(max_workers=THREADS) as executor:\r\n                    futures = [executor.submit(process_batch, i, batches[i], [tag_label]*len(batches[i])) for i in range(len(batches))]\r\n                    for future in as_completed(futures):\r\n                        log_messages.append(future.result())\r\n\r\n                return f\"🎯 {table_name}: {len(df)} rows inserted.\"\r\n            except Exception as e:\r\n                return f\"❌ {table_name} 처리 실패: {e}\"\r\n\r\n        for tag in tags:\r\n            table_name = tag.lower()\r\n            result = embed_table(table_name, tag)\r\n            log_messages.append(result)\r\n            gc.collect()\r\n\r\n        conn.close()\r\n        collection.flush()\r\n        elapsed = (time.time() - start_total) / 60\r\n        log_messages.append(f\"🏁 전체 완료 ({elapsed:.2f}분 소요)\")\r\n        log_messages.append(f\"✅ 최종 엔티티 수: {collection.num_entities}\")\r\n        return Data(value=\"\\n\".join(log_messages))\r\n"
              },
              "dbname": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Database Name",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "dbname",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "forensic_db"
              },
              "host": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "DB Host",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "host",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "localhost"
              },
              "milvus_host": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Milvus Host",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "milvus_host",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "localhost"
              },
              "milvus_port": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Milvus Port",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "milvus_port",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "19530"
              },
              "password": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "DB Password",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "password",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "admin123"
              },
              "port": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "DB Port",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "port",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "5432"
              },
              "tags": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Tags (comma-separated)",
                "dynamic": false,
                "info": "Tags detected by HyDE Tag Parser, e.g., 'System, UserActivity'",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tags",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "user": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "DB Username",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "user",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "postgres"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ForensicMilvusEmbedderUnified"
        },
        "dragging": false,
        "id": "ForensicMilvusEmbedderUnified-28zYl",
        "measured": {
          "height": 801,
          "width": 320
        },
        "position": {
          "x": -2826.4941773208775,
          "y": 3086.920464214751
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 585.1199713560345,
      "y": 210.43008821269632,
      "zoom": 0.2
    }
  },
  "description": "Load your data for chat context with Retrieval Augmented Generation.",
  "endpoint_name": null,
  "id": "a2946997-3315-4d76-a56d-6c0caaff3b47",
  "is_component": false,
  "last_tested_version": "1.5.0.post2",
  "name": "진짜 원본 (1)",
  "tags": [
    "openai",
    "astradb",
    "rag",
    "q-a"
  ]
}